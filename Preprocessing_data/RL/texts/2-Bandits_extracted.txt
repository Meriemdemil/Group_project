Reinforcement learning 
 Dr. 
 The National School of Artificial Intelligence 
 aissa.boulmerka@ensia.edu.dz 
 2024 - 2025 
 In reinforcement learning , the agent generates its own training data by 
 interacting with the world . 
 ï‚§ The agent must learn the consequences of his own actions through trial and 
 error , rather than being told the correct action . 
 ï‚§ In this chapter , we will study this evaluative aspect of reinforcement learning . 
 We will focus on the problem of decision - making in a simplified setting called 
 bandits . 
 In the k - armed bandit problem , we have a decision - maker or agent , who 
 chooses between " ğ‘˜ " different options or actions , and receives a numerical 
 reward chosen from a stationary probability distribution based on the action it 
 selects . 
 ï‚§ Imagine the next problem where the doctor has to give medicine to a patient 
 and based on which one he choose , he will receive a reward ( cure the patient ) . 
 ï‚§ For the doctor to decide which action is best , we must define the value of 
 taking each action . 
 We call these values the action values or the action value 
 function . 
 We call these values the Action - Values or the action value function . 
 The value 
 is the expected reward . 
 The sample - average method is a method for estimating the action values . 
 We 
 will use this method to compute the value of each treatment in our medical trial 
 example . 
 ï‚§ The value of selecting an action ğ‘âˆ— is the expected reward received after that 
 action has been taken . 
 ğ‘ ğ‘ â‰ ğ”¼ ğ‘… | ğ´ = ğ‘ 
 âˆ— ğ‘¡ ğ‘¡ 
 ï‚§ ğ‘âˆ— is not known to the agent , just like the doctor does n't know the effectiveness 
 of each treatment . 
 ï‚§ 
 Instead , we will need to find a way to estimate it . 
 ï‚§ We call this method of choosing actions greedy . 
 The greedy action is the action 
 that currently has the largest estimated value . 
 ï‚§ Selecting the greedy action means the agent is exploiting its current 
 knowledge . 
 It is trying to get the most reward it can right now . 
 ï‚§ If there is more than one greedy action , then a selection is made among them 
 in some arbitrary way , perhaps randomly . 
 ï‚§ Instead of that , we can behave greedily most of the time , but every once with 
 small probability epsilon ( ğœ€ ) , select randomly from among all the actions with 
 equal probability , independently of the action - value estimates . 
 We call methods using this near - greedy action selection rule ğœº -greedy 
 methods . 
 The action - value methods we have discussed so far all estimate action values as 
 sample averages of observed rewards . 
 ï‚§ How these averages can be computed in a computationally efficient manner 
 with constant memory and constant per - time - step computation ? 
 ï‚§ 
 The general form is 
 ğ‘ğ‘’ğ‘¤ğ¸ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘’ â† ğ‘‚ğ‘™ğ‘‘ğ¸ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘’ + ğ‘†ğ‘¡ğ‘’ğ‘ğ‘†ğ‘–ğ‘§ğ‘’ ğ‘‡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ âˆ’ ğ‘‚ğ‘™ğ‘‘ğ¸ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘’ 
 1 
 ğ‘„ = ğ‘„ + ğ‘… âˆ’ ğ‘„ 
 ğ‘›:1 ğ‘› ğ‘› ğ‘› 
 n 
 ï‚§ The expression ğ‘‡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ âˆ’ ğ‘‚ğ‘™ğ‘‘ğ¸ğ‘ ğ‘¡ğ‘–ğ‘šğ‘ğ‘¡ğ‘’ is an error in the estimate . 
 It is 
 reduced by taking a step toward the ğ‘‡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡. 
 ï‚§ 
 The target is presumed to indicate a desirable direction in which to move . 
 In the 
 case above , for example , the target is the ğ‘›th reward . 
 1 
 ï‚§ 
 The function bandit(a ) is assumed to take an action and return a corresponding 
 reward . 
 ï‚§ The averaging methods discussed so far are appropriate for stationary bandit 
 problems , that is , for bandit problems in which the reward probabilities do not 
 change over time . 
 ï‚§ We often encounter reinforcement learning problems that are effectively non- 
 stationary . 
 In such cases it makes sense to give more weight to recent rewards 
 than to long - past rewards . 
 ï‚§ One of the most popular ways of doing this is to use a constant step - size 
 parameter . 
 The updating equation can be written as the initial value of ğ‘„ plus a weighted 
 sum of the rewards over time . 
 ï‚§ The most recent rewards contribute most to our current estimate . 
 Exploration allows the agent to improve his knowledge about each action . 
 Hopefully , leading to long - term benefit . 
 ï‚§ By improving the accuracy of the estimated action values , the agent can make 
 more informed decisions in the future . 
 ï‚§ Exploitation on the other hand , exploits the agent 's current estimated values . 
 It 
 chooses the greedy action to try to get the most reward . 
 ï‚§ But by being greedy with respect to estimated values , may not get the most 
 reward . 
 ï‚§ How do we choose when to explore , and when to exploit ? 
 All methods used sample 
 averages as their action - value estimates . 
 The methods above are dependent to some extent on the initial action - value 
 estimates , ğ‘„ ( ğ‘ ) . 
 1 
 For the sample - average methods ( ğœ¶ ğ’‚ = ğŸ ğ’ ) , the bias disappears once all 
 ğ’ 
 actions have been selected at least once . 
 ï‚§ For methods with constant ğœ¶ , the bias is permanent . 
 The upside is that they provide an easy way to supply some prior knowledge 
 about what level of rewards can be expected . 
 ï‚§ Initial action values can also be used as a simple way to encourage exploration . 
 It would be better to select among the non - greedy actions according to their 
 potential for actually being optimal . 
 We define a preference ğ» ğ‘ for each action ğ‘. 
 ğ‘¡ 
 ï‚§ The larger the preference , the more often that action is taken . 
 ï‚§