Reinforcement learning 
 Dr. 
 The National School of Artificial Intelligence 
 aissa.boulmerka@ensia.edu.dz 
 2024 - 2025 
 Introduction 
 ï‚§ In this chapter , the approximate value function is represented not as a table 
 but as a parameterized functional form with weight vector ğ° âˆˆ â„ğ‘‘ . 
 ï‚§ We will write ğ‘£ ğ‘  , ğ° â‰ˆ ğ‘£ ( ğ‘  ) for the approximate value of state ğ‘  given the 
 ğœ‹ 
 weight vector ğ°. 
 Note that ğ° is the vector of weights . 
 ï‚§ For example , ğ‘£ might be a linear function in features of the state . 
 More 
 generally , ğ‘£ might be a non - linear function computed by a multi - layer artificial 
 neural network . 
 ï‚§ Consequently , when a single state is updated , the change generalizes from that 
 state to affect the values of many other states . 
 Such generalization makes the 
 learning potentially more powerful but also potentially more difficult to 
 manage and understand . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 10 
 How change the weight impact the value - function 
 ğ‘¿ 
 1 2 3 4 
 1 0 -1 -2 -3 
 ğ’˜ = âˆ’ğŸ 2 1 0 -1 -2 
 ğŸ 
 ğ’€ 
 ğ’˜ = ğŸ 
 ğŸ 3 2 1 0 -1 
 4 3 2 1 0 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 11 
 How change the weight impact the value - function 
 ğ‘¿ 
 1 2 3 4 
 1 5 9 13 17 
 ğ’˜ = ğŸ’ 2 6 10 14 18 
 ğŸ 
 ğ’€ 
 ğ’˜ = ğŸ 
 ğŸ 3 7 11 15 19 
 4 8 12 16 20 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 12 
 Linear value - function approximation 
 ğ‘£ ğ‘  , ğ° â‰ ğ’˜ ğ’™ ğ’” = < ğ° , ğ± ğ’” > 
 ğ’Š ğ’Š 
 ï‚§ The value of each state is represented by a linear function of the weights . 
 ï‚§ This simply means that the value of each state , is computed as the sum of the 
 weights multiplied by some fixed attributes of the state called features . 
 ï‚§ < ğ° , ğ± ğ’” > is the dot product ( inner product ) of weight vector ğ° and the 
 feature vector ğ± ğ’” . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 13 
 Limitations of Linear Value Function Approximation 
 ğ‘¿ 
 1 2 3 4 
 ğ‘£ ğ‘  , ğ° â‰ ğ’˜ ğ’™ ğ’” ? 
 ğ’Š ğ’Š 1 0 0 0 0 
 ï‚§ 
 We can not represent this as a linear 2 0 5 5 0 
 function of ğ‘¿ and ğ’€. ğ’€ 
 ï‚§ Here ğ‘¿ and ğ’€ are not good features for this 3 0 5 5 0 
 problem . 
 4 0 0 0 0 
 ï‚§ 
 There are many powerful methods to 
 construct features . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 14 
 Tabular value - functions are linear functions 
 State Value ï‚§ 
 Linear function approximation is actually 
 very general . 
 In fact , even a tabular representation is a 
 ğ‘º 
 ğŸ 
 special case of linear function 
 ğ‘º 
 ğŸ‘ approximation . 
 â‹¯ 
 ï‚§ How to implement that ? 
 ğ‘º 
 ğ’Š 
 â‹¯ 
 ğ‘º 
 ğŸğŸ” 
 Dr. Aissa Boulmerka ( ENSIA ) Reinforcement Learning ( Chapter 8) 15 
 Tabular value - functions are linear functions 
 State Value ï‚§ 
 Let 's choose our features to be indicator 
 functions for particular states . 
 â‹¯ â‹¯ 
 0 
 ğ± ğ‘  = 
 ğ‘º ğ’˜ ğ‘– 1 
 ğ’Š ğ‘– 
 â‹¯ â‹¯ ğ‘£ ğ‘  , ğ° â‰ < ğ° , ğ± ğ’” > 0 
 â‹¯ 
 ğ‘º ğ’˜ = ğ‘¤ 
 ğŸğŸ” ğŸ6 ğ‘– 
 ğ’Š-th element 0 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 16 
 Nonlinear function approximation 
 Weights 
 ğ‘º ğ‘£ ğ‘  , ğ° Neural Network 
 Layer 
 ï‚§ Neural networks are an example of a nonlinear function of state . 
 The output of 
 the network 
 is an approximate value for a given state . 
 ï‚§ The state is passed to the network as the input . 
 All the connections in the 
 network correspond to real valued weights . 
 ï‚§ This process transforms the input state through a sequence of layers to finally 
 produce the value estimate . 
 ï‚§ Imagine a robot , observing the world through a set of distance sensors . 
 In many 
 locations , it would take the same amount of time to drive to the nearest object . 
 Even though they correspond to different sensor readings , these locations have 
 similar values . 
 Thus , we might want the value function to generalize across 
 those states . 
 ï‚§ Generalization can speed learning by making better use of the experience we 
 have . 
 ï‚§ You may not have to visit every state as much to get this values correct if we 
 can learn its value from similar states . 
 ï‚§ Going back to the example of a robot , imagine it is in a state where an object is 
 three feet away , but behind a wall . 
 ï‚§ Compare this to a state where an object is three feet away , but with a clear 
 paths to reach it . 
 ï‚§ The robot would want to assign different values to these states . 
 ï‚§ So while it is useful to generalize between states with similar distance to the 
 nearest object , it is also important that we discriminate between states based 
 on other information when it is likely to impact their value . 
 This is 
 very different from reinforcement learning . 
 But supervised learning methods 
 can be useful for handling parts of the reinforcement learning problem . 
 ï‚§ In reinforcement learning , an agent interacts with an environment and 
 continually generates new data . 
 This is often called the online setting . 
 The 
 proposed function approximation technique should work in the online setting . 
 ï‚§ TD methods introduce an additional complication when applying techniques 
 from supervised learning . 
 TD methods use bootstrapping , meaning that our 
 targets now depend on our own estimates . 
 ï‚§ These estimates change as learning progresses . 
 So our targets continually 
 change . 
 This is different than supervised learning where we have access to a 
 ground truth label as the target . 
 Suppose the following idealized scenario . 
 We get a sequence of pairs of states 
 and true values . 
 We want to use this data to find a parameterized function that closely 
 approximates ğ‘£ . 
 ğœ‹ 
 ğ‘¾ 
 ğ‘† ğ‘£ ğ‘  , ğ° â‰ˆ ğ‘£ ğ‘  
 ğœ‹ 
 ï‚§ We will do this by adjusting the weights so that the output of the function 
 approximate the associated value for a given state . 
 ï‚§ To make our goal precise , we need to specify some measure of how close our 
 approximation is to the value function . 
 The error in a state ğ‘  is computed using the square of the difference between 
 the approximate value ğ‘£ ğ‘  , ğ° and the true value ğ‘£ ğ‘  . 
 ğœ‹ 
 ï‚§ Weighting this over the state space by ğœ‡ , we obtain a natural objective function , 
 the Mean Squared Value Error , denoted ğ‘‰ğ¸ : 
 2 
 ğ‘‰ğ¸ â‰ ğœ‡(ğ‘  ) ğ‘£ ğ‘  âˆ’ ğ‘£ ğ‘  , ğ° . 
 ğœ‹ 
 ğ‘ âˆˆğ’® 
 ï‚§ Often ğœ‡ ğ‘  is chosen to be the fraction of time spent in ğ‘ . 
 Under on - policy 
 training this is called the on - policy distribution . 
 The goal of defining this objective is to adapt the weights to minimize the mean 
 squared value error . 
 Also , consider ğ‘¤ for the weight vector at each step ğ‘¡. 
 ğ‘¡ 
 ï‚§ Stochastic gradient - descent ( SGD ) methods are particularly well suited to online 
 reinforcement learning . Stochastic gradient - descent ( SGD ) methods minimize 
 error on the observed examples by adjusting the weight vector after each 
 example by a small amount in the direction that most reduce the error on that 
 example : 
 1 
 ğ° â‰ ğ° âˆ’ ğ›¼ğ›» ğ‘£ ğ‘† âˆ’ ğ‘£ ğ‘† , ğ° 2 
 ğ‘¡+1 ğ‘¡ ğœ‹ ğ‘¡ ğ‘¡ ğ‘¡ 
 2 
 = ğ° + ğ›¼ ğ‘£ ğ‘† âˆ’ ğ‘£ ğ‘† , ğ° ğ›»ğ‘£ ğ‘† , ğ° 
 ğ‘¡ ğœ‹ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 27 
 From gradient descent to stochastic gradient descent 
 ğœ‡ ğ‘  ğ‘£ ğ‘  âˆ’ ğ‘£ ğ‘  , ğ° ğ›»ğ‘£ ğ‘  , ğ° 
 ğœ‹ 
 ğ‘ âˆˆğ’® 
 ğ‘† , ğ‘£ ğ‘† , ğ‘† , ğ‘£ ğ‘† , ğ‘† , ğ‘£ ğ‘† , â‹¯ 
 1 ğœ‹ 1 2 ğœ‹ 2 3 ğœ‹ 3 
 Dr. Aissa Boulmerka ( ENSIA ) Reinforcement Learning ( Chapter 8) 28 
 Gradient of the MSVE objective 
 ğ›» ğœ‡ ğ‘  ğ‘£ ğ‘  âˆ’ ğ‘£ ğ‘  , ğ‘¤ 2 Linear value function 
 ğœ‹ 
 ğ‘ âˆˆğ’® ğ‘£ ğ‘  , ğ° â‰ < ğ° , ğ± ğ’” > 
 ğœ‡ ğ‘  ğ›» ğ‘£ ğ‘  âˆ’ ğ‘£ ğ‘  , ğ° 
 2 ğ›»ğ‘£ ğ‘  , ğ° = ğ± ğ’” 
 = 
 ğœ‹ 
 ğ‘ âˆˆğ’® 
 âˆ’ ğœ‡ ğ‘  2 ğ‘£ ğ‘  âˆ’ ğ‘£ ğ‘  , ğ° ğ›»ğ‘£ ğ‘  , ğ° 
 = 
 ğœ‹ 
 ğ‘ âˆˆğ’® 
 ğ›»ğ° âˆ ğœ‡ ğ‘  ğ‘£ ğ‘  âˆ’ ğ‘£ ğ‘  , ğ° ğ›»ğ‘£ ğ‘  , ğ° 
 ğœ‹ 
 ğ‘ âˆˆğ’® 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 29 
 Gradient Monte Carlo 
 ğ° â‰ ğ° + ğ›¼ ğ’— ğ‘º âˆ’ ğ‘£ ğ‘† , ğ° ğ›»ğ‘£ ğ‘† , ğ° 
 ğ‘¡+1 ğ‘¡ ğ… ğ’• ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 ğ° â‰ ğ° + ğ›¼ ğº âˆ’ ğ‘£ ğ‘† , ğ° ğ›»ğ‘£ ğ‘† , ğ° 
 ğ‘¡+1 ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 ï‚§ Recall that : 
 ğ‘£ ğ‘  â‰ ğ”¼ ğº |ğ‘† = ğ‘  
 ğœ‹ ğœ‹ ğ‘¡ ğ‘¡ 
 ï‚§ The expectation of the gradient when we use a sampled return in place of 
 the true value , is still equal to the gradient of the MSVE . 
 ğ”¼ ğ‘£ ğ‘† âˆ’ ğ‘£ ğ‘† , ğ° ğ›»ğ‘£ ğ‘† , ğ° 
 ğœ‹ ğœ‹ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 ğº 
 = ğ”¼ âˆ’ ğ‘£ ğ‘† , ğ° ğ›»ğ‘£ ğ‘† , ğ° 
 ğœ‹ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 30 
 Gradient Monte Carlo state - value prediction 
 ï‚§ Suppose the states in the examples are the states generated by interaction 
 with the environment using policy ğœ‹. 
 ï‚§ 
 The SGD method converges to a locally optimal approximation to ğ‘£ ğ‘† . 
 ğœ‹ ğ‘¡ 
 ï‚§ 
 So , the gradient - descent version of Monte Carlo state - value prediction is 
 guaranteed to find a locally optimal solution . 
 In this case , all the probability that would have 
 gone into those missing neighbors 
 ï‚§ Termination on the left produces a reward of âˆ’1 , and termination on the right 
 produces a reward of +1 . 
 Because of this we can not guarantee this algorithm will converge to a 
 local minimum of the value error . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 38 
 Semi - gradient TD(0 ) 
 1 
 ğ›» ğ‘ˆ âˆ’ ğ‘£ ğ‘† , ğ° 
 2 
 ğ‘¡ ğ‘¡ 
 2 
 We have ğ‘ˆ â‰ ğ‘… + ğ›¾ğ‘£ ğ‘† , ğ° 
 ğ‘¡ ğ‘¡+1 ğ‘¡+1 
 = ğ‘ˆ âˆ’ ğ‘£ ğ‘† , ğ° ğ›»ğ‘ˆ âˆ’ ğ›»ğ‘£ ğ‘† , ğ° 
 ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 â‰  ğ‘ˆ âˆ’ ğ‘£ ğ‘† , ğ° ğ›»ğ‘£ ğ‘† , ğ° ( The TD update ) 
 ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 This is true only if ğ›»ğ‘ˆ = 0 
 ğ‘¡ 
 For TD : 
 ğ›»ğ‘ˆ = ğ›» ğ‘… + ğ›¾ğ‘£ ğ‘† , ğ° 
 ğ‘¡ ğ‘¡+1 ğ‘¡+1 
 = ğ›¾ğ›»ğ‘£ ğ‘† , ğ° 
 ğ‘¡+1 
 â‰  ğŸ 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 39 
 Semi - gradient TD(0 ) 
 ï‚§ Bootstrapping methods are not in fact instances of true gradient descent . 
 They 
 take into account the effect of changing the weight vector ğ° on the estimate , 
 ğ‘¡ 
 but ignore its effect on the target . 
 They include only a part of the gradient and , 
 accordingly , we call them semi - gradient methods . 
 ï‚§ Although semi - gradient ( bootstrapping ) methods do not converge as robustly 
 as gradient methods , they do converge reliably in important cases such as the 
 linear case . 
 ï‚§ Another advantage , is that they enable learning to be continual and online , 
 without waiting for the end of an episode . 
 This enables them to deal with 
 continuing problems and provides computational advantages . 
 The update could be biased because 
 estimate of the gradient of the value the estimate in the target may not be 
 error . accurate . 
 ï‚§ 
 It will approach a local minimum of ï‚§ 
 Since the value approximation will 
 the Mean Squared Value Error with never be perfect even in the limit , the 
 more and more samples . 
 target may remain biased . 
 ï‚§ Gradient Monte Carlo will converge ï‚§ 
 Semi - gradient TD can not guarantee 
 to a local minimum of the mean to converge to a local minimum at 
 squared value error . 
 the Mean Squared value error . 
 Semi- 
 Conclusions : 
 Gradient TD 
 ï‚§ We can conclude that TD often learns 
 faster than Monte Carlo . 
 This is because TD 
 Episodes 
 can learn during the episode and has lower variance 
 updates . 
 ï‚§ Monte Carlo is better on long - run performance , it 's not always the main concern . 
 ï‚§ We can never run our experiments to achieve asymptotic performance . 
 ï‚§ Early learning is perhaps more important in practice . 
 Linear function approximation 
 ï‚§ 
 Corresponding to every state ğ‘  , there is a real - valued vector 
 ğ‘‡ 
 ğ± ğ‘  â‰ ğ‘¥ ğ‘  , ğ‘¥ ğ‘  , â‹¯ , ğ‘¥ ğ‘  
 1 2 ğ‘‘ 
 ï‚§ Linear methods approximate state - value function by the inner product between 
 ğ° and ğ±(ğ‘  ): 
 ğ‘‘ 
 ğ‘‡ 
 ğ‘£ ğ‘  , ğ° â‰ ğ‘¤ ğ± ğ‘  â‰ ğ‘¤ ğ‘¥ ğ‘  . 
 ğ‘– ğ‘– 
 ğ‘–=1 
 ï‚§ 
 In this case the approximate value function is said to be linear in the weights , 
 or simply linear . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 45 
 Linear function approximation 
 ï‚§ 
 The vector ğ±(ğ‘  ) is called a feature vector representing state ğ‘ . 
 ï‚§ 
 Each component ğ‘¥ ( ğ‘  ) of ğ±(ğ‘  ) is the value of a function ğ‘¥ : ğ’® â†’ â„. 
 ğ‘– ğ‘– 
 ï‚§ We think of a feature as the entirety of one of these functions , and we call its 
 value for a state ğ‘  a feature of ğ’”. 
 ï‚§ For linear methods , features are basis functions because they form a linear 
 basis for the set of approximate functions . 
 ï‚§ Constructing ğ’…-dimensional feature vectors to represent states is the same as 
 selecting a set of ğ‘‘ basis functions . 
 ï‚§ Features may be defined in many different ways ; we cover a few possibilities 
 later in this course . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 46 
 SGD update of linear function approximation 
 ï‚§ 
 It is natural to use SGD updates with linear function approximation . 
 The 
 gradient of the approximate value function with respect to ğ° in this case is : 
 ğ›»ğ‘£ s , ğ° = ğ±(ğ‘  ) . 
 ï‚§ 
 In this case the general SGD update reduces to a particularly simple form : 
 ğ° ğ° + ğ›¼ ğ‘ˆ âˆ’ ğ‘£ ğ‘† , ğ° ğ±(ğ‘† ) 
 â‰ 
 ğ‘¡+1 ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ 
 ï‚§ Because it is so simple , the linear SGD case is one of the most favorable for 
 mathematical analysis . 
 ï‚§ Almost all useful convergence results for learning systems of all kinds are for 
 linear ( or simpler ) function approximation methods . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 47 
 TD update of linear function approximation 
 The semi - gradient TD(0 ) algorithm also converges under linear function 
 approximation . 
 The weight vector converged to is also not the global optimum , 
 but rather a point near the local optimum . 
 ï‚§ The update at each time t is 
 ğ° â‰ ğ° + ğ›¼ğ›¿ ğ±(ğ‘† ) 
 ğ‘¡+1 ğ‘¡ ğ‘¡ ğ‘¡ 
 ğ›¿ = ğ‘… + ğ›¾ğ‘£ ğ‘† , ğ° âˆ’ ğ‘£ ğ‘† , ğ° 
 ğ‘¡ ğ‘¡+1 ğ‘¡+1 ğ‘¡ ğ‘¡ ğ‘¡ 
 Where ğ›¿ is the TD error . 
 ğ‘¡ 
 ï‚§ This fixed basis given by the expert design features has a large impact on the 
 update . 
 If well - designed , we can get effective value function approximation 
 with a simple update . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 48 
 Tabular TD is a special case of linear TD 
 ï‚§ 
 We can show that linear TD is a strict generalization of both 
 tabular TD and TD with state aggregation . 
 0 
 ğ° â† ğ° + ğ›¼ ğ‘… + ğ›¾ğ‘£ ğ‘† , ğ° âˆ’ ğ‘£ ğ‘† , ğ° ğ±(ğ‘† ) 
 0 
 ğ‘¡+1 ğ‘¡+1 ğ‘¡ ğ‘¡ 
 â‹¯ 
 ï‚§ 
 In the update , the feature vector ğ±(ğ‘† ) selects a single weight 0 
 ğ‘¡ 
 ğ± ğ‘  = 
 associated with the current state . 
 It is clear that , if the system converges , it must converge to the weight vector 
 ğ° at which 
 ğ‘‡ğ· 
 ğ”¼ âˆ†ğ° = ğ›¼ ğ› âˆ’ ğ€ğ° = 0 
 ğ‘‡ğ· ğ‘‡ğ· 
 âˆ’ğŸ 
 â‡’ ğ° = ğ€ ğ› 
 ğ‘‡ğ· 
 ï‚§ If ğ€ is invertible , ğ° is a solution to this linear system . 
 We call this solution 
 ğ‘‡ğ· 
 the TD fixed point . 
 In fact linear semi - gradient TD(0 ) converges to this point . 
 ï‚§ ğ° minimizes ğ› âˆ’ ğ€ğ° ğ‘‡ ğ› âˆ’ ğ€ğ° . This objective extends the connection 
 ğ‘‡ğ· 
 between TD and Bellman equations , to the function approximation setting . 
 ï‚§ Representing a state with features that 
 overlap in this way is known as coarse 
 coding . 
 Dr. Aissa Boulmerka ( ENSIA ) Reinforcement Learning ( Chapter 8) 53 
 Coarse coding generalization 
 ï‚§ 
 Generalization in linear function approximation methods is determined by the 
 sizes and shapes of the features â€™ receptive fields . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 54 
 Tile coding 
 ï‚§ 
 In tile coding the receptive fields of the features are grouped into partitions of 
 the state space . 
 ï‚§ Each such partition is called a tiling , and each element of the partition is called a 
 tile . 
 Dr. ( ENSIA ) Reinforcement Learning ( Chapter 8) 55 
 Why we use coarse coding ? 
 In theory , a neural network need not be deep . 
 A neural network with a single 
 hidden layer can approximate any continuous function given that is sufficiently 
 wide . 
 We call this the universal approximation property . 
 ï‚§ Practical experience and theory suggests that deep neural networks may make 
 it easier to approximate complex functions . 
 ï‚§ Composition can produce more specialized features by combining modular 
 components . 
 ï‚§ Overall , depth in a network can significantly improve our agent 's ability to learn 
 features .