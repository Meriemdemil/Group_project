Reinforcement learning 
 Dr. 
 The National School of Artificial Intelligence 
 aissa.boulmerka@ensia.edu.dz 
 2024 - 2025 
 ï‚§ In reinforcement learning , methods can be categorized based on whether they 
 use a model of the environment or not . 
 ï‚§ The methods that require a model , such as dynamic programming and heuristic 
 search , these methods are called model - based RL methods . 
 ï‚§ The methods that can be used without a model , such as Monte Carlo and 
 temporal - difference methods are called model - free RL methods . 
 ï‚§ Model - based methods rely on planning as their primary component , while 
 model - free methods primarily rely on learning . 
 ï‚§ It would be even better if we could obtain an intermediate method that can 
 leverage the best of both extremes . 
 In this chapter , we will develop a unified 
 view of reinforcement learning methods that require planning and learning 
 strategies using the Dyna architecture . 
 By a model of the environment we mean anything that an agent can use to 
 predict how the environment will respond to its actions . 
 ï‚§ Given a state and an action , a model produces a prediction of the resultant 
 next state and next reward . 
 ï‚§ If the model is stochastic , then there are several possible next states and next 
 rewards , each with some probability of occurring . 
 Some models produce just one of the possibilities , Sample 
 sampled according to the probabilities ; these we call 
 sample models 
 For example , a sample model for flipping a 
 coin can generate a random sequence of heads and tails . 
 ï‚§ Other models produce a description of all possibilities and 
 their probabilities ; these we call distribution models . 
 ï‚§ Models in reinforcement learning simulate experiences by 
 Distribution 
 predicting state transitions . 
 ï‚§ Sample models can be computationally inexpensive 
 because random outcomes can be produced according to a 
 set of rules . 
 ï‚§ But it can be useful that a distribution 
 model produces the exact probability of 
 an outcome . 
 For example , we can 
 compute the expected outcome directly 
 or quantify the variability in outcomes . 
 Sample models require less memory ( they are more 
 compact than distribution models ) . 
 ï‚§ Distribution models on the other hand can be used 
 to compute the exact expected outcome by 
 summing over all outcomes weighted by their 
 probabilities . 
 ï‚§ Sample models can only approximate this expected 
 outcome by averaging many samples together . 
 ï‚§ Knowing the exact probabilities also has the 
 flexibility of assessing risk . 
 For example , when a 
 doctor is prescribing medicine , they would consider 
 many possible side effects and how likely they are to 
 occur . 
 How one can leverage a model to better inform decision - making without 
 having to interact with the world , we call this process planning with model 
 experience . 
 ï‚§ Learning methods require only experience as input , and in many cases they can 
 be applied to simulated experience just as well as to real experience . 
 Imagine that actions can only be taken at specific time points , but 
 Learning Updates can be executed relatively fast . 
 ï‚§ This results in some waiting time from after the Learning Update and 
 when the next action is taken . 
 ï‚§ We can fill in this waiting time with Planning Updates . 
 Policy / value functions 
 ï‚§ Q - planning on the other hand 
 performs updates using model 
 Q - Planning 
 ( simulated ) experience from the update 
 Q - Learning 
 model . 
 update 
 experience 
 environment 
 ï‚§ 
 In Dyna - Q , the acting , model - learning , and direct RL processes require little 
 computation , and we assume they consume just a fraction of the time . 
 ï‚§ The remaining time in each step can be devoted to the planning process , which 
 is inherently computation - intensive . 
 ï‚§ Let us assume that there is time in each step , after acting , model - learning , and 
 direct RL , to complete ğ‘› iterations ( Steps 1â€“3 ) of the Q - planning algorithm . 
 ï‚§ In the pseudocode algorithm for Dyna - Q , ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™(ğ‘  , ğ‘ ) denotes the contents of 
 the ( predicted next state and reward ) for state â€“ action pair ( ğ‘  , ğ‘ ) . 
 ï‚§ Direct reinforcement learning , model - learning , and planning are implemented 
 by steps ( d ) , ( e ) , and ( f ) , respectively . 
 Consider the following simple maze . 
 Reward is ğŸ on all transitions , except those into the goal state , it is + ğŸ. 
 ï‚§ 
 After reaching the goal state ( G ) , the agent returns to the start state ( S ) to 
 begin a new episode . 
 The general problem here is another version of the conflict between 
 exploration and exploitation . 
 ï‚§ In a planning context , exploration means trying actions that improve the model , 
 whereas exploitation means behaving in the optimal way given the current 
 model . 
 ï‚§ We want the agent to explore to find changes in the environment , but not so 
 much that performance is greatly degraded . 
 ï‚§ As in the earlier exploration / exploitation conflict , there probably is no solution 
 that is both perfect and practical , but simple heuristics are often effective . 
 since each state- 
 action pair was last tried 
 Greater elapsed time 
 suggests a higher chance the model of the pair is 
 incorrect . 
 A " bonus reward " is given for simulated 
 experiences with long - untried actions . 
 ğ‘µğ’†ğ’˜ ğ’“ğ’†ğ’˜ğ’‚ğ’“ğ’… = ğ’“ + ğœ¿ ğ‰ 
 ï‚§ where ğ’“ is the actual reward , ğ‰ is the time steps 
 since transition was last tried , and ğœ¿ is a small 
 constant . 
 ï‚§ This encourages the agent to test all accessible 
 state transitions and discover new action 
 sequences , despite the cost . 
 In this case , Dyna - Q+ 's increased exploration helps it develop a better policy 
 more quickly . 
 ï‚§ After a shortcut is introduced on the right side of the wall , Dyna - Q+ quickly 
 identifies and uses this shortcut following the environment changes . 
 ï‚§ In contrast , Dyna - Q fails to find the shortcut within the given time and would 
 take a very long time to discover it . 
 ï‚§ The persistent and systematic exploration by Dyna - Q+ proves to be crucial in 
 this environment . 
 Average performance of Dyna 
 agents on a blocking task . 
 ï‚§ Dyna - Q+ is Dyna - Q with an 
 exploration bonus that 
 encourages exploration . 
 Average performance of Dyna 
 agents on a shortcut task . 
 ï‚§ The definition of return : ( episodic or continuing , discounted or 
 undiscounted ) 
 ï‚§ Action values vs. state values vs. after state . 
 ï‚§ Action selection / exploration .