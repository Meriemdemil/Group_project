Univ Gustave Eiffel - Cosys / Grettia 
 Reinforcement Learning & Optimal Control 
 Advanced Topics in RL 
 Nadir Farhi 
 chargé de recherche , UGE - Cosys / Grettia 
 nadir.farhi@univ-eiffel.fr 
 -based RL 3- Muti - agent RL 
 Model - based methods rely on planning . 
 Both approaches compute value functions . 
 planning uses simulated experience generated by a model . 
 Planning : model ⟶ new or improved policy . 
 “ The image of the world around us , which we carry in our head , is just a model . 
 Nobody in his 
 head 
 imagines all the world , government or country . 
 He has only selected concepts , and 
 relationships between them , and uses those to represent the real system . ” 
 , , the father of system dynamics . 
 Internal model , prediction & perception 
 Internal model : 
 Our brain learns an abstract representation of both spatial and temporal aspects . 
 observe a scene ⟶ Remember an abstract description . 
 Internal model ⟶ Prediction of the future ⟶ Our perception . 
 E.g. when we face a danger ⟶ Act on internal model ⟶ Perform fast reflexive behavior 
 ⟶ without need to plan out a course of action . 
 World models 
 World model & controller model 
 Most existing model - based RL approaches : 
 = = > Learn a model of the RL environment , but still train on the actual environment . 
 World models divide a RL agent : large world model and small control model . 
 1- Train a large NN to learn a world model in an unsupervised manner . 
 2- Train the smaller controller model to learn a task using this world model . 
 3- Transfer this policy back into the actual environment . 
 World models 
 The V Model ( vision ) 
 Flow diagram of a Variational Autoencoder . 
 Environment ⟶ high dimensional observation ( 2D image frame of a video sequence ) 
 V model ⟶ learns an abstract compressed representation of observed input frame . 
 World models 
 The M model ( memory RNN ) 
 The Mixture Density RNN outputs the parameters of a mixture of Gaussian distribution used to sample a prediction , 
 The M model serves as a predictive model of the future z vectors that V is expected to produce . 
 The RNN outputs a probability density function ( complex environments are stochastic in nature ) . 
 The RNN models P(z_t+1 / a_t , z_t , h_t ) , where h_t is the hidden state of the RNN at time t. 
 During sampling , we can adjust a temperature parameter ττ to control model uncertainty . 
 World models 
 SketchRNN [ 1 ] 
 [ 1 ] Draw Together with a Neural Network , , , . 
 Google AI Experiments . 
 The C model ( Controller ) 
 C determines the course of actions to maximize the expected cumulative reward . 
 ⟶ Put Most of the model ’s complexity , and model parameters in V and M. 
 Controller 
 The number of parameters of C ( linear model ) is minimal in comparison . 
 Optimize the param . of C : Covariance - Matrix Adaptation Evolution Strategy ( CMA - ES ) [ 1,2,3 ] . 
 We evolve parameters of C on a single machine with multiple CPU cores running multiple 
 rollouts of the environment in parallel . 
 [ 1 ] The CMA Evolution Strategy : A Tutorial . 
 . 
 [ 2 ] Completely Derandomized Self - Adaptation in Evolution Strategies . 
 , A. Ostermeier . 
 MIT Press . 
 A Visual Guide to Evolution Strategies . 
 blog.otoro.net . 
 ] 
 Model - free RL and model - based RL : assume reward function is known . 
 Both collect system data to : 
 - update a learned model ( model - based ) 
 - or directly update a learned value function or policy ( model - free ) . 
 Drawbacks : 
 - Accurate representation of the true performance objectives can be challenging . 
 - Rewards may be sparse ⟶ 
 Learning process expensive ( data , time , memory ) . 
 Imitation Learning : 
 We assume reward function is unknown a priori 
 It is described implicitly through expert demonstrations . 
 [ 
 [ 1 ] 
 ⟶ Infer reward function from roll - outs of expert policy 
 Use a set of expert demonstrations ξ ∈ Ξ to determine a policy π that imitates the expert . 
 It can be accomplished through supervised learning techniques : 
 L is a cost function 
 can include p - norms ( e.g. Euclidean norm ) or f -divergences ( e.g. KL divergence ) . 
 This approach may not yield very good performance 
 since the learning process is only based on a set of samples provided by the expert . 
 Drawbacks of the direct approach ( learn policies ) 
 no way to understand the underlying reasons for the expert behavior 
 ( no reasoning about outcomes or intentions ) . 
 The “ expert ” may actually be suboptimal . 
 A policy that is optimal for the expert may not be optimal for the agent 
 ( if they have different dynamics , morphologies , or capabilities . ) 
 Alternative : Learn a representation of the underlying reward function 
 ⟶ Learn the expert ’s intent 
 ⟶ Potentially outperform the expert 
 ⟶ or adjust for differences in capabilities 
 This approach is known as : 
 Inverse RL 
 Nadir Farhi Reinforcement Learning and Optimal Control ENSIA - 23 April 2025 33 
 Apprenticeship Learning 
 [ 1 ] and . “ Apprenticeship Learning via Inverse Reinforcement Learning ” . 
 In : Proceedings of the Twenty- First International 
 Conference on Machine Learning . 
 Apprenticeship Learning [ 1 ] 
 Objective : Find π that makes μ(π ) as similar as possible to μ(π∗ ) 
 [ 1 ] and . “ Apprenticeship Learning via Inverse Reinforcement Learning ” . 
 In : Proceedings of the Twenty- First International 
 Conference on Machine Learning . 
 Environment : the agents interact in a shared environment 
 Goal : 
 Shared goal : 
 a fleet of mobile robots to collect and deliver goods within a large warehouse 
 a team of drones tasked with monitoring a power plant . 
 Conflicting goals : 
 agents trading goods in a virtual market : each agent seeks to maximize its own gains . 
 The agents begin to try actions in their environment 
 Collect experiences about how the environment changes as a result of their actions , 
 Learn how the other agents behave . 
 Learn skills needed to solve their task 
 Learn how to coordinate their actions with other agents 
 May even learn to develop a shared language to enable communication 
 Finally , they reach a certain level of proficiency 
 and have become experts at interacting optimally to achieve their goals . 
 E.g. all agents receive a reward +1 whenever an agent performs a successful action . 
 Competitive scenario , the agents are in direct competition with each other . 
 Multi - agent RL - Introduction 
 Formal ( mathematical ) definition 
 Normal - form games ( specification of players ' strategy spaces and payoff functions ) 
 Stochastic games , 
 Partially observable stochastic games 
 Solution for a game model : consists of a set of policies for the agents that satisfies certain 
 desired properties . 
 Eg . Nash Equilibrium : No individual agent can deviate from its policy in the solution to improve 
 its outcome . 
 Reinforcement Learning and Optimal Control ENSIA - 23 April 2025 45 
 Multi - agent RL - Introduction 
 Centralized vs Decentralized decision - making 
 Challenge : the agents need to coordinate their actions in order to be successful . 
 MARL may use various approaches to facilitate the learning of coordinated agent policies . 
 Autonomous driving ( urban env . ): each car requires its own local policy to drive . 
 Robots for search - and - rescue : each agent may need to act fully independently . 
 Reinforcement Learning and Optimal Control ENSIA - 23 April 2025 47 
 Multi - agent RL - Introduction 
 Dimensions in MARL 
 Size 
 How many agents exist in the environment ? 
 Is the number of agents fixed , or can it change ? 
 How many states and actions does the environment specify ? 
 Are states / actions discrete or continuous ? 
 Are actions defined as single values or multi - valued vectors ? 
 Do they know their own reward functions , and the reward of other agents ? 
 Do agents know the state transition probabilities of the environment ? 
 Can they observe the actions and/or the rewards of other agents ? 
 Nadir Farhi Reinforcement Learning and Optimal Control ENSIA - 23 April 2025 50 
 Multi - agent RL - Introduction 
 Dimensions in MARL 
 Reinforcement Learning and Optimal Control ENSIA - 23 April 2025 51 
 Multi - agent RL - Introduction 
 Dimensions in MARL 
 Objective 
 Is the agents ’ goal to learn an equilibrium joint policy ? 
 What type of equilibrium ? 
 Is performance during learning important , or only the final learned policies ? 
 Is the goal to perform well against certain classes of other agents ? 
 Nadir 
 Reinforcement Learning and Optimal Control ENSIA - 23 April 2025 52 
 Multi - agent RL - Introduction 
 Dimensions in MARL 
 Cenralization /communication 
 Can agents coordinate their actions via a central controller or coordinator ? 
 Or do agents learn fully independent policies ? 
 Multi - agent RL - Introduction 
 Dimensions in MARL 
 Training and execution algorithms 
 Centralized training and execution : 
 both stages have access to some centrally shared mechanism or 
 information , such as sharing all observations between agents . 
 Reinforcement Learning and Optimal Control ENSIA - 23 April 2025 55 
 Multi - agent RL - Introduction 
 Multi - Robot Warehouse Management 
 Observation : 
 location 
 current heading within the warehouse , 
 the items carried , 
 the current order serviced . 
 + information of other agents ( locations , items , and orders ) . 
 Actions : 
 - physical movements : rotate , accelerate , brake , pick items . 
 - send communication messages to other robots ( travel plans , etc . ) 
 Rewards : 
 individual positive reward when completing an order 
 reward when any order has been completed by any robot . 
 Multi - agent RL - Introduction 
 Autonomous Driving 
 Observation ? 
 Actions ? 
 Rewards ? 
 Multi - agent RL - Introduction 
 Autonomous Driving 
 observations 
 own controlled vehicle ( e.g. , position on lane , orientation , and speed ) 
 other nearby vehicles ( may be uncertain , noisy or incmoplete ) 
 actions 
 continuous controls , such as steering and acceleration / braking , 
 discrete actions e.g. , change lane , turning , overtaking 
 Rewards 
 Penalize collisions ( negative reward ) . 
 positive rewards for minimizing driving times , 
 negative rewards for abrupt acceleration / braking and frequent lane 
 changes . 
 Tasks are broken down into sub - tasks , ... 
 Each level of the hierarchy focuses on solving a specific aspect of the overall task . 
 HRL simplifies the learning process and improves the scalability and efficiency . 
 This idea is very similar to breaking down large number of lines of code to smaller 
 functions each performing a very specific task . 
 Hierarchical Reinforcement Learning ( HRL ) 
 Key components 
 1- Hierarchical Policies : 
 Lower - level policies : 
 focus on achieving specific goals within the context set by higher - level policies . 
 Higher - level policies : 
 determine which sub - task or lower - level policy to activate . 
 Key components 
 2- Options Framework : 
 Subgoals : intermediate states to reach the final destination . 
 Operates on a more abstract level 
 Focusing on the overall strategy to navigate the environment . 
 Low - Level Controller ( Subgoal Achievement ): 
 Responsible of achieving the subgoals set by the high - level controller . 
 Involves fine - grained control of the robot 's movements 
 such as turning , moving forward , and avoiding obstacles in the immediate vicinity . 
 Uses a standard DQN approach to learn these controls . 
 The high - level controller received a reward when the robot achieved a subgoal 
 that moved it closer to the target . 
 The low - level controller received rewards for successful execution of movements 
 that contributed to achieving these subgoals . 
 Training : 
 The H - DQN is trained in a simulated environment 
 The robot learn to navigate mazes of increasing complexity .