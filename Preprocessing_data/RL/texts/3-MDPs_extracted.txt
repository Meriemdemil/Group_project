Reinforcement learning 
 Dr. 
 The National School of Artificial Intelligence 
 aissa.boulmerka@ensia.edu.dz 
 2024 - 2025 
 â€¢ States : 
 â€¢ States can be low - level sensory readings , for example , in the pixel values of 
 the video frame . 
 â€¢ They can also be high - level such as object descriptions . 
 â€¢ Actions : 
 â€¢ Actions can be low - level , such as the wheel speed of this robot . 
 â€¢ Actions can also be high - level , such as go to the charging station . 
 â€¢ Time - steps : 
 â€¢ Time - steps can be very small or very large . 
 The goal of the robot is to pick - and - place 
 objects . 
 There are many ways to formalize this task : 
 State : The state could be the readings of the joint 
 angles and velocities . 
 Action : the amount of voltage applied to each 
 motor . 
 Reward : +100 for successfully placing each object . 
 -1 for each unit of energy consumed . 
 In RL , the goal of the agent is to maximize future reward . 
 ï‚§ Formally , the return at time step ğ‘¡ , is simply the sum of rewards obtained 
 after time step ğ‘¡. 
 We denote the return with the ğº . 
 ğ‘¡ 
 ğº â‰ ğ‘… + ğ‘… + ğ‘… + â‹¯ 
 ğ‘¡ ğ‘¡+1 ğ‘¡+2 ğ‘¡+3 
 ï‚§ The return is a random variable because the dynamics of the MDP can be 
 stochastic . 
 In general , many different trajectories from the same state are possible . 
 This is why we maximize the expected return . 
 ï‚§ For this to be well - defined , the sum of rewards must be finite . 
 Interaction goes on continually 
 ï‚§ ğœ¸ < ğŸ , the infinite sum has a finite value as long as the reward sequence 
 * ğ‘… + is bounded . 
 in this case , the agent is concerned only with maximizing 
 immediate rewards . 
 The agent is called Short - sighted agent . 
 ï‚§ ğœ¸ â†’ ğŸ , the return objective takes future rewards into account more 
 strongly . 
 The agent becomes Far - sighted agent . 
 Returns at successive time steps are related to each other in a way that is 
 important for the theory and algorithms of reinforcement learning : 
 2 3 
 ğº â‰ ğ‘… + ğ›¾ğ‘… + ğ›¾ ğ‘… + ğ›¾ ğ‘… + â‹¯ 
 ğ‘¡ ğ‘¡+1 ğ‘¡+2 ğ‘¡+3 ğ‘¡+4 
 2 
 ğº = ğ‘… + ğ›¾ ğ‘… + ğ›¾ğ‘… + ğ›¾ ğ‘… + â‹¯ 
 ğ‘¡ ğ‘¡+1 ğ‘¡+2 ğ‘¡+3 ğ‘¡+4 
 ğ‘® = ğ‘¹ + ğ›„ğ‘® 
 ğ’• ğ­+ğŸ ğ­+ğŸ 
 ï‚§ Although the return is a sum of an infinite number of terms , it is still finite if the 
 reward is nonzero and constant . 
 ï‚§ 
 It can be seen as an episodic task , where each 
 attempt to balance the pole is an episode . 
 A reward of +1 can be given for each time step 
 without failure , with the total reward being the 
 number of steps before failure . 
 ï‚§ Alternatively , it can be treated as a continuous 
 task using discounting . 
 A policy is a mapping from states to probabilities of selecting each possible 
 action . 
 The value function of a state ğ‘  under a policy ğœ‹ , denoted ğ‘£ ( ğ‘  ) , is the expected 
 ğœ‹ 
 return when starting in ğ‘  and following the policy ğœ‹. 
 ï‚§ 
 We call the 
 function ğ‘£ the state - value function for policy ğ…. 
 ğœ‹ 
 ï‚§ 
 A fundamental property of value functions used throughout reinforcement 
 learning and dynamic programming is that they satisfy recursive relationships . 
 ï‚§ It will be a recursive equation for the value of a state action pair in terms of its 
 possible successors state action pairs . 
 We can only solve small MDPs directly , but Bellman Equations will factor into 
 solutions we see later for large MDPs . 
 A policy ğœ‹ is defined to be better than or equal to a policy ğœ‹ if its expected 
 1 2 
 return is greater than or equal to that of ğœ‹ for all states . 
 ï‚§ In general it is not possible to implement this solution exactly . 
 Even if we limit 
 ourselves to deterministic policies , the number of possible policies is 
 |ğ“||ğ“¢| 
 . 
 ï‚§ We can use a brute force search to compute the value function for every policy 
 to find the optimal policy â‡’ intractable for even moderately large MDPs . 
 ï‚§ Fortunately , there 's a better way to organize the search of the policy space . 
 The solution will come in the form of yet another set of equations , 
 called the Bellman 's Optimality equations . 
 We consider a variety of such 
 methods in the following chapters . 
 Example : Solving the Gridworld 
 Suppose we solve the optimality equation for ğ‘£ for the Gridworld .