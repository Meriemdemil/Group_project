Reinforcement learning 
 Dr. 
 The National School of Artificial Intelligence 
 aissa.boulmerka@ensia.edu.dz 
 2024 - 2025 
 Research Areas : Deep learning models applied to image / video segmentation 
 and recognition . 
 This course will provide a solid 
 introduction to the field of reinforcement learning and 
 students will learn about the core challenges and 
 approaches , including generalization and exploration . 
 ï‚§ Prerequisite : 
 Linear algebra , probability , programming . 
 ï‚§ Textbook : 
 and . Reinforcement 
 Learning : An Introduction . 
 What is Reinforcement Learning ( RL ) 
 ï‚§ A way of programming agents by reward and punishment without 
 needing to specify how the task is to be achieved ( * ) . 
 ï‚§ This approach allows us to train intelligent entities ( robots , software , or 
 agents ) in a much more flexible and efficient way . 
 ï‚§ Instead of hard - coding every action or behavior the agent needs to 
 perform , we simply guide it by providing positive reinforcement for 
 desirable behaviors and negative reinforcement for undesirable ones . 
 ï‚§ 
 Over time , the agent learns on its own how to solve problems and 
 achieve goals . 
 ï‚§ ( * ) , , and . 
 " Reinforcement learning : A survey . " 
 The process of learning to ride a bike relies on trial and error , with feedback 
 from falls ( pain ) and successes ( reward ) , rather than detailed explanations . 
 RL Course at UCL . 
 RL Course at UCL . 
 There is no supervisor , only a reward signal 
 ï‚§ 
 Feedback is delayed , not instantaneous 
 Helicopter Flight Manoeuvers 
 Video : https://www.youtube.com/watch?v=0JL04JJjocc 
 , , , . 
 An Application of Reinforcement Learning to Aerobatic Helicopter 
 Flight . 
 Human - level control through deep reinforcement learning . 
 Mastering the game of Go without human knowledge . 
 Robotics : Robot Learns to Flip Pancakes 
 Robotics manipulation 
 and autonomous 
 systems : eg . elevator 
 optimization , 
 humanoid robots , 
 emergency response 
 robots , self - driving 
 vehicles , etc . 
 Video : https://www.youtube.com/watch?v=bxtPyJqVrmk 
 P. , et . al . 
 Robot motor skill coordination with EM - based Reinforcement Learning . 
 Injured robots learn to limp 
 Video : https://www.youtube.com/watch?v=KFDMm666QBU 
 , et . al . 
 Robots that can adapt like animals . 
 Supply chain and manufacturing : Amazon Robotics uses RL to optimize 
 warehouse robots for picking and placing items efficiently . 
 ï‚§ Energy : Deepmind â€™s RL reduce Google Data center cooling by 40 % 
 ï‚§ NLP : ChatGPT ( OpenAI ) uses RL from Human Feedback ( RLHF ) to fine - tune 
 responses , making them coherent and context - aware . 
 ï‚§ Recommender systems : example : online advertisements . 
 Basically , any complex dynamical system that is difficult to model analytically can 
 be application of RL 
 A reward ğ‘… is a scalar feedback signal 
 ğ‘¡ 
 ï‚§ Indicates how well agent is doing at step ğ‘¡ â€” defines the goal 
 ï‚§ The agent â€™s job is to maximize cumulative reward ( we call this the return ) 
 ğº = ğ‘… + ğ‘… + ğ‘… + â‹¯ 
 ğ‘¡ ğ‘¡+1 ğ‘¡+2 ğ‘¡+3 
 ï‚§ Reinforcement learning is based on the reward hypothesis : 
 Actions may have long term consequences 
 ï‚§ Reward may be delayed 
 ï‚§ It may be better to sacrifice immediate reward to gain more long - term reward 
 The history is the full sequence of observations , actions , rewards 
 â„‹ = ğ‘‚ , ğ´ , ğ‘… , ğ‘‚ , â‹¯ , ğ‘‚ , ğ´ , ğ‘… , ğ‘‚ 
 ğ‘¡ 0 0 1 1 ğ‘¡âˆ’1 ğ‘¡âˆ’1 ğ‘¡ ğ‘¡ 
 ï‚§ i.e. all observable variables up to time t 
 ï‚§ For instance , the sensorimotor stream of a robot 
 ï‚§ What happens next depends on the history : 
 The environment state ğ‘† ğ‘’ is 
 ğ‘¡ 
 the environment â€™s internal 
 representation 
 ï‚§ 
 The environment state is 
 usually invisible to the agent 
 ï‚§ 
 An information state ( a.k.a . state ) contains all useful information from 
 the history . 
 Suppose the agent sees the full 
 environment state 
 ï‚§ observation = environment state 
 ï‚§ The agent state could just be this 
 observation : 
 Partially Observable Environments 
 ï‚§ 
 Partial observability : The observations are not Markovian 
 ï‚§ 
 A robot with camera vision is nâ€™t told its absolute location 
 ï‚§ 
 A trading agent only observes current prices 
 ï‚§ Now agent state â‰  environment state 
 ï‚§ Formally this is a partially observable decision process ( POMDP ) 
 A policy defines the agent â€™s behaviour 
 ï‚§ It is a map from agent state to action 
 ï‚§ 
 Value Function 
 ï‚§ Value function is a prediction of future reward 
 ï‚§ Used to evaluate the goodness / badness of states 
 ï‚§ And therefore to select between actions , e.g. 
 ğ‘£ ğ‘  = ğ”¼ ğ‘… + ğ›¾ğ‘… + ğ›¾2ğ‘… + â‹¯|ğ‘† = ğ‘  , ğœ‹ 
 ğœ‹ ğ‘¡+1 ğ‘¡+2 ğ‘¡+3 ğ‘¡ 
 ï‚§ We introduced a discount factor ğ›¾ âˆˆ 0,1 
 ï‚§ Trades off importance of immediate vs long - term rewards 
 ï‚§ 
 The value depends on a policy ğœ‹ 
 ï‚§ The goal is to maximize value , by picking suitable actions 
 ï‚§ 
 Rewards and values define utility of states and action ( no supervised feedback ) 
 ï‚§ 
 A model does not immediately give us a good policy - we would still need to 
 plan 
 We could also consider stochastic ( generative ) models 
 Pick actions on joystick , see pixels 
 and scores 
 ï‚§ Rules of the game are known 
 All components are functions 
 ï‚§ Policies : ğœ‹ : ğ’® â†’ ğ’œ ( or to probabilities over ğ’œ ) 
 ï‚§ Value functions : ğ‘£ : ğ’® â†’ â„ 
 ï‚§ Models : ğ‘š : ğ’® â†’ ğ’® and/or ğ‘Ÿ:ğ’® â†’ â„ 
 ï‚§ State update : ğ‘¢ : ğ’® Ã— ğ’ª â†’ ğ’® 
 ï‚§ E.g. , we can use neural networks , and use deep learning techniques to learn 
 ï‚§ 
 ï‚§ Reinforcement learning is like trial - and - error learning 
 ï‚§ The agent should discover a good policy 
 ï‚§ ... from its experiences of the environment 
 ï‚§ ... without losing too much reward along the way 
 ï‚§ Exploration finds more information about the environment 
 ï‚§ Exploitation exploits known information to maximize reward 
 ï‚§ 
 ï‚§ Restaurant Selection 
 ï‚§ Exploitation Go to your favorite restaurant 
 ï‚§ Exploration Try a new restaurant 
 ï‚§ Online Banner Advertisements 
 ï‚§ Exploitation Show the most successful advert 
 ï‚§ Exploration Show a different advert 
 ï‚§ Oil Drilling 
 ï‚§ Exploitation Drill at the best known location 
 ï‚§ Exploration Drill at a new location 
 ï‚§ Game Playing 
 ï‚§ Exploitation Play the move you currently believe is best 
 ï‚§ 
 Exploration 
 Try a new strategy 
 Prediction and Control 
 ï‚§ Prediction : evaluate the future ( for a given policy ) 
 ï‚§ Control : optimize the future ( find the best policy ) 
 ï‚§ 
 These can be strongly related : 
 ğœ‹ ğ‘  = argmax ğ‘£ ğ‘  
 âˆ— ğœ‹ 
 ğœ‹ 
 ï‚§ If we could predict everything do we need anything else ? 
 ï‚§ What is the optimal policy ?