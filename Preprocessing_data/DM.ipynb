{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from Preprocessing_data.pdf_loader_pre import CustomPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importnb import Notebook\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "with Notebook():\n",
    "    import Preprocessing_data.module_loader as module_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 20:36:04,785 - INFO - Starting module PDF loading with advanced NLP preprocessing...\n",
      "2025-04-22 20:36:04,787 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Classification (part 1).pptx.pdf\n",
      "2025-04-22 20:36:04,787 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:37:25,748 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:37:35,877 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:37:41,491 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Classification (part 2).pdf\n",
      "2025-04-22 20:37:41,491 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:38:19,821 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:38:26,270 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:38:30,766 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 1).pdf\n",
      "2025-04-22 20:38:30,766 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:39:19,859 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:39:26,831 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:39:31,346 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 2).pptx.pdf\n",
      "2025-04-22 20:39:31,346 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:40:16,504 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:40:22,101 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:40:25,133 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 3).pdf\n",
      "2025-04-22 20:40:25,133 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:40:55,002 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:40:59,218 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:41:02,100 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 4).pdf\n",
      "2025-04-22 20:41:02,100 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:41:24,390 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:41:28,554 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:41:31,553 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Data - Part 1.pdf\n",
      "2025-04-22 20:41:31,553 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:42:38,156 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:42:51,722 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:43:00,913 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Data - Part 2.pdf\n",
      "2025-04-22 20:43:00,913 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:43:49,541 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:43:59,673 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:44:04,101 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Dimensionality Reduction (part 1).pdf\n",
      "2025-04-22 20:44:04,101 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:44:52,195 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:45:00,946 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:45:07,655 - INFO - âž¡ï¸ Processing PDF: ../DATA/3rd_year/Data_mining\\Dimensionality Reduction (part 2).pdf\n",
      "2025-04-22 20:45:07,655 - INFO - Extracting text using pytesseract OCR...\n",
      "2025-04-22 20:46:04,529 - INFO - Cleaning text semantically...\n",
      "2025-04-22 20:46:15,117 - INFO - Removing names via transformer NER...\n",
      "2025-04-22 20:46:24,054 - INFO - âœ… Completed loading 10 PDF(s) with NLP preprocessing.\n",
      "2025-04-22 20:46:24,070 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Classification (part 1).pptx_extracted.txt\n",
      "2025-04-22 20:46:24,070 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Classification (part 2)_extracted.txt\n",
      "2025-04-22 20:46:24,070 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Clustering (part 1)_extracted.txt\n",
      "2025-04-22 20:46:24,070 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Clustering (part 2).pptx_extracted.txt\n",
      "2025-04-22 20:46:24,090 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Clustering (part 3)_extracted.txt\n",
      "2025-04-22 20:46:24,094 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Clustering (part 4)_extracted.txt\n",
      "2025-04-22 20:46:24,098 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Data - Part 1_extracted.txt\n",
      "2025-04-22 20:46:24,110 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Data - Part 2_extracted.txt\n",
      "2025-04-22 20:46:24,121 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Dimensionality Reduction (part 1)_extracted.txt\n",
      "2025-04-22 20:46:24,126 - INFO - ðŸ“ Saved: Dataaaaaaa/texts\\Dimensionality Reduction (part 2)_extracted.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 10 documents.\n"
     ]
    }
   ],
   "source": [
    "module_directory = r\"../DATA/3rd_year/Data_mining\"\n",
    "module_loader = module_x.ModulePDFLoader(module_directory, ocr_lang=\"eng\")\n",
    "module_loader.load_module_pdfs()\n",
    "module_loader.save_all_texts('Data_mining/texts')\n",
    "\n",
    "\n",
    "documents = module_loader.documents\n",
    "texts = [doc.page_content for doc in documents]\n",
    "print(f\"Extracted text from {len(texts)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter_DM = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs_DM = splitter_DM.create_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export variables\n",
    "__all__ = ['split_docs_DM']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Classification ( Part1 ) } \\n\\n & \\n Chapter Overview \\n\\n W introduction to Classification \\n\\n _ J Decision Tree Induction \\n LJ Introduction to Decision tree \\n LJ Attribute Test Conditions \\n J Impurity Measures and Splitting Strategies \\n LJ Gain Ratio \\n\\n What is classification ? \\n\\n Input Output \\n Attribute set = Â» a â€” Â» \\n Class label \\n ( x ) ( y ) \\n\\n e Data Instances'), Document(metadata={}, page_content='e Data Instances \\n\\n â€” Attributes : Descriptive features of instances ( x ) . \\n â€” Class Labels : Categorical labels representing the class of an instance ( Y ) . \\n\\n e Classification \\n\\n â€” Assigning class labels to instances based on attributes . \\n\\n Â° \\n Classifier \\n â€” A function f(x ) used to classify unseen data x by assigning a class label y. \\n\\n Classification example \\n\\n os \\n E ky \\n\\n Assistant Prot Tenured ? \\n Merlisa |Associate Prof'), Document(metadata={}, page_content='/|Assistant Prof eS \\n Attributes Class labels \\n Applications of classification \\n\\n Health \\n ~ Medical diagnosis \\n â€” ~ Patient risk categorization \\n\\n Computer Security \\n ~ Spam filtering \\n ~ Malware classification \\n\\n Banking and Finance \\n ~ Loan default prediction \\n ~ Credit card fraud detection \\n\\n Retail and E - commerce \\n ~ Customer purchase pattern classification \\n ~ Sentiment analysis from customer reviews'), Document(metadata={}, page_content='Transportation and Logistics \\n â€” Cargo classification for customs \\n â€” Driver behavior classification for insurance \\n\\n Role of Classification Models \\n\\n Â¢ Predictive Models \\n\\n â€” Used to predict class labels for new , unseen data . \\n â€” Learn patterns from historical data to make predictions . \\n\\n e Descriptive Models \\n\\n â€” Help understand distinguishing features of different classes . \\n â€” Analyzes the data to find common characteristics and \\n patterns . \\n Role of Classification Models'), Document(metadata={}, page_content='Â¢ Predictive Models \\n\\n â€” Example : Classifying email messages as â€˜ urgent â€™ or \\n â€˜ non - urgent â€™ based on their content . \\n\\n e \\n Descriptive Models \\n\\n â€” Example : \\\\dentifying key factors that differentiate \\n high - risk patients in healthcare . \\n\\n\\n General Framework for Classification \\n\\n e Induction ( Training ) \\n\\n Training Set \\n\\n Learming \\n Algorithm \\n\\n â€” Learning a model from a labeled training \\n dataset . \\n\\n â€” Use learning algorithm to build models . \\n Â¢ Deduction ( Testing ) \\n\\n oy \\n o'), Document(metadata={}, page_content=\"oy \\n o \\n\\n â€” Applying the learned model to new \\n instances ( unseen ) to predict class \\n labels . \\n\\n â€” Assessing the model 's performance to \\n measure its generalization capability . \\n The feedback from testing is often used to \\n refine and improve the training process . \\n Chapter Overview \\n\\n LJ Introduction to Classification \\n\\n LJ Decision Tree Induction \\n Mi Introduction to Decision Tree \\n LJ Attribute Test Conditions \\n _ J Impurity Measures and Splitting Strategies \\n LJ Gain Ratio\"), Document(metadata={}, page_content='Decision Tree \\n\\n Bod \\n * Root Node ae â€ _ _ Root \\n â€” Initial point of decision - making . \\n node \\n\\n Internal Warm Cold \\n node \\n\\n Â¢ Internal Nodes â„¢ \\n â€” Question based on a single attribute ( ome \\n â€” Attribute test . \\n Non- | \\n\\n Han | \\n\\n e Leaf Nodes \\n â€” The final classification outcome . \\n\\n mammals \\n\\n Mammal classification tree \\n\\n 10 \\n\\n Deduction in Decision Trees \\n\\n e Start at Root Node \\n\\n ae , Unlabeled Rete tempenstoen:_| |i ) Cheon |'), Document(metadata={}, page_content='â€” Apply initial attribute test . \\n [= \\n â€” Follow Test Outcome . \\n a | zs \\n Temperature mamma \\n Visit next node \\n\\n â€” Follow Test Outcome . \\n Â¢ Reach Leaf Node \\n\\n â€” Determine final classification . \\n Mammal classification tree \\n 11 \\n\\n Hunt â€™s Algorithm for decision tree building \\n\\n Â¢ Initial Node \\n â€” Start with a root node containing all instances . \\n All instances \\n\\n 12 \\n\\n Hunt â€™s Algorithm for decision tree building'), Document(metadata={}, page_content='Hunt â€™s Algorithm for decision tree building \\n\\n Â¢ Initial Node \\n â€” Start with a root node containing all instances . \\n All instances \\n\\n Â¢ Expansion and Child Nodes Formation \\n\\n â€” - Expand nodes with mixed class instances . \\n - Select the attribute test based on a splitting Attribute 1 ? \\n criterion . \\n yes No \\n 0 % yes , 100 % No 40 % yes , 60 % No \\n Class = No Class = No \\n\\n â€” Create child nodes for each test outcome \\n - Distribute instances accordingly . \\n\\n 13'), Document(metadata={}, page_content='13 \\n\\n Hunt â€™s Algorithm for decision tree building \\n\\n Â¢ Initial Node \\n â€” Start with a root node containing all instances . \\n Attribute 1 ? \\n V1 V2 \\n\\n Defaulted = No \\n\\n Â¢ Expansion and Child Nodes Formation \\n â€” - Expand nodes with mixed class instances . \\n - Select the attribute test based on a splitting \\n criterion . \\n - Create child nodes for each test outcome \\n - \\n Distribute instances accordingly . \\n Attribute 2 ?'), Document(metadata={}, page_content='Â¢ Recursive Process \\n â€” Continue expansion for nodes with mixed \\n class instances . \\n\\n 100 % yes , 0 % No \\n Class = Yes \\n\\n 0 % yes , 100 % No \\n Class = No \\n\\n\\n Hunt â€™s Algorithm for decision tree building \\n\\n Â¢ Initial Node \\n â€” Start with a root node containing all instances . \\n Attribute 1 ? \\n V1 V2 \\n\\n Defaulted = No'), Document(metadata={}, page_content='Defaulted = No \\n\\n Â¢ Expansion and Child Nodes Formation \\n â€” - Expand nodes with mixed class instances . \\n - Select the attribute test based on a splitting \\n criterion . \\n - Create child nodes for each test outcome \\n - Distribute instances accordingly . \\n No [ Divorced J95 K [ Yes \\n Defaulted = No Defaulted = Yes \\n Decision tree algorithm questions'), Document(metadata={}, page_content='How to handle an empty test outcome ? \\n All attributes values are identical BUT different class labels ? \\n How to determine the best attribute test ? \\n\\n What are the stopping criteria for the algorithm ? \\n\\n How to handle an empty test outcome ? \\n When Does This Occur ? \\n Â¢ No training instances with specific attribute values . \\n Â¢ These attribute values can happen in testing instances . \\n Approach'), Document(metadata={}, page_content='Assign the most common class label from parent node to \\n empty nodes . \\n All attributes values are identical BUT \\n different class labels ? \\n When Does This Occur ? \\n Â¢ Expansion is impossible . \\n Â¢ We ca nâ€™t build leaves contains the same class . \\n Approach \\n\\n Declare it a leaf node and assign it the most common class \\n label in the training instances associated with this node . \\n 23 \\n\\n Chapter Overview \\n\\n LJ Introduction to Classification'), Document(metadata={}, page_content='LJ Introduction to Classification \\n\\n LJ Decision Tree Induction \\n LJ Introduction to Decision Tree \\n MM Attribute Test Conditions \\n _ J Impurity Measures and Splitting Strategies \\n\\n _ J Gain Ratio \\n\\n 24 \\n How to select the optimal grouping ? \\n 38 \\n\\n Attribute Test Conditions & Attribute type'), Document(metadata={}, page_content=\"Attribute Test Conditions & Attribute type \\n\\n â€” Binary or multiway splits { Small , { Large , { Small } { Medium , Large , \\n . . . Medium } Extra Large } Extra Large } \\n ( like nominal attributes ) . \\n â€” Binary split : grouping should ' \\n not violate the order . \\n How many possible grouping ? \\n Â¢ Ordinal Attributes \\n\\n { Small , { Medium , \\n Large } Extra Large } \\n\\n > \\n\\n 29 \\n\\n\\n Attribute Test Conditions & Attribute type \\n\\n Continuous Attributes \\n\\n e Binary Split \\n\\n O \\n\\n O \\n\\n O\"), Document(metadata={}, page_content='e Binary Split \\n\\n O \\n\\n O \\n\\n O \\n\\n Annual \\n Income \\n Comparison test ( A < V ) . \\n Discretization to convert to ordinal attribute . \\n Test condition defined like ordinal attribute . \\n â€” Pure nodes helps to stop expanding nodes . \\n â€” Impure nodes need more expansions , deepening the tree . \\n e Concerns with Larger Trees \\n â€” Susceptible to overfitting . \\n â€” Harder to interpret . \\n â€” Longer training and testing times . \\n What is pure nodes ? \\n Pure set VS Impure set'), Document(metadata={}, page_content='Â¢ Pure node \\n â€” Contains the same class label . \\n â€” Maximum impurity occurs when class \\n\\n labels are equally probable . \\n Maximum impurity for equally \\n distributed classes . \\n 37 \\n\\n GClass = I \\n Splits a node with N instances into k children { v1 , v2 , ... , vk } . \\n\\n Â° N(v ;) : Number of instances in child node vj . \\n Â° I(v ;) : Impurity value of node vj . \\n\\n Collective impurity of child nodes : \\n Weighted sum of node children impurities . \\n\\n AQ'), Document(metadata={}, page_content='AQ \\n\\n Which split is better ? \\n / Marital \\n \\\\ Status _ / \\n\\n 41 \\n\\n Which split is better ? \\n\\n / \\n I(Home Owner = yes ) = 0 ie QO 3 ik 3 5 I(Marital Status = Single ) = \\n Identifying the best attribute test condition \\n\\n Compare parent node impurity ( I(parent ) ) before splitting with after splitting . \\n A = I(parent)-I(children ) \\n\\n Large ( A ) = > better attribute test condition . \\n A_info : information gain when entropy is used .'), Document(metadata={}, page_content='I(parent)2I(children ): Gain is always non - negative . \\n Decision trees select conditions with maximum gain for splitting . \\n Maximizing gain is equivalent to minimizing weighted child impurity . \\n The complexity of finding the best binary split is O(n ) . \\n Synthesis of Attribute test selection \\n\\n Selecting the best Attribute Test for Node Expansion : \\n\\n 1 . Identify the best split ( if more than one split ) for each attribute \\n using A = I(parent)-I(children ) .'), Document(metadata={}, page_content='2 . Select the best attribute by comparing A = I(parent)-I(children ) \\n across all the attributes . \\n Limitation of the impurity measure A \\n\\n What is the A(ID ) ? \\n\\n 50'), Document(metadata={}, page_content=\"Outline \\n\\n Characteristics of Decision Trees \\n Model Overfitting \\n\\n Model Evaluation and selection \\n\\n Uuwuww gg \\n\\n Conclusion \\n\\n Characteristics of Decision Tree - Applicability \\n\\n Nonparametric Approach \\n No prior assumptions on data 's probability distribution . \\n Wide Applicability \\n\\n o \\n Suitable for categorical and continuous datasets . \\n No Data Transformation \\n\\n o \\n Attributes can be used without binarization , normalization , or standardization . \\n Multiclass Problem Handling\"), Document(metadata={}, page_content='o \\n Handel multiclass without reducing them to binary tasks . \\n interpretability \\n\\n o \\n Trained trees are easy to understand ( particularly shorter ones ) . \\n Competitive Accuracy \\n\\n o \\n The result are comparable with other techniques for many simple data sets . \\n Characteristics of Decision Tree -Expressiveness \\n\\n e Universal Representation \\n o \\n Tree can encode any function of discrete - valued attributes . \\n e Efficient Encoding'), Document(metadata={}, page_content='o \\n Discrete - valued function can be represented as an assignment table . \\n o Decision tree can represent the assignment table efficiently . \\n o Decision tree can group a combinations of attributes as leaf nodes . \\n e Limitations \\n\\n o \\n Some functions , like the parity function , require a full decision tree for \\n\\n accurate modeling . \\n Example of Compact Representation'), Document(metadata={}, page_content='Boolean function ( AAB)V(CAD ) using a simpler tree with fewer leaf nodes , \\n instead of a fully - grown tree . \\n e Simplifies complex multidimensional data into understandable segments . \\n e Effective in handling both categorical and continuous variables . \\n Characteristics of Decision Tree - Rectilinear Splits \\n\\n What are the disadvantages of rectilinear splits ? \\n Characteristics of Decision Tree - Rectilinear Splits \\n\\n Disadvantages of rectilinear splits'), Document(metadata={}, page_content='Disadvantages of rectilinear splits \\n\\n e Struggle with Non - linear Boundaries : \\n o Ineffective in capturing complex , non - linear \\n relationships in data . \\n\\n e Limited Flexibility : \\n o Restricts decision boundaries to orthogonal \\n lines , limiting flexibility . \\n e Oversimplification Risks : \\n o Can lead to oversimplified models that fail to \\n capture the true nature of the data . \\n\\n\\n Outline \\n\\n Characteristics of Decision Trees \\n Model Overfitting \\n\\n Model Evaluation and selection'), Document(metadata={}, page_content='Model Evaluation and selection \\n\\n Uu gg Uw \\n\\n Conclusion \\n\\n Model Overfitting \\n\\n Overfitting occurs when a model fits \\n training data too closely , leading to \\n\\n poor generalization . \\n A overfitted model may perform well on \\n\\n training data but poorly on test data . \\n\\n Training vs Test Error : \\n As tree size \\n increases , training error may decrease , \\n\\n but test error eventually increases . \\n\\n 09 \\n 0.85 \\n 08 \\n\\n 0.75 \\n\\n < \\n\\n 0.65 \\n\\n 0.6 \\n\\n 0.55 \\n\\n 05'), Document(metadata={}, page_content='0.75 \\n\\n < \\n\\n 0.65 \\n\\n 0.6 \\n\\n 0.55 \\n\\n 05 \\n\\n On training data â€” â€” \\n On test data --~~ \\n\\n 30 40 50 60 70 80 \\n Size of tree ( number of nodes ) \\n Causes of Overfitting \\n\\n e Limited Training Size : \\n o \\n Asmall training set may not represent \\n true patterns , leading to overfitting . \\n e High Model Complexity : \\n Overly complex models can capture \\n training - specific patterns , reducing \\n\\n . . \\n Appropriate - fitting Over - fitting \\n generalizability . \\n ( too simple to ( forcefitting -- too'), Document(metadata={}, page_content=\"explain the variance ) good to be true ) HG \\n e Spurious Patterns Recognition : \\n Models may learn irrelevant patterns \\n present in training data ( Ex . noise ) , which \\n do n't generalize to new data . \\n Overfitting vs Underfitting \\n\\n Underfitting : \\n Simple models may fail to capture essential patterns . \\n Data scientist challenge\"), Document(metadata={}, page_content='Find a model that does not overfit or underfit \\n Under - fitting Appropriate - fitting Over - fitting \\n ( too simple to ( forcefitting -- too \\n explain the variance ) good to be true ) HG \\n\\n\\n Dealing with overfitting - Pruning \\n\\n Pruning : cutting away branches that may be based on noisy \\n or misleading data to prevent overfitting .'), Document(metadata={}, page_content='e \\n Occurs during tree construction . \\n o Limits tree growth by limiting the maximum depth or \\n minimum leaf size . \\n o Prevents overfitting by avoiding overly complex models . \\n e Post pruning : Applied after the tree is fully grown . \\n o Removes branches that contribute little to classification \\n accuracy . \\n o Reduces model complexity , enhancing generalization to \\n new data .'), Document(metadata={}, page_content='Pruning \\n = \\n Fane aay ? \\n Ensure robust model evaluation . \\n Labeled Test Set \\n Utilize a separate test set , not involved in model \\n building , for unbiased evaluation . \\n Holdout Method \\n\\n e ) \\n e ) \\n\\n Randomly split data into training and test sets . \\n Use the test set to estimate generalization error . \\n Cross - Validation Method \\n\\n O'), Document(metadata={}, page_content='O \\n\\n Divide data into multiple subsets ; train and test the \\n model on different subsets for a comprehensive \\n performance estimate . \\n GOOD ? \\n MODEL PQ \\n = \\n Evaluation \\n\\n BAD ? \\n Holdout Method \\n\\n Basic technique to partition data into training \\n ( D.train ) and testing ( D.test ) sets . \\n Error Estimation \\n\\n DATASET \\n Calculate error rate on D.test ( errtest ) as a measure of \\n\\n generalization error . \\n Data Proportion \\n\\n o \\n Analysts decide the split ratio . \\n J'), Document(metadata={}, page_content='o \\n Analysts decide the split ratio . \\n J \\n\\n Trade - offs Train Model Evaluate Model \\n o \\n Balancing D.train size for model learning and D.test size for \\n reliable error estimation . \\n Repeated Holdout Method \\n\\n o Enhances reliability by repeating the process and averaging \\n error rates . \\n Model selection and validation set \\n Achieve an optimal balance between \\n model complexity and performance . \\n Complexity Measurement \\n\\n O'), Document(metadata={}, page_content='O \\n\\n Complexity can be measured by the ratio of leaf \\n nodes to training instances . \\n Limitation of Training Error : \\n\\n O \\n\\n Training error rate is insufficient for effective \\n model selection . \\n Validation Set : \\n\\n O \\n\\n Essential for assessing generalization error . \\n Model Selection Strategy : \\n\\n O \\n\\n Combine complexity with validation set \\n performance to select the most effective model . \\n â€˜ -s = - â€” \\n Single Dataset \\n B ising ) verceton Tea \\n Single Dataset \\n\\n 20 \\n\\n Cross validation'), Document(metadata={}, page_content=\"20 \\n\\n Cross validation \\n\\n e Cross validation helps to avoid the split baise . \\n Divide data into k equal folds . \\n e Each instance is used exactly once for error . \\n calculation . \\n Hi Test Set \\n e The error is calculated based on : ? | | training se \\n i= \\n N \\n\\n e \\n What if some classes do n't appear in some \\n folds ? \\n Cross validation \\n\\n Stratified Sampling \\n Ensures equal representation of classes in \\n each partition . \\n Leave - One - Out Approach \\n\\n O \\n\\n O\"), Document(metadata={}, page_content='O \\n\\n O \\n\\n A special case where each instance is \\n used once as a test set . \\n K = N \\n Estimating Error Variance \\n Repeating cross - validation with different \\n partitions provides robust error estimates . \\n Stratified K - fold \\n [ _ ] Training Data \\n [ ) Validation Data \\n\\n 23 \\n\\n Classification evaluation metrics \\n\\n e Accuracy \\n\\n o Proportion of correctly predicted instances to total instances . \\n Predicted \\n\\n 0 \\n\\n Predicted \\n\\n e Precision'), Document(metadata={}, page_content='0 \\n\\n Predicted \\n\\n e Precision \\n\\n o Ratio of true positives to total predicted positives . \\n\\n e Recall ( Sensitivity ) \\n\\n o \\n Ratio of true positives to actual positives . \\n o Harmonic mean of precision and recall \\n e Confusion Matrix \\n\\n o Visual tool categorizing true and false positives and \\n negatives . \\n Classification evaluation metrics \\n\\n Predicted Class \\n\\n ws False Negative ( FN ) Sensitivity \\n True Positive ( TP ) - - _ aR \\n ype Il Error ETT \\n\\n Actual Class'), Document(metadata={}, page_content='Actual Class \\n\\n s False Positive ( FP ) Specificity \\n Negative 1 IE True Negative ( TN ) TN \\n ype I Error FRE \\n\\n Negative Predictive Accuracy \\n\\n Precision \\n Value TP + TN \\n\\n TP \\n ( TP + FP ) \\n\\n ( TN + FN ) \\n\\n TN ( TP + TN + FP + FN ) \\n\\n 25 \\n Outline \\n\\n Characteristics of Decision Trees \\n Model Overfitting \\n\\n Model Evaluation and selection \\n\\n mw coo \\n\\n Conclusion'), Document(metadata={}, page_content='mw coo \\n\\n Conclusion \\n\\n 26 \\n Conclusion \\n Remember , the journey in data science is a continuous battle \\n against overfitting and underfitting . \\n Stay vigilant !'), Document(metadata={}, page_content='Clustering \\n Hierarchical Clustering \\n LJ DBSCAN Clustering \\n\\n LJ Cluster Evaluation \\n What is Clustering ? \\n = Given a set of objects , place them in groups such that : \\n\\n The objects in a group are similar ( or related ) to one another and different \\n from ( or unrelated to ) the objects in other groups \\n\\n\\n Think of some practical applications of clustering ? \\n Applications of Clustering'), Document(metadata={}, page_content='Marketing : Discover customer segments for targeted marketing \\n Information retrieval : Document clustering \\n\\n Land use : Identifying similar land use areas in an Earth database \\n Biology : Taxonomy levels ( kingdom to species ) \\n\\n City planning : Grouping houses by type , value , and location \\n Earthquake studies : Clustering observed epicenters along fault lines \\n Climate : Analyzing atmospheric and ocean patterns \\n\\n Economic science : Market research \\n\\n Clustering as Preprocessing tool'), Document(metadata={}, page_content='Clustering as Preprocessing tool \\n\\n = Summarization : \\n . Preprocessing for classification , regression , PCA , and association analysis \\n . \\n Compression : \\n . \\n Image processing using vector quantization \\n . \\n Finding K - nearest Neighbors \\n . \\n Outlier detection \\n\\n . \\n Outliers are often viewed as those â€œ far away â€ from any cluster \\n\\n What is a good clustering and what are the factors \\n that contribute to it ?'), Document(metadata={}, page_content='What is a Good Clustering ? \\n high intra - class similarity : cohesive within clusters \\n . \\n low inter - class similarity : distinctive between clusters \\n : The quality of a clustering method depends on : \\n . \\n the similarity measure used by the method \\n . \\n the implementation of the clustering method \\n\\n . \\n the ability to discover some or all of the hidden patterns \\n\\n What is a Good Clustering ? \\n Similarity is expressed in terms of a distance function d(i , j )'), Document(metadata={}, page_content=': \\n â€” The definitions of distance functions depend on the attribute type : \\n\\n boolean , categorical , interval - scaled , ordinal ratio , and vector variables \\n\\n : | Weights should be associated with different attributes based on the \\n\\n domain application and data semantics \\n = Quality of clustering \\n . \\n There is a â€œ quality â€ function that measures the â€œ goodness â€ of a cluster \\n\\n . \\n Distance - based ( e.g. , Euclidean , road network , vector )'), Document(metadata={}, page_content='Â« Connectivity - based ( e.g. , density or contiguity ) \\n Clustering space \\n\\n . \\n Full space ( often when low dimensional ) \\n\\n = Subspaces ( often in high - dimensional clustering ) \\n\\n 11 \\n\\n Notion of a Cluster can be Ambiguous \\n How many clusters ? \\n Notion of a Cluster can be Ambiguous \\n\\n * S e + \\n * Â® O \\n ee Â° eee â€ vv Â° oo \\n & ee V o9Â¢ \\n How many clusters ? \\n Requirements and Challenges'), Document(metadata={}, page_content='Interpretability \\n Â« = Explain and use the different clusters \\n Scalability \\n . \\n Clustering all the data instead of samples \\n Deal with different types of attributes \\n Â« = Numerical , binary , categorical , ordinal , linked , and a mixture of these \\n Constraint - based clustering \\n . \\n User may give inputs on constraints \\n Â« Use domain knowledge to determine input parameters \\n Others \\n : Ability to deal with noisy data and outliers \\n\\n . \\n Ability to detect clusters of any shape \\n\\n 14'), Document(metadata={}, page_content='14 \\n\\n Outline \\n\\n LJ Overview of Clustering \\n\\n LJ Major Clustering Approaches \\n LJ K - means Clustering \\n LI Hierarchical Clustering \\n LJ DBSCAN Clustering \\n\\n LJ Cluster Evaluation \\n\\n Major Clustering Approaches \\n\\n Agglomerative \\n\\n Divisi Distance Model \\n siesta Based Based \\n\\n Density \\n Based \\n\\n Probabilistic \\n\\n 16 \\n\\n Partitional vs Hierarchical Clustering \\n\\n Hierarchical Clusterina Partitional Clusterina \\n\\n Nested clusters Non - nested clusters \\n\\n\\n Major Clustering Approaches'), Document(metadata={}, page_content='Partitioning approach : \\n . \\n Construct various partitions and then evaluate them by some criterion \\n . \\n lypical methods : K - means , K - medoids , CLARANS \\n Hierarchical approach : \\n . \\n Create a hierarchical decomposition of the set of data using some criterion \\n . \\n Typical methods : Diana , Agnes , BIRCH , CAMELEON \\n Density - based approach : \\n . \\n Based on connectivity and density functions ( detect regions where points are concentrated )'), Document(metadata={}, page_content='Â« lypical methods : DBSCAN , OPTICS , DenClue \\n Model - based : \\n - Amodel is hypothesized for each of the clusters and tries to find the best fit'), Document(metadata={}, page_content='. \\n Jypical methods : EM , SOM , COBWEB \\n\\n 18 \\n\\n Outline \\n\\n LJ Overview of Clustering \\n\\n LJ Major Clustering Approaches \\n\\n L } \\n Clustering \\n LI Hierarchical Clustering \\n\\n LJ DBSCAN Clustering \\n\\n LJ Cluster Evaluation \\n\\n Partitional Algorithms \\n Â« Objective : Partitioning a database D of n objects into a set of K clusters . \\n Â« Â» \\n K - Means algorithm is an example of a partitional clustering algorithm . \\n = Example of clustering data points with K=3 : \\n\\n After clustering'), Document(metadata={}, page_content='After clustering \\n\\n 20 \\n Which objective function should be used ? \\n Objective Function \\n A common objective function is minimize the Sum of Squared Distances ( SSE ) \\n SSE is used with the Euclidean distance measure \\n K \\n SSE = > > dist â€™ ( m , , x ) \\n i = l xeC ; \\n . \\n Xis a data point in cluster C , and m , is the centroid or medoid for cluster C , \\n - For each point x , the error is the distance to the nearest cluster center m , \\n . \\n 1oget SSE , we square these errors and sum them .'), Document(metadata={}, page_content='= \\n SSE improves in each iteration until it reaches a local or global minima . \\n Recompute the centroid of each cluster . \\n\\n 5 : until The centroids do nâ€™t change \\n\\n 23 \\n Often terminates at a local optimal \\n\\n . \\n Sensitive to noisy data and outliers \\n\\n . \\n Not suitable to discover clusters with non - convex shapes \\n\\n 26 \\n How to improve the K - Means algorithm ? \\n Variations of the K - Means Algorithm'), Document(metadata={}, page_content='= \\n Most of the variants of the k - means differ in : \\n = Selection of the initial kK means \\n = \\n Dissimilarity calculations \\n : Strategies to calculate cluster means \\n\\n Â« = Handling categorical data with k - modes : \\n Replacing means of clusters with modes \\n . \\n Using new dissimilarity measures to deal with categorical objects \\n . \\n Using a frequency - based method to update modes of clusters \\n\\n = Amixture of categorical and numerical data : k - prototype method \\n\\n 28 \\n\\n Outline'), Document(metadata={}, page_content='28 \\n\\n Outline \\n\\n L } Overview of Clustering \\n\\n L } Major Clustering Approaches \\n Li } K - means Clustering \\n 1 Hierarchical Clustering \\n Li DBSCAN Clustering \\n\\n LJ Cluster Evaluation \\n\\n 29'), Document(metadata={}, page_content='Clustering \\n Outline \\n\\n Li Overview of Clustering \\n\\n L } Major Clustering Approaches \\n Li K - means Clustering \\n LJ Hierarchical Clustering \\n Li DBSCAN Clustering \\n\\n LJ Cluster Evaluation \\n\\n Hierarchical Clustering \\n\\n Hierarchical Clustering produce a set of nested - clusters . \\n It does not have to assume any particular number of clusters . \\n It may correspond to meaningful taxonomies ( e.g. , biological \\n taxonomy , animal kingdom , phylogeny reconstruction , ... ) . \\n\\n Nested clusters'), Document(metadata={}, page_content='Nested clusters \\n\\n Hierarchical Clustering \\n\\n distance \\n The set of nested clusters can be organized as a hierarchical tree . \\n The hierarchical tree of clusters is called a dendrogram , which records the \\n sequences of merges or splits \\n\\n Different clustering of the data can be obtained by cutting the dendrogram at \\n the desired level , then each connected component forms a cluster \\n Agglomerative : \\n Start with the points as individual clusters'), Document(metadata={}, page_content='. \\n Divisive : \\n . \\n At each step , split the least cohesive clusters until each cluster \\n contains an individual point ( or there are k clusters ) \\n\\n . \\n Popular algorithm : DIANA ( Divisive Analysis ) \\n Key Idea : Successively merge the closest clusters \\n n \\n Different approaches to defining the distance between \\n clusters distinguish the different algorithms ( Min , Max , etc . ) \\n How to Define Inter - Cluster Distance \\n\\n Distance ? \\n < > \\n MIN \\n MAX â€˜ Proximity Matrix \\n Group Average'), Document(metadata={}, page_content=\"Distance Between Centroids \\n\\n How to Define Inter - Cluster Distance \\n\\n MIN \\n MAX â€˜ Proximity Matrix \\n Group Average \\n\\n Distance Between Centroids \\n\\n 10 \\n How to Define Inter - Cluster Distance \\n\\n MIN \\n MAX â€˜ Proximity Matrix \\n Group Average \\n\\n Distance Between Centroids \\n\\n 11 \\n How to Define Inter - Cluster Distance \\n\\n MIN \\n MAX ' Proximity Matrix \\n Group Average > proximity(p,,p , ) \\n\\n p,Â¢Cluster , \\n pjÂ¢Cluster , \\n\\n proximity(Cluster , , Cluster , ) =\"), Document(metadata={}, page_content='proximity(Cluster , , Cluster , ) = \\n\\n Distance Between Centroids | Cluster ; | x ] Cluster ; | \\n\\n 12 \\n Now , the question is â€œ how do we update the proximity matrix ? â€ \\n - \\n | \\n | \\n | \\n\\n p3 p4 p9 p10 p11 p12 \\n Answer : we update the proximity matrix \\n using the different approaches to defining the \\n distance between clusters ( Min , Max , etc . ) \\n Note : to compute the distance between an individual data point \\n and a cluster , we consider that data point itself as a cluster'), Document(metadata={}, page_content='Hierarchical Clustering : MIN \\n\\n Nested Clusters Dendrogram \\n\\n Strength of MIN \\n\\n TUE al \\n It detects clusters of any shape by focusing only on the nearest points \\n between clusters , ignoring overall shape . \\n o Captures irregularly shaped clusters effectively without assuming specific \\n geometrical forms like elliptical shapes . \\n\\n 22 \\n\\n Limitations of MIN \\n\\n Â® oe \" . ; nes 2 3@ \\n â€œ eo & ae of \\n\\n Â° \\n Be \\n\\n of combined clusters . \\n Â° . ge \\n 3 Â° ofe'), Document(metadata={}, page_content=\"of combined clusters . \\n Â° . ge \\n 3 Â° ofe \\n\\n Noise sensitivity : \\n A single point can alter oy \\n the cluster 's shape . \\n Hierarchical Clustering : MAX \\n\\n 0.47 \\n\\n 0.357 \\n 0.3 ; \\n 0.257 \\n\\n 0.27 \\n 0.157 \\n\\n 0.17 \\n 0.05Â¢ \\n\\n Nested Clusters Dendrogram \\n\\n Strength of MAX \\n\\n wo a2 ? \\n Less affected by noise because it looks at the \\n be influenced by outliers . \\n Limitations of MAX \\n\\n Two Clusters \\n\\n Original Points \\n\\n Tends to break large clusters into smaller , more distinct ones . \\n O\"), Document(metadata={}, page_content='Biased towards globular clusters \\n\\n O \\n\\n 26 \\n\\n Hierarchical Clustering : Group Average \\n\\n Nested Clusters Dendrogram \\n\\n Hierarchical Clustering : Group Average \\n\\n Compromise between Single and Complete Link \\n\\n Strengths \\n\\n Averaging reduces the influence of noisy data points \\n\\n Limitations \\n\\n Biased towards globular clusters because the average \\n\\n distance favors clusters with compact , closely located points \\n\\n Hierarchical Clustering : Space and Time Complexity'), Document(metadata={}, page_content='Â« = Nis the number of data points or objects . \\n = Space : O(N â€™ ) \\n m O(N â€™ ) because the proximity matrix has N ? entries for distances \\n\\n between N points . \\n = Time : O(N â€™ ) \\n a Find the min distance of the matrix O(N â€™ ) * N iterations > \\n * ) \\n = Complexity can be reduced to O(N ? log(N ) ) \\n\\n = Accelerate finding the minimum using a heap ....'), Document(metadata={}, page_content='Strength of Hierarchical Clustering \\n = Donothave to assume any particular number of clusters . \\n Any desired number of clusters can be obtained by â€˜ cutting â€™ \\n\\n the dendrogram at the proper level . \\n They may correspond to meaningful taxonomies \\n\\n .'), Document(metadata={}, page_content='Clustering \\n Outline \\n\\n L } Overview of Clustering \\n\\n LJ Major Clustering Approaches \\n LJ K - means Clustering \\n LJ Hierarchical Clustering \\n Li DBSCAN Clustering \\n\\n LJ Cluster Evaluation'), Document(metadata={}, page_content='LJ Cluster Evaluation \\n\\n Density - based Clustering \\n e Density - based methods use density to discover clusters of any shape . \\n e Density means the concentration of data points in a given region . \\n e Clusters are regions of high density separated by regions of low density . \\n e Major features : \\n = Discover clusters of arbitrary shape \\n\\n = Handle noise \\n\\n m \\n = No need to define the number of clusters in advance \\n\\n DBSCAN Clustering'), Document(metadata={}, page_content='DBSCAN Clustering \\n\\n e DBSCAN : Density - Based Spatial Clustering of Applications with Noise \\n e lt defines a cluster as a maximal set of density - connected points . \\n Density is the number of data points within a certain radius . \\n The parameters of this algorithm are : \\n Â» â‚¬ the maximum radius of the neighborhood \\n\\n s Â» \\n the minimal number of point in a dense neighborhood \\n â‚¬ -neighborhood \\n\\n e \\n The neighborhood within a radius â‚¬ of a given object is called'), Document(metadata={}, page_content='the â‚¬ -neighborhood of the object . \\n\\n\\n Core objects \\n If the Â¢-neighborhood of an object contains at least a minimum \\n\\n number of objects MinPts , then the object is called a core object . \\n Core , Border , and Noise Points \\n Acore point has at least a specified number of points ( MinPts ) within â‚¬ \\n Aborder point is not a core point , but is in the neighborhood of a core point \\n\\n e \\n Given a set of objects D , we say that an object p is directly \\n\\n density - reachable from an object q if :'), Document(metadata={}, page_content='density - reachable from an object q if : \\n\\n â€œ p is within the â‚¬ -neighborhood of g , and q is a core object . \\n Given a set of objects D , an object p is density - reachable from an \\n\\n object q with respect to â‚¬ and MinPts if : \\n\\n â€œ There is a chain of objects p , , ... , p , , where p , = q and p , = p such \\n that p. , , is directly density - reachable from p , with respect to â‚¬ and \\n\\n MinPts , for 1SiSn and p,â‚¬ D. â€ \\n\\n MinPts= 5 \\n\\n Density Connectivity'), Document(metadata={}, page_content='MinPts= 5 \\n\\n Density Connectivity \\n\\n e \\n Given a set of objects D , an object p is density - connected to \\n\\n an object q with respect to â‚¬ and MinPts if : \\n â€œ There is an object o â‚¬ D such that both p and q are \\n\\n density - reachable from o with respect to Â¢ and MinPts â€ \\n MinPts = 5 \\n\\n Density - based Clusters \\n e Adensity - based cluster is a set of density - connected points'), Document(metadata={}, page_content='that is maximal with respect to density - reachability . \\n Every object not contained in any cluster is considered noise . \\n e e \\n ie ee * . \\n > eo Pe \\n\\n SA \\n â€˜ iad Pa * \\n\\n Sore \\n\\n 4 \\n\\n\\n DBSCAN Algorithm : Intuition \\n â€œ By recursively exploring the neighborhood of core points \\n within the Â¢-distance threshold and incorporating reachable \\n points into clusters , the DBSCAN algorithm identifies dense \\n\\n regions in the dataset while also detecting outliers ( noise'), Document(metadata={}, page_content='points ) that do not fit within these dense regions . â€ \\n\\n - 2 N \\n\\n DBSCAN Algorithm'), Document(metadata={}, page_content='Choose â‚¬ ( a positive number ) and MinPoints ( a natural number ) . \\n Select an arbitrary point P from the dataset . \\n Check if point P is a core point . \\n If yes , form a cluster including P. \\n Recursively add core points within the â‚¬ -neighborhood of the already added \\n points to the cluster . \\n Noise Identification : Label points that are neither core points nor border \\n points as noise . \\n Determining MinPts \\n General guidelines for setting MinPts :'), Document(metadata={}, page_content='General guidelines for setting MinPts : \\n e Larger datasets require a larger MinPts value . \\n In noisier datasets , choose a larger MinPts value . \\n Domain knowledge is crucial to select an appropriate MinPts value . \\n 16'), Document(metadata={}, page_content=\"DBSCAN : Determining â‚¬ using K - Distance Plot \\n Plot these K - distances in ascending order . \\n e The ' knee ' in the plot represents a threshold where a sharp change in \\n\\n distance occurs . \\n This point is indicative of the optimal â‚¬ value . \\n Helps to distinguish between core , border , and noise points in the data \\n 17 \\n DBSCAN : Determining â‚¬ and MinPts\"), Document(metadata={}, page_content=\"e Idea : for points in a cluster , their Ath nearest neighbors are at close distance \\n e \\n Noise points have the Ath nearest neighbor at farther distance \\n\\n e \\n Advantages \\n Disadvantages \\n e Advantages : \\n m Can handle clusters of different shapes and sizes \\n = Resistant to noise and outliers \\n = Does n't require predefined number of clusters .\"), Document(metadata={}, page_content='Outline \\n\\n Mi Clustering evaluation \\n\\n 1 Why cluster evaluation ? \\n\\n 1 Types of cluster evaluation measures \\n LJ Unsupervised evaluation \\n\\n LJ Cohesion vs Separation \\n LI Silhouette Coefficient \\n\\n LJ Supervised evaluation \\n\\n LJ Entropy \\n\\n LJ Precision , Recall , F - measure \\n\\n Why cluster evaluation ? \\n\\n e Generate a random data points . \\n Data without any structure \\n\\n Question : \\n\\n What is the result of applying K - Means with \\n K=3 ? \\n The following link can be used : K - Means Animation'), Document(metadata={}, page_content='Why cluster evaluation ? \\n\\n e Generate a random data points . \\n Data without any structure \\n\\n Clusters found in Random Data ! ! \\n The following link can be used : \\n K - Means Animation \\n\\n\\n Why cluster evaluation ? \\n e To avoid Detecting clusters in random Structure \\n\\n o Uncovering whether non - random structure exists in the data . \\n e \\n To evaluate Clustering Results'), Document(metadata={}, page_content='o Assessing how well the clustering aligns with the data without external reference . \\n e To compare with external known patterns \\n\\n o \\n Comparing clustering results to externally known information , e.g. , class labels . \\n e To compare different Clusterings and algorithms'), Document(metadata={}, page_content='o \\n Evaluating and comparing different sets of clusters for quality . \\n â€œ The validation of clustering structures is the most \\n difficult and frustrating part of cluster analysis . \\n Without a strong effort in this direction , cluster analysis \\n will remain a black art accessible only to those true \\n believers who have experience and great courage . \\n Algorithms for Clustering Data , Jain and Dubes \\n\\n Types of cluster evaluation measures'), Document(metadata={}, page_content='Types of cluster evaluation measures \\n\\n e \\n Unsupervised ( Internal ): measure the goodness of a clustering structure \\n without respect to external information . \\n The ground truth is not available . \\n o Examples : Cohesion , separation , SSE , Silhouette Coefficient . \\n e Supervised ( External ) : measure the extent to which cluster labels match \\n\\n externally supplied class labels . \\n The ground truth is available . \\n o Examples : Entropy , Precision , Recall , F - measure . \\n Outline'), Document(metadata={}, page_content='LJ \\n Clustering evaluation \\n 1 Why cluster evaluation ? \\n\\n 1 Types of cluster evaluation measures \\n M@ Unsupervised evaluation \\n\\n LJ Cohesion vs Separation \\n LI Silhouette Coefficient \\n\\n LJ Supervised evaluation \\n\\n LJ Entropy \\n\\n LJ Precision , Recall , F - measure \\n\\n Cohesion vs Separation \\n\\n Cluster cohesion ( Compactness )'), Document(metadata={}, page_content='Cluster cohesion ( Compactness ) \\n\\n e \\n Measure how closely related object in a \\n cluster . \\n Cluster Separation \\n Measure how distinct or well- separated \\n a cluster is from other clusters . \\n Graph - Based View \\n\\n Weighted graph where the weights are the distances \\n between data points .'), Document(metadata={}, page_content='Cohesion : Sum of proximities in a cluster . \\n e Cohesion : Sum of proximities to the cluster centroid . \\n cohesion(C ;) = > , proximity(x , c ;) \\n e Separation : Sum of proximites between centroids . \\n Between a cluster centroid and the global centroid \\n\\n separation(C ;) = proximity(ci , c ) \\n\\n\\n Prototype - Based View \\n\\n Represent a clusters using their centroids . \\n\\n Cohesion : Sum of proximities to the cluster centroid . \\n\\n cohesion(C ;) = > , proximity(x , c ;)'), Document(metadata={}, page_content='cohesion(C ;) = > , proximity(x , c ;) \\n\\n o \\n Between a cluster centroid and the global centroid \\n\\n separation(Ci ;) = proximity(ci , c ) \\n\\n Silhouette Coefficient \\n\\n Silhouette coefficient combines cohesion and \\n\\n separation . \\n\\n Lo , as a = av \\n For an individual point / 9 \\n Oo a = average distance of / to the points in its cluster an \\n\\n o b = min ( average distance of / to points in another cluster ) C. \\n Outline \\n\\n LJ Clustering evaluation \\n\\n 1 Why cluster evaluation ?'), Document(metadata={}, page_content='1 Why cluster evaluation ? \\n\\n 1 Types of cluster evaluation measures \\n LJ Unsupervised evaluation \\n\\n LJ Cohesion vs Separation \\n LI Silhouette Coefficient \\n\\n HM Supervised evaluation \\n\\n LJ Entropy \\n\\n LJ Precision , Recall , F - measure'), Document(metadata={}, page_content='Entropy \\n Entropy measures the extent to which the clustering structure \\n matches external class labels . \\n e Pure cluster is cluster that contain only one class label . \\n e We measure the purity of a cluster using the entropy . \\n e How to Use Entropy for Evaluation : \\n o Calculate entropy for each cluster . \\n o Sum the entropies to get an overall measure . \\n Lower values indicate better alignment with external class labels . \\n K a \\n Uk \\n So = ) W SLk * Sik = ) \\n DPLk logs Plk \\n 1 1'), Document(metadata={}, page_content='Pure cluster \\n\\n Impure cluster \\n\\n z= WwnNneR * \\n\\n wnre \\n\\n a - \\n 0 1 0 ) S _ = 0 \\n 0 0 1 ) \\n Sik \\n 4/8 2/8 2/8 15 \\n ; 7 7 \" S , = 0.971 \\n 2/6 0 4/6 0.918 \\n Sik \\n 2/6 2/6 2/6 1.585 \\n 2/6 2/6 Â« 2/6 ~=â€”-1.585 S- = 1.585 \\n K Ny L \\n S.= > \\n W Suk Si = > Pu log , Pix \\n\\n\\n Precision , Recall , F - measure \\n\\n e Precision : The fraction of a cluster / that consists of objects of a specified class . \\n Recall : The extent to which a cluster contains all objects of a specified class .'), Document(metadata={}, page_content='Data Mining \\n Status Income Cheat \\n o Examples : eye color of a person , temperature , etc . \\n Attribute is also known as _ variable , field , \\n\\n characteristic , dimension , or feature \\n\\n Single \\n Married \\n Single \\n Married \\n\\n Divorced \\n\\n Objects \\n\\n Married \\n\\n e Object : collection of attributes \\n e \\n Object is also known as record , point , case , \\n sample , entity , or instance \\n\\n Divorced \\n\\n Single \\n\\n oO ON OO a BKB WN = \\n Types of Attributes \\n e Nominal ( Categories )'), Document(metadata={}, page_content='o Examples : ID numbers , eye color , zip codes \\n\\n e Ordinal ( Ordered Categories ) \\n Examples : Zip codes , counts , or words in documents . \\n Discrete \\n Represented as integers . \\n Note : Binary attributes are a special case of \\n discrete attributes . \\n Â© \\n oe \\n e Continuous Attribute : \\n Values are real numbers . \\n Discrete v / s'), Document(metadata={}, page_content='o Examples : Temperature , height , weight . \\n Real values , practically measured with finite digits \\n Represented as floating - point variables . \\n Asymmetric Attributes \\n\\n e \\n Inasymmetric attributes , only the presence ( non - zero value ) matters . \\n o Examples : Words present in documents : Focus on words that appear . \\n o Items present in customer transactions : Emphasize purchased items . \\n e Real - Life Scenario : \\n\\n In a grocery store encounter , would we say :'), Document(metadata={}, page_content=\"In a grocery store encounter , would we say : \\n\\n â€œ Our purchases are similar because we did n't buy most of the same products ? â€ \\n\\n Important Characteristics of Data \\n\\n e Dimensionality ( Number of Attributes ) \\n o \\n High - dimensional data presents unique challenges . \\n e Sparsity \\n o Emphasizes the importance of presence over absence .\"), Document(metadata={}, page_content=\"e \\n Resolution \\n o Patterns can vary based on the scale of measurement . \\n e Size \\n o The type of analysis often depends on the data 's size . \\n e Distribution \\n o Considers centrality and dispersion in the data . \\n Types of Data Sets \\n\\n e Record Data : records with fixed attributes Person : \\n\\n o \\n Relational records \\n o Data matrix ... po | Miler Paul | bondon _ | \\n > Transaction Data norelation \\n\\n | 4 | | | Rom _ _ \\n Car : \\n\\n 10 1973 \\n 103 \\n ice \\n\\n 3 \\n Types of Data Sets\"), Document(metadata={}, page_content='10 1973 \\n 103 \\n ice \\n\\n 3 \\n Types of Data Sets \\n\\n e Record Data : records with fixed attributes \\n o \\n Relational records \\n Data matrix ... \\n o Transaction Data \\n\\n e Graphs and Networks \\n o Transportation network \\n o \\n information networks ... \\n o Molecular Structures \\n Types of Data Sets \\n\\n e Record Data : records with fixed attributes \\n o ~~ Relational records \\n o = > Data matrix ... \\n o Transaction Data'), Document(metadata={}, page_content='e Graphs and Networks \\n o Transportation network \\n o Social or information networks ... \\n o \\n Molecular Structures \\n\\n e Ordered ( Sequence ) Data \\n o Video : sequence of image \\n o Genetic Sequence Data \\n o Temporal sequence ... \\n\\n\\n Types of Data Sets \\n\\n e Record Data \\n o ~â€”- Relational records \\n o ~)>0 Data matrix ...'), Document(metadata={}, page_content='o Transaction Data Political/ \\n Aoministrative \\n ouncgarnes \\n e Graphs and Networks \\n o Transportation network - Streets \\n o Social or information networks ... \\n Vector \\n o Molecular Structures p \\n arcels \\n e Ordered ( Sequence ) Data \\n o Video : sequence of image ; Land Usage \\n o Genetic Sequence Data < \\n o Temporal sequence ... \\n Elevation \\n\\n e Spatial Data \\n o RGB Images \\n o = Satellite images \\n\\n Real World \\n\\n\\n 2- Data preprocessing \\n\\n What is Data Preprocessing ? â€” Major Tasks'), Document(metadata={}, page_content='Data cleaning \\n e Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies \\n Data integration \\n e Integration of multiple databases , data cubes , or files \\n Data transformation and data discretization \\n e \\n Normalization \\n e Discretization \\n e Sampling \\n Data reduction ( covered in the next chapter ) \\n e \\n Dimensionality reduction \\n e Data compression \\n sf Quality \\n Poor Data Quality adversely affects data processing efforts .'), Document(metadata={}, page_content='Example : Poor data can result in wrong loan decisions . \\n â€” Some credit - worthy candidates are denied loans \\n â€” More loans are given to individuals that default'), Document(metadata={}, page_content='e \\n What types of data quality issues exist ? and how can we identify them ? \\n e What can we do about these problems ? \\n Examples of data quality problems : \\n â€” Noise and outliers \\n â€” Wrong data \\n â€” Fake data \\n â€” Missing values \\n â€” Duplicate data \\n Noise \\n\\n e Noise in Objects : Extraneous \\n elements affecting data integrity . \\n\\n e Noise in Attributes : Modification of \\n original attribute values .'), Document(metadata={}, page_content='e Examples : \\n o Distorted voice on a poor phone line . \\n o \\n \" Snow \" on a television screen . \\n o Erroneous entries caused by data entry \\n\\n errors or system glitches . \\n Outliers'), Document(metadata={}, page_content='e \\n Data objects with characteristics significantly different \\n from the majority in the dataset . \\n o Outliers can be noise that disrupts data analysis . \\n o Incertain scenarios , outliers are the primary focus of analys \\n o \\n Credit card fraud detection \\n o Intrusion detection . \\n e Determining Causes : \\n o Explore the reasons behind the presence of outliers . \\n How to Handle Noisy Data ? \\n Binning : Sort data into bins , enabling smoothing using means , medians , \\n or boundaries .'), Document(metadata={}, page_content='e \\n Regression : Smooth data through regression functions . \\n o \\n Use other attributes to predict the noisy attributes \\n\\n e \\n Clustering : Identify and eliminate outliers . \\n Semi - supervised : Combine automated and human inspection to identify \\n\\n noise and outliers . \\n Missing values \\n Missing Values \\n\\n Age SidSp Parch # icket \\n\\n 22 1 0 A / S 21171 \\n\\n 1 PC 17599 \\n L ) STON / O2 . 3101282 \\n 1 113803 53.1 \\n 373450 8.05'), Document(metadata={}, page_content='Reasons for missing values 3s \\n Eliminate data objects or variables \\n o Estimate missing values \\n = Example : time series of temperature \\n = Example : census results \\n o \\n Ignore the missing value during analysis \\n\\n Duplicate Data \\n Occurrence of identical or nearly identical data objects . \\n e Common when merging data from diverse sources . \\n o Example : Identical individuals with multiple email addresses . \\n How to handle duplicate data \\n\\n \\\\ \\n\\n e Remove duplicate data objects . \\n H'), Document(metadata={}, page_content='\\\\ \\n\\n e Remove duplicate data objects . \\n H \\n\\n e Keep Duplicate Data : When and Why ? \\n Smitha relly ? \\n o Customers with multiple accounts may unintentionally \\n\\n accumulate points separately . \\n o Keeping duplicate data ensures they receive all earned benefits . \\n e Discretization : Converting continuous data into discrete categories . \\n e Sampling : Selecting a subset to represent a larger population . \\n Normalization'), Document(metadata={}, page_content=\"e Normalization ensures that variables are on a consistent scale . \\n e Normalization is crucial for many data mining algorithms . \\n e Improved Algorithm Convergence . \\n\\n Min - max normalization : to [ new_min , , new_max , ] \\n\\n _ v â€” min : \\n v ' = â€” â€” _ â€” â€” â€” _ ( new _ max:â€”new _ min)+new _ min : \\n Max : â€” Min : \\n\\n Z - score normalization ( yu : mean , o : standard deviation ): \\n _ ATB \\n Oo \\n\\n rh \\n\\n Min - max normalization VS Z - score normalization \\n\\n Un - normalized Houses\"), Document(metadata={}, page_content='Un - normalized Houses \\n\\n â‚¬ & \\n Min - max normalization : \\n o Guarantees all attributes willhave the â€” â‚¬ \\n exact same scale . \\n = ie 75 \\n o Does not handle outliers well . \\n Years old \\n\\n Z - score normalization : \\n o Handles outliers . \\n o Does not produce normalized data \\n with the exact same scale . \\n Normalized Houses using min - max normalizatio \\n\\n 0.0 0.2 0.4 0.6 0.8 1.0 \\n Years old ( normalized ) \\n\\n Number of rooms ( normalized ) \\n\\n Min - max normalization VS Z - score normalization'), Document(metadata={}, page_content='Min - max normalization : \\n\\n O \\n\\n O \\n\\n Guarantees all attributes will have the \\n exact same scale . \\n Does not handle outliers well . \\n\\n Z - score normalization : \\n\\n O \\n\\n O \\n\\n Handles outliers . \\n Does not produce normalized data \\n with the exact same scale .'), Document(metadata={}, page_content='Un \\n normalized \\n Converting a continuous attribute into an ordinal attribute . \\n A potentially infinite number of values are mapped to a small number of categories . \\n e Discretization is used in both unsupervised and supervised settings . \\n Discretization \\n\\n e \\n Unsupervised \\n\\n o \\n Binning : Top - down split \\n\\n o \\n Histogram analysis : Top - down split \\n\\n o \\n Clustering analysis : top - down split or bottom - up merge \\n Supervised \\n\\n o \\n Decision - tree analysis : top - down split'), Document(metadata={}, page_content='o \\n Correlation analysis : bottom - up merge \\n Note : All the methods can be applied recursively'), Document(metadata={}, page_content='Sampling \\n Sampling is selecting a subset of data from a larger dataset to make it more \\n manageable for analysis while maintaining its representativeness . \\n e Weuse sampling because obtaining the entire dataset of interest is : \\n o Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . \\n o   Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . \\n e Challenges :'), Document(metadata={}, page_content='e Challenges : \\n o Ensuring the sample is representative of the population . \\n o Addressing potential bias in the sampling process . \\n Sampling is an essential tool in data analysis , achieving a crucial equilibrium \\n between resource efficiency and the ability to derive meaningful insights .'), Document(metadata={}, page_content='Sample size \\n\\n h and \\n\\n | decision in researc \\n\\n + , \\n oe \\n we \\n 3 \\n\\n * \\n a \\n\\n . \\n\\n Pa \\n\\n fuse * , hi atts \\n\\n Ize IS a Critica \\n\\n analysis . \\n , UPSET SAR oF \\n\\n â€œ ten AAG BRR Cats \\n 5 \\n\\n late sample s \\n\\n ing an appropria \\n\\n Select \\n\\n\\n Random sampling \\n with replacement \\n\\n Sampling methods \\n\\n Random sampling \\n without replacement \\n\\n Simple random sampling \\n i Cluster sampling \\n\\n Origian ! \\n\\n data'), Document(metadata={}, page_content='Origian ! \\n\\n data \\n\\n e \\n Equal probability of selecting any particular item \\n Sampling without replacement cial wires \\n Once an object is selected , it is removed from the population \\n Sampling with replacement \\n Aselected object is not removed from the population \\n\\n Stratified sampling \\n Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i.e. , \\n\\n approximately the same percentage of the data ) \\n\\n 3- Similarity and Dissimilarity Measures'), Document(metadata={}, page_content='3- Similarity and Dissimilarity Measures \\n\\n Similarity and Dissimilarity Measures \\n\\n e \\n Similarity Measure : \\n o Quantifies data object likeness . \\n Higher values indicate greater similarity . \\n o Lower values indicate greater similarity . \\n o Often starts at O and varies in the upper limit . \\n a â€ \\n\\n Similar Not Similar'), Document(metadata={}, page_content='Similar Not Similar \\n\\n e Proximity : \\n o Refers to either similarity or dissimilarity . \\n Similarity reveal valuable data relationships for \\n pattern recognition , clustering , and classification . \\n Properties of a Distance \\n\\n e \\n Distance t is a metric if it satisfies these properties : \\n\\n o \\n Non - Negativity : \\n Symmetry : \\n m d(x , y ) = d(y , x ) for all x and y.'), Document(metadata={}, page_content='o Triangle Inequality : \\n , z ) < d(x , y ) + d(y , Z ) \\n for all x , y , and z. \\n Metrics ensure that distances align with real - world geometric properties'), Document(metadata={}, page_content='Metrics guarantee meaningful and reliable distance measurements in data \\n analysis . \\n Note : This property may not always hold , e.g. , cosine similarity . \\n Symmetry : \\n Oo ( xX , y ) = S(y , X ) for all x and y. \\n o \\n Symmetry ensures that the order of comparison does not affect the similarity score . \\n Understanding these properties helps ensure the reliability and consistency \\n of similarity measures in data analysis . \\n Similarity and dissimilarity matrix \\n\\n e Distance Matrix \\n\\n O \\n\\n O'), Document(metadata={}, page_content='e Distance Matrix \\n\\n O \\n\\n O \\n\\n O \\n\\n Distances between all data objects in a dataset . \\n Useful for clustering and nearest neighbor algorithms . \\n Symmetric , with values reflecting dissimilarities . \\n e Similarity Matrix \\n\\n O \\n\\n O \\n\\n O \\n\\n Similarities between data objects . \\n Valuable for clustering , recommendation systems , ... \\n Often symmetric , with higher values indicating \\n stronger similarities . \\n lersthialll titel'), Document(metadata={}, page_content='Distances and similarity examples \\n e Proximity measures for numerical vectors \\n o Euclidean Distance \\n o Minkowski Distance \\n o Cosine Similarity \\n o Linear correlation \\n\\n e Proximity measures for binary vectors \\n o \\n Simple Matching Coefficient ( SMC ) \\n o Jaccard Coefficient \\n\\n Euclidean Distance ( applicable to numerical vectors ) \\n\\n d(x , y ) â€” \\n\\n e â€˜ 7 : number of attributes . \\n e Lk , YK : kth attributes for objects x \\n and y , respectively . \\n\\n - _ â€” \\n cad \\n tw \\n - \\n â€”'), Document(metadata={}, page_content='- _ â€” \\n cad \\n tw \\n - \\n â€” \\n\\n - \\n | \\n - \\n = \\n\\n Standardization is necessary , if \\n scales differ . \\n Example : Euclidean Distance matrix \\n\\n 7 \\n\\n bet es \\n\\n oes a1 } ol anal 162 ] â€” sv \\n\\n 7 . \\n 2ersf of data 3.162 \\n\\n . 3 3 \\n\\n _ _ pt | 5099 ) 36x ) 2 ) \\n 0 2 3 5 6 \\n\\n Minkowski Distance ( applicable to numerical vectors ) \\n\\n n lyr \\n d(x , y)= { > â€” |xn â€” ye â€ \\n el'), Document(metadata={}, page_content='Generalization of Euclidean Distance . \\n r : parameter \\n n : number of attributes \\n . \\n th . . \\n x , and y , are , respectively , the k â„¢ attributes or objects x and y. \\n The hyperparameters r Allows to adapt the distance to the characteristics of data . \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n Ideal for measuring distances in grid - like paths . \\n Binary vector example : Hamming distance counts differing bits . \\n The most commonly used distance metric .'), Document(metadata={}, page_content='The most commonly used distance metric . \\n Measures the straight - line distance in Euclidean space . \\n e Supremum Distance ( r â€” ~ ): \\n o \\n Also called Lmax norm or L Â» norm distance . \\n Calculates the maximum difference between any component of vectors . \\n Appropriate when movement is unrestricted in any direction . \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n Ideal for measuring distances in grid - like paths .'), Document(metadata={}, page_content='Binary vector example : Hamming distance counts differing bits . \\n The most commonly used distance metric . \\n Measures the straight - line distance in Euclidean space . \\n e Supremum Distance ( r â€” ~ ): \\n o \\n Also called Lmax norm or L Â» norm distance . \\n Calculates the maximum difference between any component of vectors . \\n Appropriate when movement is unrestricted in any direction . \\n Also known as Manhattan , taxicab , or L1 norm distance .'), Document(metadata={}, page_content='Ideal for measuring distances in grid - like paths . \\n Binary vector example : Hamming distance counts differing bits . \\n The most commonly used distance metric . \\n Measures the straight - line distance in Euclidean space . \\n e Supremum Distance ( r â€” ~ ): \\n o \\n Also called Lmax norm , L~ or chebyshev distance . \\n Calculates the maximum difference between any component of vectors . \\n Appropriate when movement is unrestricted in any direction .'), Document(metadata={}, page_content='o Omeans orthogonal ( no linear relationship ) . \\n Perfect negative \\n correlation'), Document(metadata={}, page_content='e \\n Commonly used in statistical analysis and data exploration . \\n e tunable to capture nonlinear associations . \\n Distances and similarity examples \\n\\n e \\n Proximity measures for numerical vectors \\n o Euclidean Distance \\n o Minkowski Distance \\n o Cosine Similarity \\n o Linear correlation \\n\\n e Proximity measures for binary vectors \\n o Simple Matching Coefficient ( SMC ) \\n o Jaccard Coefficient \\n\\n Similarity Between Binary Vectors'), Document(metadata={}, page_content='Similarity Between Binary Vectors \\n\\n e Simple Matching Coefficient ( SMC ): the number of matches divided by \\n the \\n total number of attributes \\n SMC = ( fF , + Soo ) ! \\n Sto F Soo * Fi ) \\n Choice of the right proximity measure depends on the domain \\n\\n e \\n Comparing Documents Using Word presence \\n o Proximity Measure : Jaccard Coefficient \\n o \\n Similarity : \\n Documents are considered similar if they use high number of common words . \\n Similarity : \\n Similarity :'), Document(metadata={}, page_content='Exploratory Data Analysis ( EDA ) \\n\\n & \\n\\n Exploratory Data Analysis ( EDA ) \\n\\n LJ EDA is a set of statistical and visualization techniques . \\n L ) _ Used for seeing what the data can tell us before the preprocessing and modeling : \\n _ ) Understand the data and summarize its keys properties . \\n\\n Discover noisy data and outliers . \\n Comprehend the distribution of the data . \\n The method is either univariate or multivariate ( Usually just bivariate ) . \\n Exploratory Data Analysis ( EDA )'), Document(metadata={}, page_content='1 . Statics of Data \\n\\n 2 . \\n Asample is an unbiased subset that best \\n\\n represents the entire population . \\n Measuring the Central Tendency : ( 1 ) Mean \\n\\n O Mean ( algebraic measure ) ( sample vs. population ): \\n\\n nN \\n nis sample size and N is population size . \\n X \\n X X ; _ â€” \\n WM j= \\n Weighted arithmeticmean : y= â€” = !'), Document(metadata={}, page_content=\"OQ \\n Trimmed mean : Chopping extreme values ( e.g. , olympics gymnastics score computation ) \\n Median is the middle value in a data set when values are ordered . \\n How to calculate \\n\\n Q \\n Sort data in ascending order . \\n Repeat values according to their frequency . \\n Ifthere 's an odd number of data points , the median is the middle value . \\n Useful for skewed distributions . \\n Median is in the position ceil(13/2 ) = 7 \\n OQ \\n The median bin is the value 10 - 20\"), Document(metadata={}, page_content='How to find the median value ? \\n\\n\\n Median calculation for grouped data \\n\\n : : ( n/2)â€”B \\n Estimated Median = L + â€” ~ â€” \\n L : the lower class boundary of the median bin . \\n\\n n : the total number of values . \\n\\n B : cumulative frequency of the bins before the median \\n bin . \\n\\n G : the frequency of the median bin . \\n Measuring the Central Tendency : ( 3 ) Mode \\n\\n QO Mode : Value that occurs most frequently in the data'), Document(metadata={}, page_content='Q Unimodal \\n Q \\n QO Positive Covariance : both variables move together . \\n O Negative Covariance : variables move in opposite directions . \\n QO Zero Covariance : bo clear pattern in variable movements . \\n Understanding relationships between variables . \\n Covariance is useful but sensitive to scale , while correlation \\n addresses this issue by standardizing the measurement . \\n ye OXi2 ~~ fiz ) \\n\\n n is the number of tuples , , and p , are the respective means of X , and X , ,'), Document(metadata={}, page_content='oO , and O , are the respective standard deviation of X , and X , \\n If p , , > 0 : A and B are positively correlated ( X , â€™s values increase as X , â€™s ) \\n If p , , = 0 : independent ( under the same assumption as discussed in co - variance ) \\n If p , , < 0 : negatively correlated \\n\\n Correlation Matrix ( Correlation Heatmap ) \\n\\n QO \\n Visualizing Changes of Correlation Coefficient \\n\\n -1.00 \\n\\n -0.90 \\n Data visualization \\n\\n 2- Data Visualization \\n\\n Boxplot'), Document(metadata={}, page_content='2- Data Visualization \\n\\n Boxplot \\n\\n Histogram and Bar chart \\n Quantile plot \\n Quantile - quantile ( Q - Q ) plot \\n Scatter plot \\n Tokyo \\n\\n Chicago New York Boston Atlanta \\n\\n Categories \\n\\n Q Histogram : Tabulated frequencies represented by bars . \\n\\n Q Bar chart : Categorical data with bars proportional to the values they represent . \\n Histogram Chart \\n\\n 50 4 \\n\\n 40 + \\n\\n 30 + \\n\\n Frequency \\n count \\n 20 ~ \\n\\n 20 40 60 80 100 \\n\\n Numeric ranges \\n\\n 28'), Document(metadata={}, page_content='20 40 60 80 100 \\n\\n Numeric ranges \\n\\n 28 \\n\\n QO \\n Bars can be reordered in bar charts , but not in histograms us : \\n\\n QO \\n In histograms , it is the area of the bar that denotes the â€” Â« = \\n\\n Differences between Histograms and Bar charts : \\n\\n 407 \\n 35 \\n\\n QO \\n Histograms show distributions of variables , while bar \\n\\n charts compare variables \\n\\n QO \\n But they have rather different data distributions \\n Histogram Analysis'), Document(metadata={}, page_content='QO \\n Divide data into buckets and store average ( sum ) for each bucket . \\n QO Partitioning rules : \\n\\n OQ \\n Equal - width : equal bucket range \\n\\n 4 Equal - frequency ( or equal - depth ) \\n\\n Equal Width Binning Equal frequency binning \\n\\n 5 6 \\n 4 \\n 3 \\n\\n 2 \\n 0 \\n [ 10 , 21 ] [ 22 , 33 ] ( 34 , 45 ] [ 46 , 55 ] [ 10 , 16 ] [ 17 , 30 } [ 31 , 48 ] [ 49 , 55 \\n ] \\n\\n Count of AGE_bins Count of AGE_bins \\n\\n\\n Quantile Plot \\n\\n O Purpose : Visualizes all quantile information for a specific attribute'), Document(metadata={}, page_content=\"O Benefits \\n\\n â€” _ \\n wS \\n fo ) \\n\\n _ } Provides a comprehensive view of the \\n attribute 's distribution . \\n 120 \\n\\n â€” _ \\n oo \\n oo \\n\\n _ ) Helps identify both general trends and \\n outliers . \\n\\n Unit price ( $ ) \\n on \\n S \\n\\n LJ Construction \\n\\n . \\n 0.500 0.750 1.000 \\n L ) Fora data { X1 , X2 , ... , XN } sorted in \\n\\n f - value \\n\\n increasing order . \\n\\n 4 f , indicates that approximately f of the data \\n point have values S$ x. \\n\\n Data Mining : Concepts and Techniques\"), Document(metadata={}, page_content='Data Mining : Concepts and Techniques \\n\\n Quantile - Quantile ( Q - Q ) Plot \\n\\n OQ Purpose \\n\\n QC \\n Assess the distributional similarity between an attribute and \\n\\n Norma ! data quantiles \\n\\n either another attribute or a theoretical distribution . \\n : , | \\n 4 ~= â€” Deviations from the line indicate differences in distribution . \\n -â€”3 = 2 1 0 1 2 3 \\n Norma ! \\n theoretical quantiles \\n Items sold'), Document(metadata={}, page_content='Scatter plot \\n Each pair of values is treated as a pair of coordinates and plotted as points in the plane . \\n Unit price ( $ ) \\n\\n Life Expectancy \\n\\n 70 \\n\\n 60 \\n\\n 30 \\n\\n \" oe ? \\n Â° â€œ e \\n Â° iret ee 2 \\n 9,2 % ow Â° ~ * \\n a Se eee \\n . : % . Â° Â° , @uwee \\n Â° e Â° â€œ a â€ 505 ef Â° 3 \\n Â° e \\n hal 2 ee . \\n â€˜ Cd os e e - \\n e oe â€ e \\n 1000 2 5 10k 2 5 100k \\n\\n GDP per Capita'), Document(metadata={}, page_content='GDP per Capita \\n\\n Positively and Negatively Correlated Data \\n QO Aline chart displays information as a series of data points called â€˜ markers â€™ . \\n Parallel Coordinates Plots of Iris Data \\n A parallel coordinate plot maps each row in the data table as a line , or profile . \\n Each attribute of a row is represented by a point on the line . \\n ~~Setosa \\n ~â€”Versicolor 8- \\n ~-~Virginica â€” ~~Setosa \\n 7/ â€” â€” Versicolor \\n ~~Virginica \\n\\n e = \\n\\n 2 5 \\n\\n 2 o5 + Â¥ \\n\\n = ELLZ \\n\\n [ = , - â€œ Wy \\n\\n 8 8 Ce'), Document(metadata={}, page_content='2 o5 + Â¥ \\n\\n = ELLZ \\n\\n [ = , - â€œ Wy \\n\\n 8 8 Ce \\n\\n Â® â€˜ N O21 \\n\\n Ss 2 \\n SERS = \\n 1 1 = . 0 1 1 ay . \\n\\n eapel length sepal width petal length petal width â€” sepal width sepal length petal length petal width'), Document(metadata={}, page_content='Dimensionality Reduction \\n Dimensionality Reduction \\n Dimensionality reduction is a technique in data analysis that simplifies the data by \\n reducing the number of variables or dimensions . \\n Dimensionality Reduction should ensure the following : \\n\\n e Preserving Relevance \\n\\n o \\n It retains the most crucial information from high - dimensional spaces . \\n e Eliminating Redundancy'), Document(metadata={}, page_content='o \\n Dimensionality reduction efficiently removes redundant and irrelevant attributes for the analysis . \\n Transformation into a lower - dimensional space \\n\\n o \\n it transforms data into a lower - dimensional space , making it more manageable for analysis and \\n visualization . \\n Motivations for Dimensionality Reduction'), Document(metadata={}, page_content='e Improve model \\n o Improved Model Performance \\n o Reduce Overfitting \\n e Efficiency and Resource Management \\n o Computational Efficiency \\n o Memory and Storage Efficiency \\n e Interpretability and Understanding \\n o Data Visualization \\n o Enhanced Interpretability \\n e \\n Data quality enhancement \\n o Removal of Redundant Information \\n o Noise Reduction \\n Volume of the hypersphere \\n\\n\\n The Curse of Dimensionality'), Document(metadata={}, page_content='The Curse of Dimensionality \\n\\n Sparsity : when points are uniformly generated within a high - dimensional \\n hypercube , most points are much farther from the center than expected . \\n The Curse of Dimensionality'), Document(metadata={}, page_content='e \\n Traditional Distance Metrics \\n In high dimensions , traditional distance metrics like Euclidean distance lose their effectiveness . \\n Points become dispersed , posing challenges for distance - based analysis . \\n The curse of dimensionality impacts diverse data analysis tasks ( nearest - neighbor \\n algorithms , classification , and clustering in high - dimensional spaces ... ) . \\n Taxonomy of Dimensionality Reduction Methods'), Document(metadata={}, page_content='e \\n Feature extraction : transforms the original attributes into new ones . \\n Linear methods ( PCA ): Transform the original attributes into a new set of linear attributes . \\n Non - linear methods ( t - SNE ): Transform the original attributes into a new set of non - linear attributes . \\n Feature selection : selects a subset of original attributes . \\n Filter methods : Select attributes based on their statistical properties .'), Document(metadata={}, page_content='o Wrapper methods : Use the performance of a machine learning model to evaluate the goodness of \\n selected attributes . \\n Outline'), Document(metadata={}, page_content='e Principal Component Analysis ( PCA ) \\n\\n O \\n\\n e ) \\n e ) \\n e ) \\n\\n What is PCA ? \\n How to perform PCA ? \\n Applications of PCA \\n\\n Motivation example : oscillating spring \\n\\n . . . . \\n e Tracking motion of a ball on an oscillating \\n spring at regular intervals . \\n | camera C \\n e 3cameras record ( x , y ) position from different \\n Motivation example : oscillating spring \\n\\n . \\n camera B \\n e Tracking motion of a ball on an oscillating \\n spring at regular intervals . \\n | camera C'), Document(metadata={}, page_content='e 3cameras record ( x , y ) position from different \\n e Each sample is 6D vector : \\n X = [ xA , yA , xB , yB , xC , yC ] \\n\\n cameraA camera B camera C \\n\\n Physically , we know the underlying motion is \\n 1D , along the spring . \\n TAL . \\n\\n\\n Motivation example : oscillating spring \\n\\n . \\n camera B \\n e Tracking motion of a ball on an oscillating \\n spring at regular intervals . \\n | camera C \\n\\n e 3cameras record ( x , y ) position from different \\n angles \\n\\n camera A'), Document(metadata={}, page_content='camera A \\n\\n e Each sample is 6D vector : \\n X = [ xA , yA , xB , yB , xC , yC ] \\n\\n cameraA camera B camera C \\n\\n How do we extract this underlying 1D \\n dynamic hidden in 6D recordings ? \\n\\n TAL . \\n Correlations is common in Real Data ( to be finished ) \\n Correlation is a common characteristic of real - world datasets , often reflecting complex \\n relationships . \\n\\n Examples'), Document(metadata={}, page_content=\"Examples \\n\\n e Weight and height correlation in individuals . \\n Spatial pixel correlations in images . \\n e Study hours and test score relationships . \\n Data Redundancy \\n\\n e Correlations can indicate redundancy in the data . \\n e Reducing redundancy can enhance the performance of data mining algorithms . \\n Recognizing and addressing correlations is essential for effective data analysis and \\n machine learning applications . \\n What is Principal Components Analysis ( PCA ) ? \\n PCA 's goal\"), Document(metadata={}, page_content=\"e Transform the original data into new variables that follow \\n the variation in the data . \\n Principal Components \\n\\n e \\n After rotation , new axes , called principal components , are \\n formed . \\n PCA 's goal PE \\n\\n e Transform the original data into new variables that follow \\n the variation in the data . \\n Principal Components\"), Document(metadata={}, page_content=\"e \\n After rotation , new axes , called principal components , are \\n formed . \\n Transform the original data into new variables that follow \\n the variation in the data . \\n PCA ' Attribute 2 \\n\\n Using PC1 , we reduce the data 's \\n\\n dimensionality while retaining most of \\n its variation . \\n\\n Princ \\n\\n e \\n After rotation , new axes , called principal components , are \\n formed . \\n PCA 's goal PC1 \\n\\n e Transform the original data into new variables that follow \\n the variation in the data .\"), Document(metadata={}, page_content=\"PCA â€™ Attribute 2 \\n\\n Using PC1 , we reduce the data 's \\n\\n dimensionality while retaining most of \\n its variation . \\n\\n Princ \\n e After rotation , new axes , called principal components , are \\n formed . \\n PCA 's goal \\n\\n e \\n Transform the original data into new variables that follow \\n the variation in the data . \\n PCA â€™ \\n\\n Using PC1 , we reduce the data 's\"), Document(metadata={}, page_content=\"Using PC1 , we reduce the data 's \\n\\n dimensionality while retaining most of \\n its variation . \\n After rotation , new axes , called principal components , are \\n formed . \\n What is Principal Components Analysis ( PCA ) ? \\n PCA identifies the directions , called principal \\n\\n components , along which the data varies the most . \\n Subsequent components are orthogonal ( uncorrelated ) to \\n the preceding ones . \\n x2\"), Document(metadata={}, page_content=\"Retaining only the most important components \\n reduces the data 's dimensionality . \\n Â° \\n\\n PCA is widely used in \\n\\n o \\n Subsequent principal components express the \\n remaining variation . \\n Successive components should also minimize \\n projection errors . \\n oS \\n x \\n Os r ge \\n Nad \\n \\\\ \\n\\n Objectives of PCA - A Mathematical Perspective \\n\\n datapoint \\n\\n D= D+ D , \\n\\n initial â€” remaining + lost \\n variance variance variance \\n\\n 2 2 \\n la = |lwell+|la â€” welf\"), Document(metadata={}, page_content='2 2 \\n la = |lwell+|la â€” welf \\n\\n this is maximize or minimize \\n constant this this'), Document(metadata={}, page_content='e D1 is the remaining variance expressed in the component . \\n e D2 is the projection error or the lost variance . \\n e D3 is the original fixed variance independent to the component . \\n Subtract the mean from each attribute ( column ) to center the data . \\n Scale attributes if their scales differ . \\n Calculate the covariance matrix , denoted as sigma . \\n Compute the eigenvalues and eigenvectors of the covariance matrix to find principal components .'), Document(metadata={}, page_content='Choose k < m eigenvectors as principal components . \\n Reduce data by projecting it onto the k principal components'), Document(metadata={}, page_content='The input of the algorithm \\n xX = eS X5 ... x | \\n nxm \\n\\n n : is the number of objects ( rows ) . \\n m : is the number of attributes ( columns ) . \\n Covariance Matrix calculation \\n\\n Wee â€” X)(Â¥ ; -Y ) \\n Cov(X , Y ) = = â€” \\n\\n Subtracting the mean from each attribute centers the data around zero , which \\n makes the covariance formula easier to calculate : \\n\\n nm \\n Â¥ , 44 % ; \\n\\n Covariance Matrix calculation ( Matrix notation )'), Document(metadata={}, page_content='Cov ( Xm , X1 ) Cov ( Xm , X2 ) ... Var ( Xm ) \\n This produce a symmetric covariance matrix : \\n\\n x?x \\n\\n Eigenvalues and Eigenvectors \\n\\n Diagonalization in PCA'), Document(metadata={}, page_content='Diagonalization in PCA \\n\\n e \\n A fundamental concept in PCA for simplifying the variance matrix . \\n = = PDP â€™ \\n e Diagonal matrix ( D ) consists of eigenvalues ( A ) . \\n e Eigenvectors are arranged in the matrix P enable data projection and \\n dimensionality reduction . \\n e Spectral theorem ensures eigenvectors â€™ orthogonality . \\n e Singular value decomposition ( SVD ) can be used to do this \\n decomposition . \\n\\n y= USv ! \\n Choosing k ( number of principal components )'), Document(metadata={}, page_content='Select k < n eigenvectors to make them components \\n\\n | | | | \\n aan ; | \\n UF = ) apf2 ) Â© lA ) | gl ) Ee Rm \\n | | | | \\n | | | | \\n\\n Select k components \\n\\n Choosing k ( number of principal components ) \\n\\n e Average squared projeenon error \\n\\n = Ly > fe - EO ) cx \\n\\n i= ] \\n\\n kl \\n if i = l \\n Choose k to be smallest value so that \\n 1 2 \\n ~ Ah 1 le \" = on \\n\\n â€œ 99 % of variance is retained â€ \\n\\n e \\n Total variation in the data \\n\\n < 0.01 \\n\\n\\n Choosing k ( number of principal components )'), Document(metadata={}, page_content='Choosing k ( number of principal components ) \\n\\n e Average squared projeenon error \\n\\n = a } je Dex \\n\\n i= ] \\n\\n 1 Le { 2 \\n nt i=1 \\n\\n Choose k to be smallest value so that \\n\\n e Total variation in the data \\n\\n k \\n eae Si ; Duin ii a > 0.99 \\n dint Si dain Si \\n\\n â€œ 99 % of variance is retained â€ \\n\\n Choosing k ( number of principal components ) \\n\\n Scree Plot'), Document(metadata={}, page_content='Scree Plot \\n\\n e Scree Plot \\n o Tool aids in deciding how many \\n principal components to retain(k ) . \\n How to choose k \\n o Elbow point in the plot indicates a \\n significant drop in eigenvalues , \\n suggesting an optimal number of Component Number \\n components . \\n Determining Coordinates in Principal Components \\n\\n T= XU'), Document(metadata={}, page_content='T= XU \\n\\n e TT : \\n Coordinates matrix of data points in PCA components . \\n e X : Data matrix ( variables as columns , data points as rows ) . \\n e P : Eigenvectors matrix of the covariance matrix . \\n The matrix is approximation of the centred data . \\n e To reconstruct the original data , the mean should be added to each columns . \\n PCA applications : Denoising with PCA \\n\\n Purpose'), Document(metadata={}, page_content='Purpose \\n\\n e \\n Reduce noise in data or images . \\n How It Works \\n Data is represented as a matrix . \\n e PCA identifies principal components . \\n e High - variance components retain signal ; \\n e low - variance components capture noise . \\n Use Cases : \\n\\n e \\n Image denoising . \\n e Enhancing data quality in various fields . \\n PCA applications : Face Recognition with PCA \\n\\n Purpose \\n\\n e \\n Identify and authenticate individuals from \\n facial images \\n How It Works'), Document(metadata={}, page_content='e \\n Eigenfaces : Eigenvalue and eigenvector \\n decomposition of face images . \\n e Reduced - dimension representation . \\n e Compare face features for recognition . \\n Use Cases \\n\\n e \\n Security systems . \\n e Biometric authentication . \\n e Video surveillance . \\n PCA applications : Data Visualization with PCA \\n\\n Purpose \\n\\n Transform high - dimensional data into a \\n lower - dimensional space for visualization . \\n How It Works'), Document(metadata={}, page_content='original data space \\n PCAreduces data dimensions while \\n preserving data variance . \\n e Data points are projected onto a \\n lower - dimensional ( 2D,3D ) subspace . \\n e Exploratory data analysis . \\n e Cluster analysis . \\n PCA applications : Other \\n\\n Versatility \\n\\n e PCAis used in various fields and \\n applications . \\n Examples'), Document(metadata={}, page_content='e Anomaly Detection : Identifying \\n unusual data patterns . \\n e Data Compression : Reducing data \\n dimensionality . \\n e Recommendation Systems : \\n Extracting user preferences . \\n e Genomic Data Analysis : Identifying \\n gene expression patterns . \\n Original points \\n O New points \\n @ Reconstruction'), Document(metadata={}, page_content='Dimensionality Reduction \\n What is Feature Subset Selection ( FSS ) ? \\n Why doing Feature Subset Selection ? \\n Taxonomy of Feature Selection methods \\n o \\n Unsupervised Feature Selection \\n o Supervised Feature Selection \\n = Filter methods for FSS \\n = Wrapper methods for FSS \\n\\n e \\n Comparison of Feature Selection methods \\n\\n What is Feature Subset Selection ( FSS ) ? \\n Feature Selection ( FSS ) is the process of selecting a subset of \\n\\n features from a given feature set . \\n o ---X'), Document(metadata={}, page_content='features from a given feature set . \\n o ---X \\n\\n The goal is to optimize an objective function for the chosen \\n features . \\n e FSS can be seen as a form of feature extraction . \\n However , the methods and goals differ significantly . \\n e Feature extraction : \\n o Transforming the existing features into a lower dimensional space . \\n e \\n Feature selection : \\n\\n o Selecting a subset of the existing features without a transformation . \\n What is Feature Subset Selection ( FSS ) ? \\n Feature Set :'), Document(metadata={}, page_content='A = { a,;|7=1, ... ,m } \\n Objective : \\n Find a subset : \\n\\n Yn = 45 % , , % , , .. -- 0 . 2 TH < H \\n that optimizes the objective function J(Y ; , . ) \\n Optimization : \\n\\n Y = arg max J(Y , , ) \\n\\n M , tm \\n\\n\\n Why doing Feature Subset Selection ? \\n Why not stick to feature extraction , as it essentially accomplishes the same \\n task of dimensionality reduction ? \\n Why doing Feature Subset Selection ? \\n Features may be expensive to obtain'), Document(metadata={}, page_content='o \\n Evaluate a large number of features ( Sensors ) in the test bed and select a few for the final implementation \\n You may want to extract meaningful rules from your classifier \\n\\n o \\n When you transform or project , the measurement units ( length , weight , etc . ) of your features are lost . \\n Features may not be numeric \\n\\n o \\n Atypical situation in the data mining . \\n Fewer features means fewer parameters for pattern recognition'), Document(metadata={}, page_content='o \\n Improved the generalization capabilities and avoid overfitting . \\n Reduced complexity and run - time . \\n Why Feature selection is challenging ? \\n Number of features = 3 \\n O { A1 , A2 , A3 } \\n\\n @ \\n How many features subsets ? \\n O { A1 } , { A2 } , { A3 } \\n O { A1 , A2 } , { A1 , A3 } , { A2 , A3 } \\n O { A1 , A2 , A3 } \\n\\n Why Feature selection is challenging ? \\n e Number of features = n \\n O { A1 , A2 , A3 , ... , An }'), Document(metadata={}, page_content='@ How many features subsets ? \\n O { A1 } , { A2 } , { A3 } ..... \\n O { A1 , A2 } , { A1 , A3 } , { A2 , A3 } .... \\n O { A1 , A2 , A3 } .... \\n\\n e For n=100 \\n 1 267 650 600 228 229 401 496 703 205 376 \\n\\n Taxonomy of Feature Selection methods \\n\\n Feature \\n selection \\n\\n methods \\n\\n 1 \\n\\n Ea \\n Correlation Information Chi - squared Forward Backward Genetic \\n gain test selection elimination algorithms \\n\\n Mutual \\n information \\n\\n\\n Unsupervised Feature Selection'), Document(metadata={}, page_content='Unsupervised Feature Selection \\n\\n Unsupervised feature selection typically uses statistical measures to evaluate features . \\n\\n The most common statistical measures include : \\n\\n e Variance : Features with high variance are more likely to be informative and useful \\n\\n for prediction . \\n\\n e Correlation : Features that are highly correlated with each other are likely to contain \\n redundant information .'), Document(metadata={}, page_content='e \\n Features with high mutual information are more likely to be predictive of each other . \\n correlated filters \\n po | \\n poo | \\n po | 8 \\n poo | 6 \\n poo | \\n poo \\n\\n e \\n Taxonomy of Feature Selection methods \\n\\n Feature \\n selection \\n\\n methods \\n\\n 1 \\n Mutual i ) \\n Parrchatiwn Information Chi - squared Forward Backward Genetic \\n gain test selection elimination algorithms \\n\\n r \\n Unsupervised \\n Variance Correlation \\n\\n\\n Filter methods for FSS'), Document(metadata={}, page_content='Filter methods for FSS \\n\\n Filter methods evaluate features independently and select the most important \\n features based on statistical measures , such as : \\n correlation with the target variable , information gain , or chi - squared test . \\n Choose a filter metric ( correlation , information gain , or chi - squared test ) . \\n Select the features with the highest filter metric scores . \\n Filter methods : Correlation'), Document(metadata={}, page_content=\"e Correlation is a widely used filter method in feature selection . \\n e It measures how well a feature relates to the target variable . \\n e High correlation with the target : \\n Features strongly correlated with the target variable are often good candidates for predictive models \\n Low inter - feature correlation : \\n\\n o \\n It 's also essential to consider inter - feature correlations to avoid redundancy in the feature set\"), Document(metadata={}, page_content='The correlation does not consider the non - linear correlation with the target and the \\n interaction between features \\n\\n e \\n Cool colors . \\n for negative . \\n e Neutral for no correlation . \\n Helps in visualizing the \\n correlation between attributes \\n\\n\\n Filter methods : \\n Information Gain'), Document(metadata={}, page_content='Information gain is a measure of how much information is gained about the \\n target categorical variable when a dataset is split on \\n a given categorical feature . \\n It is calculated as the difference between the entropy of the target variable \\n before and after the split . \\n What is entropy ? \\n What is entropy ? \\n It measures the level of uncertainty , randomness , or disorder in a system or dataset . \\n In the context of data , entropy quantifies the impurity of a set . \\n What is entropy ?'), Document(metadata={}, page_content='What is entropy ? \\n It measures the level of uncertainty , randomness , or disorder in a system or dataset . \\n In the context of data , entropy quantifies the impurity of a set . \\n H(8 ) = â€” ) Â° p(x ) logs p(2i )'), Document(metadata={}, page_content='i=1 \\n\\n S= { x1,x2 .... xn } , which is the set of elements . \\n What is entropy ? \\n\\n H(S ) = â€” p(2 ;) log , p(x ) \\n\\n i=1 \\n\\n e p(red ) = 6/12=0.5 \\n e p(green ) = 6/12=0.5 \\n e H(S ) = -(0.5*log(0.5)+0.5*log(0.5 ) ) = -(-0.5 - 0.5 ) \\n\\n H(S ) = 1 \\n\\n Very Impure \\n\\n What is entropy ? \\n\\n H(S ) = â€” p(2 ;) log , p(x ) \\n\\n i=1 \\n\\n e p(red ) = 3/12 \\n e p(green ) = 9/12 \\n e H(S ) = -((3/12)*log(3/12)+9/12*log(9/12 ) ) \\n\\n H(S ) = 0.811278 \\n\\n Less Impure \\n\\n What is entropy ? \\n\\n H(S ) = â€” ) _ \\n i=1'), Document(metadata={}, page_content='What is entropy ? \\n\\n H(S ) = â€” ) _ \\n i=1 \\n\\n e p(red ) = 0 \\n e p(green ) = 1 \\n e H(S ) = -(0 + 1*log(1 ) ) \\n\\n H(S ) = 0 \\n\\n Minimum Impurity \\n\\n Filter methods : Information Gain'), Document(metadata={}, page_content='Filter methods : Information Gain \\n\\n Information gain is a measure of how much information is gained about the \\n target categorical variable when a dataset is split on \\n a given categorical feature . \\n It is calculated as the difference between the entropy of the target variable \\n before and after the split . \\n Example : Information gain for Road Tested \\n\\n Mileage Road Tested Buy \\n Pure set \\n Pure set \\n Filter methods for FSS \\n\\n Advantages \\n e Computational Efficiency'), Document(metadata={}, page_content='Advantages \\n e Computational Efficiency \\n\\n Filter methods are computationally efficient \\n and scalable to large datasets and high \\n dimensionality as no machine learning \\n model is trained . \\n e Algorithm Independence \\n\\n Independent of any machine learning \\n algorithm , making them unbiased toward \\n any particular algorithm . \\n\\n Disadvantages \\n e Suboptimal Selection'), Document(metadata={}, page_content='Disadvantages \\n e Suboptimal Selection \\n\\n May not always identify the best feature \\n subset for a specific machine learning \\n algorithm because it does not consider the \\n model performance . \\n\\n e \\n Complex Relationships \\n\\n May not be able to handle complex \\n relationships and interactions between \\n features . \\n How to use filter methods for FSS \\n\\n e Filter Methods as an initial step'), Document(metadata={}, page_content=\"e Filter Methods as an initial step \\n\\n o \\n Filter methods serve as an excellent initial step in the feature selection process . \\n o Particularly valuable when dealing with large datasets . \\n e Hybridization is key \\n o \\n It 's advisable to combine filter methods with other feature selection techniques . \\n o Wrapper methods and embedded methods complement filter methods effectively . \\n 27 \\n\\n Taxonomy of Feature Selection methods \\n\\n Feature \\n selection \\n\\n methods \\n\\n i\"), Document(metadata={}, page_content='Feature \\n selection \\n\\n methods \\n\\n i \\n\\n Variance Correlation ! Mutual \\n information \\n\\n Correlation \\n\\n Information \\n gain \\n\\n rt \\n Backward \\n elimination \\n\\n Chi - squared \\n test \\n\\n Forward \\n selection \\n\\n Genetic \\n algorithms \\n 28 \\n\\n Wrapper methods for FSS'), Document(metadata={}, page_content='Wrapper methods for FSS \\n\\n Wrapper methods are feature selection methods that evaluate features by using a machine \\n learning model as a black box . \\n The model is trained on different subsets of features , and the subset that produces the best \\n performance is selected . \\n How to use filter methods for FSS'), Document(metadata={}, page_content='Feature Subset Generation : \\n The search algorithm generates various feature subsets for evaluation \\n Model Evaluation : \\n Each subset is input to the model and its performance is evaluated via model performance \\n Selection Criterion : \\n A selection criterion ( e.g. accuracy ) guides the choice of the best - performing feature subset \\n\\n Iteration : \\n The process is repeated with different subsets until an optimal set is found \\n 29 \\n\\n Complete \\n feature set \\n\\n s , 7ou \"'), Document(metadata={}, page_content='Complete \\n feature set \\n\\n s , 7ou \" \\n\\n Wrapper Feature \\n Selector \\n\\n Classification model \\n\\n Optimal \\n feature set \\n\\n Classification model \\n\\n 30 \\n\\n Search strategy and objective function \\n\\n Complete feature set \\n\\n Feature Subset Selection : \\n\\n e \\n Search Strategy : A strategy to select candidate subsets \\n o \\n Exhaustive evaluation of all subsets is impossible . \\n Feature Subset Selection \\n\\n Feature \\n subset \\n Objective \\n function \\n\\n Final feature subset \\n\\n PR \\n algorithm'), Document(metadata={}, page_content='Final feature subset \\n\\n PR \\n algorithm \\n\\n o \\n Asearch strategy navigates the vast feature subset space efficiently \\n â€œ Goodness â€ \\n\\n e Objective Function : Function to evaluate these candidates \\n\\n o \\n Evaluates candidate feature subsets \\n\\n o \\n Quantitative measure of the \" goodness \" or quality of a subset \\n o \\n Feedback from the objective function to guide the search strategy \\n\\n 31 \\n Search strategy and objective functions'), Document(metadata={}, page_content='31 \\n Search strategy and objective functions \\n\\n e \\n Forward selection \\n Add features step by step , choosing the one that improve the model the most at \\n each iteration . \\n e Backward elimination \\n Remove features step by step , choosing the one that does not improve the model \\n the at each iteration . \\n e Genetic Algorithm ( GA ) \\n o \\n An optimization technique , inspired by natural selection , used to evolve feature \\n subsets for improved model performance . \\n 32'), Document(metadata={}, page_content='Forward Selection \\n Performance threshold . \\n Elbow methods . \\n o < J \\n\\n\\n Genetic Algorithm ( GA ) for Feature Selection \\n\\n â€” \\n Fitness Population \\n\\n Evaluation at \\n\\n py ty 4 8 eR \\n What is a genetic algorithm ? \\n Ss \\n snail oe Recombination \\n 35 \\n\\n Genetic Algorithm ( GA ) for Feature Selection \\n\\n e Crossover \\n o \\n Produce good children \\n e Mutation \\n o Random change \\n o Exploration'), Document(metadata={}, page_content=\"e Selection \\n o \\n Select the best \\n o Survival of fittest \\n In feature selection and GA , we can encode the selected feature using 1 's and the discarded one by O 's \\n\\n crossover \\n\\n | [ 1011101010 \\n\\n Guemaeamer \\n\\n | 1100101010 } â€” â€” s > | MMIGIOIlo 1010 \\n 1011101110 NE \\n\\n 0011011001 \\n\\n 1100110001 \\n\\n encoding \\n\\n nut fatto ton â€” \\n\\n 0101181011 \\n â€” â€” \\n\\n selection evaluation _ \\n ere cai ( â€” offspring \\n new 110010 1110| \\n aimee 101110 10101 \\n\\n 00110 01001 \\n v decoding \\n\\n = \\n | \\n \\\\ fitness\"), Document(metadata={}, page_content='00110 01001 \\n v decoding \\n\\n = \\n | \\n \\\\ fitness \\n\\n computation \\n\\n roulette wheel \\n\\n 36 \\n\\n Wrapper methods for FSS \\n\\n Advantages \\n e Optimal Subset Selection \\n\\n Can detect the ideal feature subset for a \\n given machine learning algorithm . \\n\\n e \\n Managing Complex Relationships \\n They are effective in handling \\n relationships between features by \\n evaluating a subset , not a feature . \\n Disadvantages \\n e Computational Intensity'), Document(metadata={}, page_content='Tend to be computationally demanding , \\n particularly with large datasets . \\n e Algorithm Bias \\n\\n They may exhibit bias towards the \\n machine learning algorithm used for \\n feature evaluation . \\n 37 \\n\\n Comparison of Feature Selection methods'), Document(metadata={}, page_content=\"e \\n Unsupervised methods \\n o Advantages : Simple , fast , no need for labels \\n o Disadvantages : May miss complex relationships , does n't optimize for prediction \\n e \\n Filter methods \\n o Advantages : Fast , scalable , algorithm independent \\n Disadvantages : May miss complex relationships , suboptimal for specific algorithms \\n e \\n Wrapper methods \\n o Advantages : Finds optimal subset for algorithm , handles complex relationships \\n Disadvantages : Slow , algorithm bias \\n e \\n Forward selection\"), Document(metadata={}, page_content='e \\n Forward selection \\n o Advantages : Starts with no features , adds features incrementally \\n Disadvantages : suboptimal and does not consider feature interaction . \\n Backward elimination \\n o Advantages : Starts with all features , removes features incrementally \\n Disadvantages : Can be slow for high dimensional data \\n e \\n Genetic algorithms \\n o Advantages : Searches and optimizes feature sets through evolution \\n o \\n Disadvantages : Computationally intensive but thorough search \\n 38'), Document(metadata={}, page_content=\"Comparison of Feature Selection methods \\n\\n Unsupervised methods \\n\\n Filter methods \\n\\n Wrapper methods \\n\\n Forward selection \\n Backward elimination \\n\\n Genetic algorithms \\n\\n Simple & Fast \\n\\n No need for labels \\n\\n Fast \\n\\n Scalable \\n\\n algorithm independent \\n\\n Finds optimal subset for algorithm \\n Handles complex relationships \\n\\n Fast and simple \\n\\n Can be optimal for high dimensional \\n data \\n\\n Thorough search \\n\\n May miss complex relationships \\n Does n't optimize for prediction\"), Document(metadata={}, page_content='May miss complex relationships \\n\\n Suboptimal for specific algorithms \\n\\n Slow \\n algorithm bias \\n\\n Can be suboptimal for high- \\n dimensional data \\n\\n Can be slow \\n\\n Computationally intensive \\n\\n 39 \\n How to choose a Feature Selection method ? \\n Size of dataset e \\n Domain knowledge \\n\\n o Filter methods work well for large datasets o \\n Unsupervised methods do not use labels \\n Wrapper methods work well for small datasets o \\n Supervised methods use domain labels \\n Computational budget e Algorithm fit'), Document(metadata={}, page_content='o Filter methods are fast o \\n Wrapper methods tailor to specific ML algorithm \\n o Wrapper methods are slow o \\n Filter methods are independent of any algorithm \\n Need for interpretability e \\n Feature interactions \\n o Filter methods allow inspection of feature importance o \\n Wrapper methods handle feature interactions \\n Wrapper methods act as black box o \\n Filter methods assess features independently \\n The goal is to select an optimal subset of features'), Document(metadata={}, page_content='Main approaches : \\n o Unsupervised : Evaluate features independently using statistical measures \\n o \\n Supervised Filter : Fast evaluation of features using metrics like correlation \\n o Supervised Wrapper : Use model performance to search feature subsets , slow but thorough \\n Considerations : \\n\\n Dataset size and dimensionality \\n Computational budget and time constraints \\n Need for interpretability vs performance \\n Domain knowledge of features \\n Relationships between features'), Document(metadata={}, page_content='O00 0 0 \\n No one - size - fits - all method - choose based on goals , data , and constraints \\n Hybrid approaches combine strengths of different techniques \\n Feature selection critical step for ML pipelines , balances model performance and efficiengy')]\n"
     ]
    }
   ],
   "source": [
    "print(split_docs_DM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
