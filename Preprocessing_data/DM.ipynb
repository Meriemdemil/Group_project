{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdf_loader_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_loader_pre import CustomPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['& & & Outline 1- Data 1- Data 1-  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as _ variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance Objects oo NO a k. B WNY = a. K fo }  125 K 100 K 70 K 120 K 95 K 60 K 220 K 85 K 75 K 90 K  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  (= , # ) Ordinal _ Distinctness ( = , # ) Order ( < > ) Interval _ Distinctness ( = , # ) Order ( < > )  ( + , - ) Ratio _ Distinctness ( = , # ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents .  as integers . Note : Binary attributes are a special case of discrete attributes . © ®  : Values are real numbers . Discrete v / s Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables .  asymmetric attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  asymmetric attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data . Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Person : © Relational records © Data matrix ... To | _ [  |i | | _ _ Alvaro ] Valencia__F~ norelation [ 4 | | [ fom — _ ] Car : [ cart | Model | Year |__Value | Pers 1D |_1o1_| [ 1973 | 100000 =| [ 102 |  | 1965 | 330000 ] 0 _ | [ 04 | | 2005 | 150000 ] 4 _ | [ 0s [ renaut [ 1998 | — 2000 [ 3 _ _ ] 106 — [ _ Renautt [ 2007 | — 7o00 | 3 ] [ ior [ smart 199 ] 2000 ] 2 ] Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records © Data matrix ...  and  network Social or information networks ...  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes [ ) [ ) [ ) Relational records Data matrix ...  and Networks [ ) [ ) [ ) Transportation network Social or information networks ...  ( Sequence ) Data [ ) [ ) [ ) Video : sequence of image  sequence ... -| | J 34 j \\\\ i | Mi | \\\\ is ai py | c ot +1 \" ve 4 Lt jell 1s 202 J WI Vi YS 154 | if  \\\\ 1 WA 10 | Hi 54 1 10 40 50 60 70 80 0 9  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records © Data matrix ... /  and  network Streets ° Social or information networks ...  ( Sequence )  : sequence of image  sequence ...  RGB  images  of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images 2- Data preprocessing 2- Data preprocessing 2- Data preprocessing What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  adversely affects data processing efforts . Data r ° sy Quality ES 5Fe are r = Example : Poor data can result in wrong loan decisions . — Some credit - worthy candidates are denied loans — More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : — Noise and outliers — Wrong data — Fake data — Missing values — Duplicate data  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : © Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . * Case 1 : Outliers as Noise : © Outliers can be noise that disrupts data analysis . : Case 2 : Outliers as the Focus : © Incertain scenarios , outliers are the primary focus of analys © Credit card fraud detection  detection . © &  : ae Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence of outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers .  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis Missing values  .  A / S 21171 PC 17599 STON / O2 . 3101282 113803 $ 3.1 C123 373450 8.05 Reasons for missing values 3 Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) 330877 8.4583 [ ) [ ) [ ) [ ) Handling missing values Eliminate data objects or variables Estimate missing values = Example : time series of temperature = Example : census results Ignore the missing value during analysis  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . 2 How to handle duplicate data \\\\ Remove duplicate data objects . }  : When and Why ? prviemn ders Customers with multiple accounts may unintentionally accumulate points separately . © Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ):  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ mew_min , , new_max , ] ; v — min : . ; vi = — — — — — _ _ ( new _ max:—new _ min:)+new _ min : max : — min : Z - score normalization ( yu : mean , : standard deviation ): _ X 7b o. O z  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ): Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses 100 Min - max normalization : 3 59 vo Guarantees all attributes will have the Fs exact same scale . = 0 — — 0 20 40 60 80 Does not handle outliers well . Years old Z - score normalization : .  using min - max normalizatio Handles outliers . 3 Vv B ® E10 ; E 107 eo ° S Does not produce normalized data & fe * . 2 | 2 a ye with the exact same scale . E 0.54 a. A ene 8 - © ° 8 = ob , 0 corde i. J ® * ee g COT — 2 0.0 0.2 0.4 0.6 0.8  old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses pa lo } 50 Number of rooms Min - max normalization : Guarantees all attributes will have the 0 Miumalp . exact same scale . 0 20 40 60 80 . Years old Does not handle outliers well . 3 2Z - score normalization : s  using z - score normalizatio Handles outliers . 5 24 ° 33 } @ Does not produce normalized data z ol x : with the exact same scale . 3 Pee ny YS g T T T 2 22 = 3 4 5 = Years old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings . Discretization [ ie ae or L , CONTINUOUS DISCRETE VALUE VALUE Converting a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights . Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sample size h and | decision in researc itica ize is acr iate sample si ing an appropria Select analysis . eg ee Pe oe lee # PSs et g. S Shheaal ote o. P gor hae atk A Siee sed FS ? Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Random sampling with replacement Sampling methods Random sampling without replacement Simple random sampling Cluster sampling Origian ! data Equal probability of selecting any particular item Sampling without replacement ciel cana Once an object is selected , it is removed from the population Sampling with replacement Aselected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) 3- Similarity and  3- Similarity and  3- Similarity and  and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : © Quantifies data object likeness . © Higher values indicate greater similarity . © Typically within the range [ 0 , 1 ] .  : © Quantifies data object differences . * Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . —  : © Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : © Non - Negativity : m d(x , y ) 20 for all x and y. m d(x , y ) = 0 if and only if x = y. Symmetry : m d(x , y ) = d(y , x ) for all x and y.  : m d(x , Z ) S$ d(x , y ) + d(y , Z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a  : Oo § ( x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : Oo § ( x , y ) = S(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger similarities . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , ... Often symmetric , with higher values indicating stronger similarities . EEE Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger similarities . Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) d(x , y ) — 7 ? : number of attributes . Uk , ye : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ . Example :  matrix Example :  matrix 4 - 2 @p ! Ft ot | le | 03 ad | opt | of 2.828 ] 3.162 ] 5.099 ] _ a | pr | agosto aia ] 3.162 22 | ps | 3.62 ] iat io 0 , | ps [ | 5090 ] 3162 ) a ] Example :  matrix  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) a 1 / r a(x , y ) = { Solow — al k=1 Generalization of  . r : parameter n : number of attributes x , and y , are , respectively , the k * \" attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . siaciniaed  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths .  vector example : Hamming distance counts differing bits . @  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . &  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm , L~ » or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors )  ( applicable to numerical vectors ) n A - B » AWB fe n di y/ B ; i=1 i=1 A .B is dot product of the two vectors cos(9 ) It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . y Values are between -1 and 1 : -1 ( completely dissimilar ) © 1 ( perfect similarity ) . Omeans orthogonal ( no similarity ) . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors ) Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Linear correlation ( applicable to numerical vectors ) Perfect positive correlation > ( x%i- * ) ( i- ) > ( xi- * ) ? ( i -¥ ) ? Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . paren Omeans orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( 4 , t+ Son ) or thio * Soo t. A ) Jo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0 Soo = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ OO ” non - zero attributes . tis designed for asymmetric binary attributes . J = Su / ( fo thot fi ) fo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0 Jog = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 Jaccard = 0 Example : SMC vs  x= 1000000000 y= 0000001001 SMC = 0.7 = 0 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 = 0 How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  ° Similarity : Documents are considered similar if they use high number of common words .  in Celsius of  :  © Similarity : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute : Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute :  : 1 te=—9 % Nominal ue S= ) 9 vey  ( values mapped to integers 0 ton—1 , | s=1-—d where n is the number of values ) Interval or Ratio = lo s=—-d , s=7q . Q , s= \" d — min_d max - d — min_d s = l1- Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute']\n"
     ]
    }
   ],
   "source": [
    "path = r\"../DATA/3rd_year/Data_mining/Data - Part 1.pdf\"\n",
    "\n",
    "One_lecture = pdf_loader_pre.CustomPDFLoader(path)\n",
    "documents = One_lecture.load()\n",
    "texts = [doc.page_content for doc in documents] \n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = splitter.create_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export variables\n",
    "__all__ = ['split_docs']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "import logging\n",
    "\n",
    "class ModulePDFLoader:\n",
    "    def __init__(self, module_directory, ocr_lang=\"eng\"):\n",
    "        self.module_directory = module_directory\n",
    "        self.ocr_lang = ocr_lang\n",
    "        self.documents = []  # Initialize an empty list to store documents\n",
    "\n",
    "    def load_module_pdfs(self):\n",
    "        \"\"\"\n",
    "        Load all PDFs in the module directory and process them.\n",
    "        \"\"\"\n",
    "        for root, _, files in os.walk(self.module_directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    pdf_path = os.path.join(root, file)\n",
    "                    logging.info(f\"Processing PDF: {pdf_path}\")\n",
    "                    \n",
    "                    # Create an instance of CustomPDFLoader for each PDF\n",
    "                    pdf_loader = CustomPDFLoader(pdf_path, ocr_lang=self.ocr_lang)\n",
    "                    documents = pdf_loader.load()\n",
    "                    \n",
    "                    # Add the documents from this PDF to the list\n",
    "                    self.documents.extend(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 10 documents.\n"
     ]
    }
   ],
   "source": [
    "module_directory = r\"../DATA/3rd_year/Data_mining\"\n",
    "module_loader = ModulePDFLoader(module_directory, ocr_lang=\"eng\")\n",
    "module_loader.load_module_pdfs()\n",
    "\n",
    "# Access the processed documents\n",
    "documents = module_loader.documents\n",
    "texts = [doc.page_content for doc in documents]\n",
    "print(f\"Extracted text from {len(texts)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter_DM = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs_DM = splitter_DM.create_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export variables\n",
    "__all__ = ['split_docs_DM']  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
