{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../Preprocessing_data\"))  \n",
    "\n",
    "import pdf_loader_pre  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists: True\n",
      "Contents: ['data_mining.py', 'DM.ipynb', 'pdf_loader_pre.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # Check if the path exists\n",
    "# path_to_check = os.path.abspath(os.path.join('..', 'Preprocessing_data'))\n",
    "# print(f\"Path exists: {os.path.exists(path_to_check)}\")\n",
    "# print(f\"Contents: {os.listdir(path_to_check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 15:02:06,434 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:03:55,382 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:03:55,411 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Classification (part 1).pptx.pdf\n",
      "2025-04-05 15:03:55,414 - INFO - Starting PDF loading process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['& & & Outline 1- Data 1- Data 1-  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as _ variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance Objects oo NO a k. B WNY = a. K fo }  125 K 100 K 70 K 120 K 95 K 60 K 220 K 85 K 75 K 90 K  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  (= , # ) Ordinal _ Distinctness ( = , # ) Order ( < > ) Interval _ Distinctness ( = , # ) Order ( < > )  ( + , - ) Ratio _ Distinctness ( = , # ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents .  as integers . Note : Binary attributes are a special case of discrete attributes . © ®  : Values are real numbers . Discrete v / s Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables .  asymmetric attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  asymmetric attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data . Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Person : © Relational records © Data matrix ... To | _ [  |i | | _ _ Alvaro ] Valencia__F~ norelation [ 4 | | [ fom — _ ] Car : [ cart | Model | Year |__Value | Pers 1D |_1o1_| [ 1973 | 100000 =| [ 102 |  | 1965 | 330000 ] 0 _ | [ 04 | | 2005 | 150000 ] 4 _ | [ 0s [ renaut [ 1998 | — 2000 [ 3 _ _ ] 106 — [ _ Renautt [ 2007 | — 7o00 | 3 ] [ ior [ smart 199 ] 2000 ] 2 ] Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records © Data matrix ...  and  network Social or information networks ...  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes [ ) [ ) [ ) Relational records Data matrix ...  and Networks [ ) [ ) [ ) Transportation network Social or information networks ...  ( Sequence ) Data [ ) [ ) [ ) Video : sequence of image  sequence ... -| | J 34 j \\\\ i | Mi | \\\\ is ai py | c ot +1 \" ve 4 Lt jell 1s 202 J WI Vi YS 154 | if  \\\\ 1 WA 10 | Hi 54 1 10 40 50 60 70 80 0 9  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records © Data matrix ... /  and  network Streets ° Social or information networks ...  ( Sequence )  : sequence of image  sequence ...  RGB  images  of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images 2- Data preprocessing 2- Data preprocessing 2- Data preprocessing What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  adversely affects data processing efforts . Data r ° sy Quality ES 5Fe are r = Example : Poor data can result in wrong loan decisions . — Some credit - worthy candidates are denied loans — More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : — Noise and outliers — Wrong data — Fake data — Missing values — Duplicate data  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : © Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . * Case 1 : Outliers as Noise : © Outliers can be noise that disrupts data analysis . : Case 2 : Outliers as the Focus : © Incertain scenarios , outliers are the primary focus of analys © Credit card fraud detection  detection . © &  : ae Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence of outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers .  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis Missing values  .  A / S 21171 PC 17599 STON / O2 . 3101282 113803 $ 3.1 C123 373450 8.05 Reasons for missing values 3 Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) 330877 8.4583 [ ) [ ) [ ) [ ) Handling missing values Eliminate data objects or variables Estimate missing values = Example : time series of temperature = Example : census results Ignore the missing value during analysis  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . 2 How to handle duplicate data \\\\ Remove duplicate data objects . }  : When and Why ? prviemn ders Customers with multiple accounts may unintentionally accumulate points separately . © Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ):  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ mew_min , , new_max , ] ; v — min : . ; vi = — — — — — _ _ ( new _ max:—new _ min:)+new _ min : max : — min : Z - score normalization ( yu : mean , : standard deviation ): _ X 7b o. O z  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ): Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses 100 Min - max normalization : 3 59 vo Guarantees all attributes will have the Fs exact same scale . = 0 — — 0 20 40 60 80 Does not handle outliers well . Years old Z - score normalization : .  using min - max normalizatio Handles outliers . 3 Vv B ® E10 ; E 107 eo ° S Does not produce normalized data & fe * . 2 | 2 a ye with the exact same scale . E 0.54 a. A ene 8 - © ° 8 = ob , 0 corde i. J ® * ee g COT — 2 0.0 0.2 0.4 0.6 0.8  old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses pa lo } 50 Number of rooms Min - max normalization : Guarantees all attributes will have the 0 Miumalp . exact same scale . 0 20 40 60 80 . Years old Does not handle outliers well . 3 2Z - score normalization : s  using z - score normalizatio Handles outliers . 5 24 ° 33 } @ Does not produce normalized data z ol x : with the exact same scale . 3 Pee ny YS g T T T 2 22 = 3 4 5 = Years old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings . Discretization [ ie ae or L , CONTINUOUS DISCRETE VALUE VALUE Converting a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights . Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sample size h and | decision in researc itica ize is acr iate sample si ing an appropria Select analysis . eg ee Pe oe lee # PSs et g. S Shheaal ote o. P gor hae atk A Siee sed FS ? Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Random sampling with replacement Sampling methods Random sampling without replacement Simple random sampling Cluster sampling Origian ! data Equal probability of selecting any particular item Sampling without replacement ciel cana Once an object is selected , it is removed from the population Sampling with replacement Aselected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) 3- Similarity and  3- Similarity and  3- Similarity and  and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : © Quantifies data object likeness . © Higher values indicate greater similarity . © Typically within the range [ 0 , 1 ] .  : © Quantifies data object differences . * Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . —  : © Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : © Non - Negativity : m d(x , y ) 20 for all x and y. m d(x , y ) = 0 if and only if x = y. Symmetry : m d(x , y ) = d(y , x ) for all x and y.  : m d(x , Z ) S$ d(x , y ) + d(y , Z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a  : Oo § ( x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : Oo § ( x , y ) = S(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger similarities . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , ... Often symmetric , with higher values indicating stronger similarities . EEE Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger similarities . Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) d(x , y ) — 7 ? : number of attributes . Uk , ye : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ . Example :  matrix Example :  matrix 4 - 2 @p ! Ft ot | le | 03 ad | opt | of 2.828 ] 3.162 ] 5.099 ] _ a | pr | agosto aia ] 3.162 22 | ps | 3.62 ] iat io 0 , | ps [ | 5090 ] 3162 ) a ] Example :  matrix  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) a 1 / r a(x , y ) = { Solow — al k=1 Generalization of  . r : parameter n : number of attributes x , and y , are , respectively , the k * \" attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . siaciniaed  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths .  vector example : Hamming distance counts differing bits . @  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . &  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm , L~ » or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors )  ( applicable to numerical vectors ) n A - B » AWB fe n di y/ B ; i=1 i=1 A .B is dot product of the two vectors cos(9 ) It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . y Values are between -1 and 1 : -1 ( completely dissimilar ) © 1 ( perfect similarity ) . Omeans orthogonal ( no similarity ) . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors ) Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Linear correlation ( applicable to numerical vectors ) Perfect positive correlation > ( x%i- * ) ( i- ) > ( xi- * ) ? ( i -¥ ) ? Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . paren Omeans orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( 4 , t+ Son ) or thio * Soo t. A ) Jo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0 Soo = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ OO ” non - zero attributes . tis designed for asymmetric binary attributes . J = Su / ( fo thot fi ) fo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0 Jog = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 Jaccard = 0 Example : SMC vs  x= 1000000000 y= 0000001001 SMC = 0.7 = 0 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 = 0 How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  ° Similarity : Documents are considered similar if they use high number of common words .  in Celsius of  :  © Similarity : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute : Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute :  : 1 te=—9 % Nominal ue S= ) 9 vey  ( values mapped to integers 0 ton—1 , | s=1-—d where n is the number of values ) Interval or Ratio = lo s=—-d , s=7q . Q , s= \" d — min_d max - d — min_d s = l1- Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 15:05:36,905 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:05:36,905 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Classification (part 2).pdf\n",
      "2025-04-05 15:05:36,905 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:06:34,612 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:06:34,612 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 1).pdf\n",
      "2025-04-05 15:06:34,612 - INFO - Starting PDF loading process...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Notebook():\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPreprocessing_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDM\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdm_module\u001b[39;00m  \n\u001b[0;32m     10\u001b[0m DM \u001b[38;5;241m=\u001b[39m dm_module\u001b[38;5;241m.\u001b[39msplit_docs\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Now use dm_module.split_docs\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\importnb\\loader.py:190\u001b[0m, in \u001b[0;36mLoader.exec_module\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alias:\n\u001b[0;32m    188\u001b[0m     sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(alias, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\importnb\\loader.py:181\u001b[0m, in \u001b[0;36mLoader.exec_module\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot load module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m when \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_code() returns None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mCO_COROUTINE \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _get_co_flags_set(code\u001b[38;5;241m.\u001b[39mco_flags):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# if there isn't any async non sense then we proceed with convention.\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[43mbootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_frames_removed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maexec_module_sync(module)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\Desktop\\group_project\\Group_project\\Preprocessing_data\\DM.ipynb:117\u001b[0m\n\u001b[0;32m    115\u001b[0m module_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../DATA/3rd_year/Data_mining\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m module_loader \u001b[38;5;241m=\u001b[39m ModulePDFLoader(module_directory, ocr_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m \u001b[43mmodule_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Access the processed documents\u001b[39;00m\n\u001b[0;32m    120\u001b[0m documents \u001b[38;5;241m=\u001b[39m module_loader\u001b[38;5;241m.\u001b[39mdocuments\n",
      "File \u001b[1;32mc:\\Users\\DELL\\Desktop\\group_project\\Group_project\\Preprocessing_data\\DM.ipynb:95\u001b[0m, in \u001b[0;36mModulePDFLoader.load_module_pdfs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Create an instance of CustomPDFLoader for each PDF\u001b[39;00m\n\u001b[0;32m     94\u001b[0m pdf_loader \u001b[38;5;241m=\u001b[39m CustomPDFLoader(pdf_path, ocr_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mocr_lang)\n\u001b[1;32m---> 95\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Add the documents from this PDF to the list\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39mextend(documents)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\Desktop\\group_project\\Group_project\\Preprocessing_data\\pdf_loader_pre.py:164\u001b[0m, in \u001b[0;36mCustomPDFLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting PDF loading process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 164\u001b[0m     extracted_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_text(extracted_text)\n\u001b[0;32m    166\u001b[0m     final_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_names(cleaned_text)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\Desktop\\group_project\\Group_project\\Preprocessing_data\\pdf_loader_pre.py:42\u001b[0m, in \u001b[0;36mCustomPDFLoader.extract_text_from_pdf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m extracted_text\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Extract text from images using OCR\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m     44\u001b[0m     img_text \u001b[38;5;241m=\u001b[39m pytesseract\u001b[38;5;241m.\u001b[39mimage_to_string(img, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mocr_lang)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\pdf2image\\pdf2image.py:241\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[1;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[0;32m    236\u001b[0m         startupinfo \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSTARTUPINFO()\n\u001b[0;32m    237\u001b[0m         startupinfo\u001b[38;5;241m.\u001b[39mdwFlags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSTARTF_USESHOWWINDOW\n\u001b[0;32m    238\u001b[0m     processes\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    239\u001b[0m         (\n\u001b[0;32m    240\u001b[0m             thread_output_file,\n\u001b[1;32m--> 241\u001b[0m             \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m                \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartupinfo\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    244\u001b[0m         )\n\u001b[0;32m    245\u001b[0m     )\n\u001b[0;32m    247\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uid, proc \u001b[38;5;129;01min\u001b[39;00m processes:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from importnb import Notebook\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "with Notebook():\n",
    "    import Preprocessing_data.DM as dm_module  \n",
    "\n",
    "DM = dm_module.split_docs\n",
    "# Now use dm_module.split_docs\n",
    "print(dm_module.split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 15:06:56,875 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:08:39,222 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:08:39,267 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Classification (part 1).pptx.pdf\n",
      "2025-04-05 15:08:39,269 - INFO - Starting PDF loading process...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['& & & Outline 1- Data 1- Data 1-  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as _ variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance Objects oo NO a k. B WNY = a. K fo }  125 K 100 K 70 K 120 K 95 K 60 K 220 K 85 K 75 K 90 K  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  (= , # ) Ordinal _ Distinctness ( = , # ) Order ( < > ) Interval _ Distinctness ( = , # ) Order ( < > )  ( + , - ) Ratio _ Distinctness ( = , # ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents .  as integers . Note : Binary attributes are a special case of discrete attributes . © ®  : Values are real numbers . Discrete v / s Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables .  asymmetric attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  asymmetric attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data \\'s size .  centrality and dispersion in the data . Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Person : © Relational records © Data matrix ... To | _ [  |i | | _ _ Alvaro ] Valencia__F~ norelation [ 4 | | [ fom — _ ] Car : [ cart | Model | Year |__Value | Pers 1D |_1o1_| [ 1973 | 100000 =| [ 102 |  | 1965 | 330000 ] 0 _ | [ 04 | | 2005 | 150000 ] 4 _ | [ 0s [ renaut [ 1998 | — 2000 [ 3 _ _ ] 106 — [ _ Renautt [ 2007 | — 7o00 | 3 ] [ ior [ smart 199 ] 2000 ] 2 ] Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records © Data matrix ...  and  network Social or information networks ...  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes [ ) [ ) [ ) Relational records Data matrix ...  and Networks [ ) [ ) [ ) Transportation network Social or information networks ...  ( Sequence ) Data [ ) [ ) [ ) Video : sequence of image  sequence ... -| | J 34 j \\\\ i | Mi | \\\\ is ai py | c ot +1 \" ve 4 Lt jell 1s 202 J WI Vi YS 154 | if  \\\\ 1 WA 10 | Hi 54 1 10 40 50 60 70 80 0 9  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records © Data matrix ... /  and  network Streets ° Social or information networks ...  ( Sequence )  : sequence of image  sequence ...  RGB  images  of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images 2- Data preprocessing 2- Data preprocessing 2- Data preprocessing What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  adversely affects data processing efforts . Data r ° sy Quality ES 5Fe are r = Example : Poor data can result in wrong loan decisions . — Some credit - worthy candidates are denied loans — More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : — Noise and outliers — Wrong data — Fake data — Missing values — Duplicate data  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : © Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . * Case 1 : Outliers as Noise : © Outliers can be noise that disrupts data analysis . : Case 2 : Outliers as the Focus : © Incertain scenarios , outliers are the primary focus of analys © Credit card fraud detection  detection . © &  : ae Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence of outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers .  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis Missing values  .  A / S 21171 PC 17599 STON / O2 . 3101282 113803 $ 3.1 C123 373450 8.05 Reasons for missing values 3 Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) 330877 8.4583 [ ) [ ) [ ) [ ) Handling missing values Eliminate data objects or variables Estimate missing values = Example : time series of temperature = Example : census results Ignore the missing value during analysis  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . 2 How to handle duplicate data \\\\ Remove duplicate data objects . }  : When and Why ? prviemn ders Customers with multiple accounts may unintentionally accumulate points separately . © Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ):  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ mew_min , , new_max , ] ; v — min : . ; vi = — — — — — _ _ ( new _ max:—new _ min:)+new _ min : max : — min : Z - score normalization ( yu : mean , : standard deviation ): _ X 7b o. O z  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ): Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses 100 Min - max normalization : 3 59 vo Guarantees all attributes will have the Fs exact same scale . = 0 — — 0 20 40 60 80 Does not handle outliers well . Years old Z - score normalization : .  using min - max normalizatio Handles outliers . 3 Vv B ® E10 ; E 107 eo ° S Does not produce normalized data & fe * . 2 | 2 a ye with the exact same scale . E 0.54 a. A ene 8 - © ° 8 = ob , 0 corde i. J ® * ee g COT — 2 0.0 0.2 0.4 0.6 0.8  old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses pa lo } 50 Number of rooms Min - max normalization : Guarantees all attributes will have the 0 Miumalp . exact same scale . 0 20 40 60 80 . Years old Does not handle outliers well . 3 2Z - score normalization : s  using z - score normalizatio Handles outliers . 5 24 ° 33 } @ Does not produce normalized data z ol x : with the exact same scale . 3 Pee ny YS g T T T 2 22 = 3 4 5 = Years old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings . Discretization [ ie ae or L , CONTINUOUS DISCRETE VALUE VALUE Converting a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights . Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sample size h and | decision in researc itica ize is acr iate sample si ing an appropria Select analysis . eg ee Pe oe lee # PSs et g. S Shheaal ote o. P gor hae atk A Siee sed FS ? Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Random sampling with replacement Sampling methods Random sampling without replacement Simple random sampling Cluster sampling Origian ! data Equal probability of selecting any particular item Sampling without replacement ciel cana Once an object is selected , it is removed from the population Sampling with replacement Aselected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) 3- Similarity and  3- Similarity and  3- Similarity and  and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : © Quantifies data object likeness . © Higher values indicate greater similarity . © Typically within the range [ 0 , 1 ] .  : © Quantifies data object differences . * Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . —  : © Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : © Non - Negativity : m d(x , y ) 20 for all x and y. m d(x , y ) = 0 if and only if x = y. Symmetry : m d(x , y ) = d(y , x ) for all x and y.  : m d(x , Z ) S$ d(x , y ) + d(y , Z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a  : Oo § ( x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : Oo § ( x , y ) = S(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger similarities . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , ... Often symmetric , with higher values indicating stronger similarities . EEE Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger similarities . Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) d(x , y ) — 7 ? : number of attributes . Uk , ye : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ . Example :  matrix Example :  matrix 4 - 2 @p ! Ft ot | le | 03 ad | opt | of 2.828 ] 3.162 ] 5.099 ] _ a | pr | agosto aia ] 3.162 22 | ps | 3.62 ] iat io 0 , | ps [ | 5090 ] 3162 ) a ] Example :  matrix  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) a 1 / r a(x , y ) = { Solow — al k=1 Generalization of  . r : parameter n : number of attributes x , and y , are , respectively , the k * \" attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . siaciniaed  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths .  vector example : Hamming distance counts differing bits . @  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . &  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm , L~ » or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors )  ( applicable to numerical vectors ) n A - B » AWB fe n di y/ B ; i=1 i=1 A .B is dot product of the two vectors cos(9 ) It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . y Values are between -1 and 1 : -1 ( completely dissimilar ) © 1 ( perfect similarity ) . Omeans orthogonal ( no similarity ) . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors ) Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Linear correlation ( applicable to numerical vectors ) Perfect positive correlation > ( x%i- * ) ( i- ) > ( xi- * ) ? ( i -¥ ) ? Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . paren Omeans orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( 4 , t+ Son ) or thio * Soo t. A ) Jo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0 Soo = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ OO ” non - zero attributes . tis designed for asymmetric binary attributes . J = Su / ( fo thot fi ) fo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0 Jog = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 Jaccard = 0 Example : SMC vs  x= 1000000000 y= 0000001001 SMC = 0.7 = 0 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 = 0 How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  ° Similarity : Documents are considered similar if they use high number of common words .  in Celsius of  :  © Similarity : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute : Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute :  : 1 te=—9 % Nominal ue S= ) 9 vey  ( values mapped to integers 0 ton—1 , | s=1-—d where n is the number of values ) Interval or Ratio = lo s=—-d , s=7q . Q , s= \" d — min_d max - d — min_d s = l1- Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 15:10:22,375 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:10:22,375 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Classification (part 2).pdf\n",
      "2025-04-05 15:10:22,375 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:11:10,886 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:11:10,903 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 1).pdf\n",
      "2025-04-05 15:11:10,903 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:12:16,795 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:12:16,807 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 2).pptx.pdf\n",
      "2025-04-05 15:12:16,807 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:13:25,542 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:13:25,542 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 3).pdf\n",
      "2025-04-05 15:13:25,542 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:14:12,504 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:14:12,504 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Clustering (part 4).pdf\n",
      "2025-04-05 15:14:12,504 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:14:42,591 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:14:42,591 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Data - Part 1.pdf\n",
      "2025-04-05 15:14:42,591 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:16:21,406 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:16:21,407 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Data - Part 2.pdf\n",
      "2025-04-05 15:16:21,408 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:17:35,116 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:17:35,116 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Dimensionality Reduction (part 1).pdf\n",
      "2025-04-05 15:17:35,116 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:18:47,731 - INFO - PDF loading process completed successfully.\n",
      "2025-04-05 15:18:47,731 - INFO - Processing PDF: ../DATA/3rd_year/Data_mining\\Dimensionality Reduction (part 2).pdf\n",
      "2025-04-05 15:18:47,731 - INFO - Starting PDF loading process...\n",
      "2025-04-05 15:20:05,894 - INFO - PDF loading process completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 10 documents.\n",
      "[Document(metadata={}, page_content='Classification ( Part1 ) & Classification ( Part1 ) & Classification ( Part1 ) &  ❏ Introduction to Classification ❏  ❏ Introduction to Decision tree ❏  ❏  and  ❏  2  @ introduction to Classification LJ  LI Introduction to Decision tree LI  LJ  and  LI  ❏ Introduction to Classification ❏  ❏ Introduction to Decision tree ❏  ❏  and  ❏  2 What is classification ?  – Attributes : Descriptive features of instances ( x ) . –  : Categorical labels representing the class of an instance ( Y ) .'), Document(metadata={}, page_content='representing the class of an instance ( Y ) . Classification – Assigning class labels to instances based on attributes . Classifier ( Model ) – A function f(x ) used to classify unseen data x by assigning a class label y. 3 What is classification ?  set — » a — » Class label ( x ) ( y ) ¢  — Attributes : Descriptive features of instances ( x ) . —  : Categorical labels representing the class of an instance ( Y ) . ¢ Classification — Assigning class labels to instances based on attributes . *'), Document(metadata={}, page_content='class labels to instances based on attributes . * Classifier ( Model ) — A function f(x ) used to classify unseen data x by assigning a class label y. What is classification ?  – Attributes : Descriptive features of instances ( x ) . –  : Categorical labels representing the class of an instance ( Y ) . Classification – Assigning class labels to instances based on attributes . Classifier ( Model ) – A function f(x ) used to classify unseen data x by assigning a class label y. 3 Classification'), Document(metadata={}, page_content='x by assigning a class label y. 3 Classification example  labels 4 Classification example  | “ 1 |Joseph | ( e. S  labels  ? J Classification example  labels 4 Applications of classification Health – Medical diagnosis – Patient risk categorization  – Spam filtering – Malware classification Banking and Finance – Loan default prediction – Credit card fraud detection Retail and E - commerce – Customer purchase pattern classification – Sentiment analysis from customer reviews Transportation and'), Document(metadata={}, page_content='analysis from customer reviews Transportation and Logistics – Cargo classification for customs – Driver behavior classification for insurance 5 Applications of classification Health ~ Medical diagnosis - Patient risk categorization  - Spam filtering - Malware classification Banking and Finance ~ Loan default prediction ~ Credit card fraud detection Retail and E - commerce ~ Customer purchase pattern classification ~ Sentiment analysis from customer reviews Transportation and Logistics -~ Cargo'), Document(metadata={}, page_content='reviews Transportation and Logistics -~ Cargo classification for customs -~ Driver behavior classification for insurance Applications of classification Health – Medical diagnosis – Patient risk categorization  – Spam filtering – Malware classification Banking and Finance – Loan default prediction – Credit card fraud detection Retail and E - commerce – Customer purchase pattern classification – Sentiment analysis from customer reviews Transportation and Logistics – Cargo classification for'), Document(metadata={}, page_content='and Logistics – Cargo classification for customs – Driver behavior classification for insurance 5 Role of  – Used to predict class labels for new , unseen data . – Learn patterns from historical data to make predictions .  – Help understand distinguishing features of different classes . – Analyzes the data to find common characteristics and patterns . 6 Role of  ¢  — Used to predict class labels for new , unseen data . — Learn patterns from historical data to make predictions . ¢  — Help'), Document(metadata={}, page_content=\"historical data to make predictions . ¢  — Help understand distinguishing features of different classes . — Analyzes the data to find common characteristics and patterns . Role of  – Used to predict class labels for new , unseen data . – Learn patterns from historical data to make predictions .  – Help understand distinguishing features of different classes . – Analyzes the data to find common characteristics and patterns . 6 Role of  – Example : Classifying email messages as ' urgent ' or '\"), Document(metadata={}, page_content=\": Classifying email messages as ' urgent ' or ' non - urgent ' based on their content .  – Example : Identifying key factors that differentiate high - risk patients in healthcare . 7 Role of  ¢  — Example : Classifying email messages as ‘ urgent ’ or ‘ non - urgent ’ based on their content . ¢  — Example : \\\\dentifying key factors that differentiate high - risk patients in healthcare . Role of  – Example : Classifying email messages as ' urgent ' or ' non - urgent ' based on their content .  –\"), Document(metadata={}, page_content=\"' or ' non - urgent ' based on their content .  – Example : Identifying key factors that differentiate high - risk patients in healthcare . 7  for  ( Training ) – Learning a model from a labeled training dataset . – Use learning algorithm to build models . Deduction ( Testing ) – Applying the learned model to new instances ( unseen ) to predict class labels . – Assessing the model 's performance to measure its generalization capability . The feedback from testing is often used to refine and\"), Document(metadata={}, page_content=\"feedback from testing is often used to refine and improve the training process . 8  for Classification ¢ Induction ( Training ) — Learning a model from a labeled training dataset . — Use learning algorithm to build models . ¢ Deduction ( Testing ) — Applying the learned model to new instances ( unseen ) to predict class labels . — Assessing the model 's performance to measure its generalization capability . The feedback from testing is often used to refine and improve the training process .  :\"), Document(metadata={}, page_content='to refine and improve the training process .  : “  ” Deduction : \"  ”  for  ( Training ) – Learning a model from a labeled training dataset . – Use learning algorithm to build models . Deduction ( Testing ) – Applying the learned model to new instances ( unseen ) to predict class labels . – Assessing the model \\'s performance to measure its generalization capability . The feedback from testing is often used to refine and improve the training process . 8  ❏ Introduction to Classification ❏  ❏'), Document(metadata={}, page_content='. 8  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  9  LJ Introduction to Classification LI  MM Introduction to  LI  LJ  and  LI  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  9  – Initial point of decision - making .  – Question based on a single attribute . – Attribute test .  – The final classification outcome . Mammal classification tree 10  point of decision - making .  based on a single attribute Attribute test .  final classification outcome .'), Document(metadata={}, page_content='Attribute test .  final classification outcome .  node Goon ) Yes NO one - * \\\\ Z N “  se nodes mammals Mammal classification tree 10  – Initial point of decision - making .  – Question based on a single attribute . – Attribute test .  – The final classification outcome . Mammal classification tree 10 Deduction in  at  – Apply initial attribute test . –  . Visit next node –  .  – Determine final classification . Mammal classification tree 11 Deduction in  at  — Apply initial attribute test .'), Document(metadata={}, page_content='in  at  — Apply initial attribute test . data Name _ | Body temperature |  lies Class |  | ... ? | ‘ Nn I jon- jmamma —  .  | ¢ Visit next node | | —  . / ¢  owl ?  } ee — Determine final classification . Mammal classification tree 11 Deduction in  at  – Apply initial attribute test . –  . Visit next node –  .  – Determine final classification . Mammal classification tree 11 Hunt ’s Algorithm for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand'), Document(metadata={}, page_content='all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has instances of only one class . All instances 12 Hunt ’s Algorithm for decision tree building *  - Start with a root node containing all instances . All instances 12 Hunt ’s Algorithm for'), Document(metadata={}, page_content='. All instances 12 Hunt ’s Algorithm for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has instances of only one class . All instances 12 Hunt ’s Algorithm for decision tree'), Document(metadata={}, page_content='instances 12 Hunt ’s Algorithm for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has instances of only one class . All instances 0 % yes , 100 %  =  1 ? 40 % yes , 60 %  = No'), Document(metadata={}, page_content='0 % yes , 100 %  =  1 ? 40 % yes , 60 %  = No yes No 13 Hunt ’s Algorithm for decision tree building *  - Start with a root node containing all instances . | All instances * Expansion and  - Expand nodes with mixed class instances . - Select the attribute test based on a splitting Attribute 1 ? criterion . — Create child nodes for each test outcome - Distribute instances accordingly . 0 % yes , 100 % No 40 % yes , 60 %  =  = No x 13 Hunt ’s Algorithm for decision tree building  – Start with a'), Document(metadata={}, page_content='for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has instances of only one class . All instances 0 % yes , 100 %  =  1 ? 40 % yes , 60 %  = No yes No 13 Hunt ’s Algorithm for'), Document(metadata={}, page_content='yes , 60 %  = No yes No 13 Hunt ’s Algorithm for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has instances of only one class . All instances Defaulted =  1 ? 0 % yes , 100 %'), Document(metadata={}, page_content='. All instances Defaulted =  1 ? 0 % yes , 100 %  =  2 ? 100 % yes , 0 %  = Yes V1 V2 V1 V2 14 ’s Algorithm for decision tree building ¢  - Start with a root node containing all instances . | ) JUSancee * Expansion and  - Expand nodes with mixed class instances . - Select the attribute test based on a splitting criterion . - Create child nodes for each test outcome - Distribute instances accordingly . Attribute 1 ? Defaulted = No % 100 % yes , 0 % No 0 % yes , 100 %  =  = No ¢  - Continue'), Document(metadata={}, page_content=', 0 % No 0 % yes , 100 %  =  = No ¢  - Continue expansion for nodes with mixed class instances . Hunt ’s Algorithm for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has'), Document(metadata={}, page_content='instances . Termination – Stop when a node has instances of only one class . All instances Defaulted =  1 ? 0 % yes , 100 %  =  2 ? 100 % yes , 0 %  = Yes V1 V2 V1 V2 14 Hunt ’s Algorithm for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes'), Document(metadata={}, page_content='accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has instances of only one class . All instances Defaulted =  1 ? Class =  2 ? Class = Yes V1 V2 V1 V2 15 Hunt ’s Algorithm for decision tree building *  - Start with a root node containing all instances . All instances * Expansion and  - Expand nodes with mixed class instances . - Select the attribute test based on a splitting criterion . - Create child nodes for each test outcome -'), Document(metadata={}, page_content='. - Create child nodes for each test outcome - Distribute instances accordingly . Attribute 1 ? Defaulted = No * Termination - Stop when a node has instances of only one Class =  = No class . ¢  - Continue expansion for nodes with mixed class instances . Hunt ’s Algorithm for decision tree building  – Start with a root node containing all instances . Expansion and  – Expand nodes with mixed class instances . – Select the attribute test based on a splitting criterion . – Create child nodes for'), Document(metadata={}, page_content='a splitting criterion . – Create child nodes for each test outcome – Distribute instances accordingly .  – Continue expansion for nodes with mixed class instances . Termination – Stop when a node has instances of only one class . All instances Defaulted =  1 ? Class =  2 ? Class = Yes V1 V2 V1 V2 15 Hunt ’s Algorithm : Example 10 instances ( 3 Yes , 7 No ) Defaulted = No 16 Hunt ’s Algorithm : Example 10 instances ( 3 Yes , 7 No ) Defaulted =  ad  1 2 3 4 5 6 7 8 9 as So 16 Hunt ’s Algorithm :'), Document(metadata={}, page_content='1 2 3 4 5 6 7 8 9 as So 16 Hunt ’s Algorithm : Example 10 instances ( 3 Yes , 7 No ) Defaulted = No 16 Hunt ’s Algorithm : Example 3 instances ( 0 Yes , 3 No ) Defaulted =  ? 7 instances ( 3 Yes , 4 No ) Defaulted = No yes No 17 Hunt ’s Algorithm :  ? 3 instances ( 0 Yes , 3 No ) Defaulted = No 7 instances ( 3 Yes , 4 No ) Defaulted = No * i. D  [ Single [ 125k _ _ |No___| Married | 100 K Single | 70 K Yes [ warmed [ 120 K [ No ] Divorced | 95 K  | 60 K  [ Dworeed [ 220 K [ No |  85 K  |75 K'), Document(metadata={}, page_content='K  | 60 K  [ Dworeed [ 220 K [ No |  85 K  |75 K  |90 K Yes 17 1 2 3 4 5 6 7 8 9 par  ’s Algorithm : Example 3 instances ( 0 Yes , 3 No ) Defaulted =  ? 7 instances ( 3 Yes , 4 No ) Defaulted = No yes No 17 Hunt ’s Algorithm :  =  ? 3 instances ( 0 Yes , 3 No ) Defaulted =  status ? 4 instances ( 3 Yes , 1 No ) Defaulted =  , divorced Married yes No 18 Hunt ’s Algorithm :  ?  = No status ? Single , divorced Married 4 instances ( 3 Yes , 1 No ) Defaulted = Yes 3 instances ( 0 Yes , 3 No )'), Document(metadata={}, page_content='No ) Defaulted = Yes 3 instances ( 0 Yes , 3 No ) Defaulted = No * Ro [ sinale [ e. K [ Yes | | ’s Algorithm :  =  ? 3 instances ( 0 Yes , 3 No ) Defaulted =  status ? 4 instances ( 3 Yes , 1 No ) Defaulted =  , divorced Married yes No 18 ’s Algorithm :  =  ? Defaulted =  status ? Single , divorced Married yes No 1 instances ( 0 Yes , 1 No ) Defaulted =  income < 78 K ? 3 instances ( 3 Yes , 0 No ) Defaulted = Yes yes No 19 Hunt ’s Algorithm :  ?  = No status ? Single , divorced  income < 78 K'), Document(metadata={}, page_content='?  = No status ? Single , divorced  income < 78 K ? Defaulted = No 1 instances ( 0 Yes , 1 No ) 3 instances ( 3 Yes , O No ) Defaulted =  =  ’s Algorithm :  =  ? Defaulted =  status ? Single , divorced Married yes No 1 instances ( 0 Yes , 1 No ) Defaulted =  income < 78 K ? 3 instances ( 3 Yes , 0 No ) Defaulted = Yes yes No 19 Hunt ’s Algorithm :  =  ? Defaulted =  status ? Single , divorced Married yes  =  income < 78 K ? Defaulted = Yes yes No 20 Hunt ’s Algorithm :  ?  = No status ? Single'), Document(metadata={}, page_content='20 Hunt ’s Algorithm :  ?  = No status ? Single , divorced  income < 78 K ? yes Defaulted = No 1 2 3 4 5 6 7 8 9 ry S Defaulted =  = Yes 20 Hunt ’s Algorithm :  =  ? Defaulted =  status ? Single , divorced Married yes  =  income < 78 K ? Defaulted = Yes yes No 20 Decision tree algorithm questions How to handle an empty test outcome ? All attributes values are identical BUT different class labels ? How to determine the best attribute test ? What are the stopping criteria for the algorithm ? 21'), Document(metadata={}, page_content='are the stopping criteria for the algorithm ? 21 Decision tree algorithm questions How to handle an empty test outcome ? All attributes values are identical BUT different class labels ? How to determine the best attribute test ? What are the stopping criteria for the algorithm ? Decision tree algorithm questions How to handle an empty test outcome ? All attributes values are identical BUT different class labels ? How to determine the best attribute test ? What are the stopping criteria for the'), Document(metadata={}, page_content='test ? What are the stopping criteria for the algorithm ? 21 How to handle an empty test outcome ?  ? No training instances with specific attribute values . These attribute values can happen in testing instances .  the most common class label from parent node to empty nodes . 22 How to handle an empty test outcome ?  ? ¢ No training instances with specific attribute values . ¢ These attribute values can happen in testing instances .  the most common class label from parent node to empty nodes .'), Document(metadata={}, page_content='class label from parent node to empty nodes . 22 How to handle an empty test outcome ?  ? No training instances with specific attribute values . These attribute values can happen in testing instances .  the most common class label from parent node to empty nodes . 22 All attributes values are identical BUT different class labels ?  ? Expansion is impossible . We ca n’t build leaves contains the same class .  it a leaf node and assign it the most common class label in the training instances'), Document(metadata={}, page_content='most common class label in the training instances associated with this node . 23 All attributes values are identical BUT different class labels ?  ? ¢ Expansion is impossible . ¢ We ca n’t build leaves contains the same class .  it a leaf node and assign it the most common class label in the training instances associated with this node . 23 All attributes values are identical BUT different class labels ?  ? Expansion is impossible . We ca n’t build leaves contains the same class .  it a leaf'), Document(metadata={}, page_content='build leaves contains the same class .  it a leaf node and assign it the most common class label in the training instances associated with this node . 23  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  24  LJ Introduction to Classification LI  LI Introduction to  MB  LJ  and  LI  24  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  24 How to determine the best attribute test ? How to split ? Splitting criterion How to determine the best attribute test ?'), Document(metadata={}, page_content='How to determine the best attribute test ? How to split ? Splitting criterion How to determine the best attribute test ? How to split ? Splitting criterion  & Attribute type  – Outcomes : True or False . –  : Two outcomes  –  : More than two possible outcomes . –  : Two outcomes Multiway split Binary split 26  & Attribute type - Cold- blooded blooded ¢  — Outcomes : True or False . —  : Two outcomes ¢  split F vention Status —  : More than two possible outcomes . —  : Two outcomes  split 26  &'), Document(metadata={}, page_content='outcomes . —  : Two outcomes  split 26  & Attribute type  – Outcomes : True or False . –  : Two outcomes  –  : More than two possible outcomes . –  : Two outcomes Multiway split Binary split 26  & Attribute type Binary split can be used with more than two outcomes ( Ex . CART algorithm ) . Binary split { by grouping attribute values } Multiway split 27  & Attribute type  split  { Married } { Single , { Single } { Married , { Single , { Divorced } Divorced } Divorced } Married } Binary split {'), Document(metadata={}, page_content='} Divorced } Divorced } Married } Binary split { by grouping attribute values } OR OR Binary split can be used with more than two outcomes ( Ex . CART algorithm ) . 27  & Attribute type Binary split can be used with more than two outcomes ( Ex . CART algorithm ) . Binary split { by grouping attribute values } Multiway split 27  & Attribute type 2^(k)-1 potential groupings . How to select the optimal grouping ? Binary split { by grouping attribute values } Multiway split 28  & Attribute type'), Document(metadata={}, page_content='values } Multiway split 28  & Attribute type  split  OR OR { Married } { Single , { Single } { Married , { Single , { Divorced } Divorced } Divorced } Married } Binary split { by grouping attribute values } 24(k)-1 potential groupings . How to select the optimal grouping ? 28  & Attribute type 2^(k)-1 potential groupings . How to select the optimal grouping ? Binary split { by grouping attribute values } Multiway split 28  & Attribute type  – Binary or multiway splits ( like nominal attributes'), Document(metadata={}, page_content='or multiway splits ( like nominal attributes ) . – Binary split : grouping should not violate the order . How many possible grouping ? 29  & Attribute type — Binar r multiwa S lits { Small , { Large , { Small } { Medium , Large , . ary 0 . ys . P Medium }  }  } ( like nominal attributes ) . — Binary split : grouping should ‘ not violate the order . How many possible grouping ? *  < { Small , { Medium , Large }  } ae  & Attribute type  – Binary or multiway splits ( like nominal attributes ) . –'), Document(metadata={}, page_content='multiway splits ( like nominal attributes ) . – Binary split : grouping should not violate the order . How many possible grouping ? 29  & Attribute type  test ( A < V ) . Min . Train(attribute ) < V < Max . Train(attribute ) .  values can be considered for splits V.  ( Vi ≤ A < Vi+1 ) for i = 1 , ... , k.  intervals . Discretization to convert to ordinal attribute . Test condition deﬁned like ordinal attribute . 30  & Attribute type  ( 0 ) ( 0 ) ( 0 )  test ( A < V ) . > 80 K Min .'), Document(metadata={}, page_content='( 0 ) ( 0 ) ( 0 )  test ( A < V ) . > 80 K Min . Train(attribute ) < V < Max . Train(attribute ) .  values can be considered for splits V. 30  & Attribute type  test ( A < V ) . Min . Train(attribute ) < V < Max . Train(attribute ) .  values can be considered for splits V.  ( Vi ≤ A < Vi+1 ) for i = 1 , ... , k.  intervals . Discretization to convert to ordinal attribute . Test condition deﬁned like ordinal attribute . 30  & Attribute type  test ( A < V ) . Min . Train(attribute ) < V < Max .'), Document(metadata={}, page_content='( A < V ) . Min . Train(attribute ) < V < Max . Train(attribute ) .  values can be considered for splits V.  ( Vi ≤ A < Vi+1 ) for i = 1 , ... , k.  intervals . Discretization to convert to ordinal attribute . Test condition deﬁned like ordinal attribute . 31  & Attribute type  > 80 K  test ( A < V ) . Min . Train(attribute ) < V < Max . Train(attribute ) .  values can be considered for splits V.  ( Vis A < Vit1 ) fori = 1 , ... , k.  intervals . © Discretization to convert to ordinal attribute'), Document(metadata={}, page_content='© Discretization to convert to ordinal attribute . Test condition defined like ordinal attribute . ( 10 K , 25 K } { 25 K , 50 K } { 50 K , 80 K } 31  & Attribute type  test ( A < V ) . Min . Train(attribute ) < V < Max . Train(attribute ) .  values can be considered for splits V.  ( Vi ≤ A < Vi+1 ) for i = 1 , ... , k.  intervals . Discretization to convert to ordinal attribute . Test condition deﬁned like ordinal attribute . 31  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and'), Document(metadata={}, page_content='to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  32  LJ Introduction to Classification LJ  LI Introduction to  LI  ME impurity Measures and  LJ  32  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  32  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  33  LJ Introduction to Classification LJ  LI Introduction to  LI  ME impurity Measures and  LJ  33  ❏ Introduction to Classification ❏  ❏ Introduction to  ❏  ❏  and  ❏  33 Measures for Selecting an  –'), Document(metadata={}, page_content='❏  ❏  and  ❏  33 Measures for Selecting an  –  tests leading to pure child nodes . Why ? – Pure nodes helps to stop expanding nodes . – Impure nodes need more expansions , deepening the tree . Concerns with  – Susceptible to overfitting . – Harder to interpret . – Longer training and testing times . What is pure nodes ? 34 Measures for Selecting an  ¢ Objective —  tests leading to pure child nodes . ¢ Why ? — Pure nodes helps to stop expanding nodes . — Impure nodes need more expansions ,'), Document(metadata={}, page_content='nodes . — Impure nodes need more expansions , deepening the tree . ¢ Concerns with  — Susceptible to overfitting . — Harder to interpret . — Longer training and testing times . What is pure nodes ? Measures for Selecting an  –  tests leading to pure child nodes . Why ? – Pure nodes helps to stop expanding nodes . – Impure nodes need more expansions , deepening the tree . Concerns with  – Susceptible to overfitting . – Harder to interpret . – Longer training and testing times . What is pure'), Document(metadata={}, page_content='Longer training and testing times . What is pure nodes ? 34 Pure set VS Impure set Pure set Impure set Pure node – Contains the same class label . – Only one class label have probability 1 . Impure node – Contains a mixture of different class labels . – Maximum impurity occurs when class labels are equally probable . 35 Pure set VS Impure set ¢ Pure node — Contains the same class label . — Only one class label have probability 1 . Pure set ¢ Impure node — Contains a mixture of different class'), Document(metadata={}, page_content='node — Contains a mixture of different class labels . — Maximum impurity occurs when class labels are equally probable . Impure set 35 Pure set VS Impure set Pure set Impure set Pure node – Contains the same class label . – Only one class label have probability 1 . Impure node – Contains a mixture of different class labels . – Maximum impurity occurs when class labels are equally probable . 35  for a  of a node measures how dissimilar the class labels of instances in a node . : relative'), Document(metadata={}, page_content='class labels of instances in a node . : relative frequency of class i at node t , c is the number of classes . 36  for a  of a node measures how dissimilar the class labels of instances in a node . c — l Entropy = — Spit ) logs pi(t ) = c—1  = 1 — S — pit ) ? i=0  = 1 — max [ p;(t ) ] Di ( t ) : relative frequency of class i at node t , c is the number of classes . 36  for a  of a node measures how dissimilar the class labels of instances in a node . : relative frequency of class i at node t ,'), Document(metadata={}, page_content='. : relative frequency of class i at node t , c is the number of classes . 36  for a  impurity for a single - class node . Maximum impurity for equally distributed classes . The three measures are consistent . 37  for a  impurity for a single - class node . Maximum impurity for equally distributed classes . The three measures are consistent . 0.9- 0.8 } 0.7 + 0.6F 0.5 0.4- 0.3 } 37  for a  impurity for a single - class node . Maximum impurity for equally distributed classes . The three measures'), Document(metadata={}, page_content='equally distributed classes . The three measures are consistent . 37 Examples 38 Class=0 | 0 Class=0 | 1 1 Examples 38 Examples 38 Examples 39  = 1 — ( 0/6 ) ? — ( 6/6 ) ? = 0 | Class=0 | 0 | Entropy = — ( 0/6 ) logs(0/6 ) — ( 6/6 ) log2(6/6 ) = 0 | Class=1 | 6 | Error = 1 — max(0/6,6/6 ] = 0 Gini = 1 — ( 1/6 ) ? — ( 5/6 ) ? = 0.278 Entropy = — ( 1/6 ) logs(1/6 ) — ( 5/6 ) logo(5/6 ) = 0.650 Class=1 Error = 1 — max{1/6 , 5/6 ] = 0.167 Gini = 1 - ( 3/6 ) ? - ( 3/6 ) ? = 0.5 Entropy = — ( 3/6 )'), Document(metadata={}, page_content='- ( 3/6 ) ? - ( 3/6 ) ? = 0.5 Entropy = — ( 3/6 ) loga(3/6 ) — ( 3/6 ) loga(3/6 ) = 1 Error = 1 — max[3/6 , 3/6 ] = 0.5 39 Examples 39  of  a node with N instances into k children { v1 , v2 , ... , vk } . : Number of instances in child node vj . : Impurity value of node vj . Collective impurity of child nodes : – Weighted sum of node children impurities . 40  of  ¢ Splits a node with N instances into k children { v1 , v2 , ... , vk } . ° N(v , ) : Number of instances in child node vj . ° I(v ;)'), Document(metadata={}, page_content=': Number of instances in child node vj . ° I(v ;) : Impurity value of node vj . N I ( children ) = o. F i k I(v ;) j * Collective impurity of child nodes : - Weighted sum of node children impurities . 40  of  a node with N instances into k children { v1 , v2 , ... , vk } . : Number of instances in child node vj . : Impurity value of node vj . Collective impurity of child nodes : – Weighted sum of node children impurities . 40 Which split is better ? 41 Which split is better ? ( Marital , \\\\.'), Document(metadata={}, page_content='? 41 Which split is better ? ( Marital , \\\\. Status _ / 41 Which split is better ? 41 Which split is better ? 42 Which split is better ? 0 0 3 3 I( = yes ) = -3 logs aa logs - 0 3 3 4 4 I( = no ) = = logs 777 logs 7= 0.985 I( ) = = x. O+ i x 0.985 = 0.690 Ma rital\\\\ \\\\. Status_/ Divorced 2 2 3 3 I( = Single ) 5 logs 7 5 082 5 0.971 I( = Married ) ; logs : : logs ; = 0 1 | 1 I( = Divorced ) 3 logs a5 logs 3= 1.000 : a ) 3 2 = I( ) = io * 0.971 + io * O+ 0 * 1 = 0.686 42 Which split is better ? 42'), Document(metadata={}, page_content='* O+ 0 * 1 = 0.686 42 Which split is better ? 42 Identifying the best attribute test condition Compare parent node impurity ( I(parent ) ) before splitting with after splitting . Δ = I(parent)−I(children ) Large ( Δ ) ⇒ better attribute test condition . Δ_info : information gain when entropy is used . I(parent)≥I(children ): Gain is always non - negative . Decision trees select conditions with maximum gain for splitting . Maximizing gain is equivalent to minimizing weighted child impurity . 43'), Document(metadata={}, page_content='to minimizing weighted child impurity . 43 Identifying the best attribute test condition Compare parent node impurity ( I(parent ) ) before splitting with after splitting . A=|(parent)-I(children ) Large ( A ) = > better attribute test condition . * A_info : information gain when entropy is used . I(parent)2I(children ): Gain is always non - negative . Decision trees select conditions with maximum gain for splitting . Maximizing gain is equivalent to minimizing weighted child impurity .'), Document(metadata={}, page_content='to minimizing weighted child impurity . Identifying the best attribute test condition Compare parent node impurity ( I(parent ) ) before splitting with after splitting . Δ = I(parent)−I(children ) Large ( Δ ) ⇒ better attribute test condition . Δ_info : information gain when entropy is used . I(parent)≥I(children ): Gain is always non - negative . Decision trees select conditions with maximum gain for splitting . Maximizing gain is equivalent to minimizing weighted child impurity . 43 Splitting'), Document(metadata={}, page_content='minimizing weighted child impurity . 43 Splitting of  Δ_info = 0.881 - 0.690 = 0.191 Δ_info = 0.881 - 0.686 = 0.195 44 Splitting of  3 3 7 ae I(parent ) = a. T ; logs io io logs io > 0.881 Home a Pienek / Marital pc i  : 0 ] | Yes : 3 No : 3 No : 4 A_info = 0.881 - 0.690 A_info = 0.881 - 0.686 = 0.191 = 0.195 44 Splitting of  Δ_info = 0.881 - 0.690 = 0.191 Δ_info = 0.881 - 0.686 = 0.195 44 Splitting of  Δ_info = 0.881 - 0.690 = 0.191 Δ_info = 0.881 - 0.686 = 0.195 45 Splitting of  3 3 7 7 re'), Document(metadata={}, page_content='0.881 - 0.686 = 0.195 45 Splitting of  3 3 7 7 re I(parent ) = a. T ; logs io io log . io > 0.881 Home — .  meee _ Status .  : 0 } | Yes : 3 No : 3 || No : 4 A_info = 0.881 - 0.690 A_info = 0.881 - 0.686 = 0.191 = 0.195 Splitting of  Δ_info = 0.881 - 0.690 = 0.191 Δ_info = 0.881 - 0.686 = 0.195 45  of  46  of  [  ,  of  46  of  47  of  [ Parent ]  of  47  of  1 . Order attribute values with O(nlog(n ) ) complexity . 2 . Choose split positions at midpoints between adjacent sorted values . 3 .'), Document(metadata={}, page_content='midpoints between adjacent sorted values . 3 .  index in one pass , O(n ) . 4 . Identify optimal split using the Gini index . The complexity of finding the best binary split is O(n ) . 48  of  — ,  attribute values with O(nlog(n ) ) complexity . Choose split positions at midpoints between adjacent sorted values .  index in one pass , O(n ) . BP wn > Identify optimal split using the Gini index . The complexity of finding the best binary split is O(n ) . 48  of  1 . Order attribute values with'), Document(metadata={}, page_content='O(n ) . 48  of  1 . Order attribute values with O(nlog(n ) ) complexity . 2 . Choose split positions at midpoints between adjacent sorted values . 3 .  index in one pass , O(n ) . 4 . Identify optimal split using the Gini index . The complexity of finding the best binary split is O(n ) . 48 Synthesis of Attribute test selection Selecting the best  for  : 1 . Identify the best split ( if more than one split ) for each attribute using Δ = I(parent)−I(children ) . 2 . Select the best attribute by'), Document(metadata={}, page_content=') . 2 . Select the best attribute by comparing Δ = I(parent)−I(children ) across all the attributes . 49 Synthesis of Attribute test selection Selecting the best  for  : 1 . Identify the best split ( if more than one split ) for each attribute using A = I(parent)-I(children ) . 2 . Select the best attribute by comparing A = I(parent)-I(children ) across all the attributes . Synthesis of Attribute test selection Selecting the best  for  : 1 . Identify the best split ( if more than one split )'), Document(metadata={}, page_content='the best split ( if more than one split ) for each attribute using Δ = I(parent)−I(children ) . 2 . Select the best attribute by comparing Δ = I(parent)−I(children ) across all the attributes . 49 Limitation of the impurity measure Δ What is the Δ(ID ) ? 50 Limitation of the impurity measure A What is the A(ID ) ? 50 Limitation of the impurity measure Δ What is the Δ(ID ) ? 50'), Document(metadata={}, page_content=\"Classification ( Part 2 )  & Classification ( Part 2 ) & Classification ( Part 2 ) & Outline ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 2  of  and selection oc oc fg  ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 2 Characteristics of  -  prior assumptions on data 's probability distribution .  for categorical and continuous datasets .  can be used without binarization , normalization , or standardization .  multiclass without reducing them to binary tasks .  trees are easy\"), Document(metadata={}, page_content=\"reducing them to binary tasks .  trees are easy to understand ( particularly shorter ones ) .  result are comparable with other techniques for many simple data sets . 3 Characteristics of  -  prior assumptions on data 's probability distribution .  for categorical and continuous datasets .  can be used without binarization , normalization , or standardization .  multiclass without reducing them to binary tasks .  trees are easy to understand ( particularly shorter ones ) .  result are\"), Document(metadata={}, page_content=\"( particularly shorter ones ) .  result are comparable with other techniques for many simple data sets . Characteristics of  -  prior assumptions on data 's probability distribution .  for categorical and continuous datasets .  can be used without binarization , normalization , or standardization .  multiclass without reducing them to binary tasks .  trees are easy to understand ( particularly shorter ones ) .  result are comparable with other techniques for many simple data sets . 3\"), Document(metadata={}, page_content='other techniques for many simple data sets . 3 Characteristics of  - can encode any function of discrete - valued attributes .  - valued function can be represented as an assignment table . Decision tree can represent the assignment table efficiently . Decision tree can group a combinations of attributes as leaf nodes .  functions , like the parity function , require a full decision tree for accurate modeling . 4 Characteristics of  - can encode any function of discrete - valued attributes .  -'), Document(metadata={}, page_content='any function of discrete - valued attributes .  - valued function can be represented as an assignment table . © Decision tree can represent the assignment table efficiently . © Decision tree can group a combinations of attributes as leaf nodes . Limitations | A | | 0 | | 0 | | 0 | | 0 | | 0 | | 0 | | 0 | | 0 | 1 | 1 | ak fe © Some functions , like the parity function , require a full decision tree for accurate modeling . Characteristics of  - can encode any function of discrete - valued'), Document(metadata={}, page_content='- can encode any function of discrete - valued attributes .  - valued function can be represented as an assignment table . Decision tree can represent the assignment table efficiently . Decision tree can group a combinations of attributes as leaf nodes .  functions , like the parity function , require a full decision tree for accurate modeling . 4 Example of  function ( A∧B)∨(C∧D ) using a simpler tree with fewer leaf nodes , instead of a fully - grown tree . Only 7 leaves 5 Example of'), Document(metadata={}, page_content='fully - grown tree . Only 7 leaves 5 Example of  function ( AAB)V(C AD ) using a simpler tree with fewer leaf nodes , instead of a fully - grown tree . Only 7 leaves Example of  function ( A∧B)∨(C∧D ) using a simpler tree with fewer leaf nodes , instead of a fully - grown tree . Only 7 leaves 5 Example of  6 x y Parity 0 0 0 0 1 1 1 0 1 1 1 0 Example of  x y Parity 0 0 0 0 1 1 1 0 1 1 1 0  of  6 x y Parity 0 0 0 0 1 1 1 0 1 1 1 0 Characteristics of  -  use rectilinear splits to divide the data'), Document(metadata={}, page_content=\"of  -  use rectilinear splits to divide the data space . Simplifies complex multidimensional data into understandable segments . Effective in handling both categorical and continuous variables . 7 Characteristics of  -  1- — _ — _ . — _ _ — _ 09 } ° v 0.8 } Vv 4 0.7 } 5 | 0.6 > O5- ° | 0.4¢ v ' 0.3 tea | 0.2 + Vv 0.17 Vv fo 0 1 1 1 1 i 1 1 1 1 1 0 01 02 03 0.4 05 06 07 08 09 1  use rectilinear splits to divide the data space . Simplifies complex multidimensional data into understandable\"), Document(metadata={}, page_content='complex multidimensional data into understandable segments . Effective in handling both categorical and continuous variables . Characteristics of  -  use rectilinear splits to divide the data space . Simplifies complex multidimensional data into understandable segments . Effective in handling both categorical and continuous variables . 7 Characteristics of  -  are the disadvantages of rectilinear splits ? 8 Characteristics of  -  are the disadvantages of rectilinear splits ? Characteristics of'), Document(metadata={}, page_content='of rectilinear splits ? Characteristics of  -  are the disadvantages of rectilinear splits ? 8 Characteristics of  -  of rectilinear splits Struggle with Non - linear Boundaries : Ineffective in capturing complex , non - linear relationships in data .  : Restricts decision boundaries to orthogonal lines , limiting flexibility .  : Can lead to oversimplified models that fail to capture the true nature of the data . 9 Characteristics of  -  of rectilinear splits Struggle with Non - linear'), Document(metadata={}, page_content='of rectilinear splits Struggle with Non - linear Boundaries : Ineffective in capturing complex , non - linear relationships in data .  : © Restricts decision boundaries to orthogonal lines , limiting flexibility .  : © Can lead to oversimplified models that fail to capture the true nature of the data . Characteristics of  -  of rectilinear splits Struggle with Non - linear Boundaries : Ineffective in capturing complex , non - linear relationships in data .  : Restricts decision boundaries to'), Document(metadata={}, page_content='in data .  : Restricts decision boundaries to orthogonal lines , limiting flexibility .  : Can lead to oversimplified models that fail to capture the true nature of the data . 9 Outline ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 10  of  and selection co CO gio  ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 10  occurs when a model fits training data too closely , leading to poor generalization . A overfitted model may perform well on training data but poorly on test data .'), Document(metadata={}, page_content='well on training data but poorly on test data . Training vs  : As tree size increases , training error may decrease , but test error eventually increases . 11  occurs when a model fits training data too closely , leading to poor generalization . A overfitted model may perform well on training data but poorly on test data . Training vs  : As tree size increases , training error may decrease , but test error eventually increases . 30 40 50 60 70 Size of tree ( number of nodes )  occurs when a'), Document(metadata={}, page_content='Size of tree ( number of nodes )  occurs when a model fits training data too closely , leading to poor generalization . A overfitted model may perform well on training data but poorly on test data . Training vs  : As tree size increases , training error may decrease , but test error eventually increases . 11 Causes of  : A small training set may not represent true patterns , leading to overfitting .  : Overly complex models can capture training - specific patterns , reducing generalizability .'), Document(metadata={}, page_content=\"specific patterns , reducing generalizability .  : Models may learn irrelevant patterns present in training data ( Ex . noise ) , which do n't generalize to new data . 12 Causes of  : Asmall training set may not represent true patterns , leading to overfitting .  : Overly complex models can capture training - specific patterns , reducing . aye Under - fitting Appropriate - fitting Over - fitting generalizabil ity . ( too simple to ( forcefitting -- too explain the variance ) good to be true )\"), Document(metadata={}, page_content=\"-- too explain the variance ) good to be true ) DE  : Models may learn irrelevant patterns present in training data ( Ex . noise ) , which do n't generalize to new data . Causes of  : A small training set may not represent true patterns , leading to overfitting .  : Overly complex models can capture training - specific patterns , reducing generalizability .  : Models may learn irrelevant patterns present in training data ( Ex . noise ) , which do n't generalize to new data . 12 Overfitting vs\"), Document(metadata={}, page_content=\"n't generalize to new data . 12 Overfitting vs  : Simple models may fail to capture essential patterns . Data scientist challenge Find a model that does not overfit or underfit 13 Overfitting vs  : Simple models may fail to capture essential patterns . Data scientist challenge Find a model that does not overfit or underfit Under - fitting Appropriate - fitting Over - fitting ( too simple to ( forcefitting -- too explain the variance ) good to be true ) DE Overfitting vs  : Simple models may\"), Document(metadata={}, page_content='be true ) DE Overfitting vs  : Simple models may fail to capture essential patterns . Data scientist challenge Find a model that does not overfit or underfit 13 Dealing with overfitting -  : cutting away branches that may be based on noisy or misleading data to prevent overfitting . Pre - pruning : Occurs during tree construction . Limits tree growth by limiting the maximum depth or minimum leaf size . Prevents overﬁtting by avoiding overly complex models . Post pruning : Applied after the tree'), Document(metadata={}, page_content='models . Post pruning : Applied after the tree is fully grown . Removes branches that contribute little to classiﬁcation accuracy . Reduces model complexity , enhancing generalization to new data . 14 Dealing with overfitting -  : cutting away branches that may be based on noisy or misleading data to prevent overfitting . Pre - pruning : Occurs during tree construction . Limits tree growth by limiting the maximum depth or minimum leaf size . Prevents overfitting by avoiding overly complex'), Document(metadata={}, page_content='. Prevents overfitting by avoiding overly complex models . Post pruning : Applied after the tree is fully grown . Removes branches that contribute little to classification accuracy . Reduces model complexity , enhancing generalization to new data . Pruning ( ec = > @ & Dealing with overfitting -  : cutting away branches that may be based on noisy or misleading data to prevent overfitting . Pre - pruning : Occurs during tree construction . Limits tree growth by limiting the maximum depth or'), Document(metadata={}, page_content='tree growth by limiting the maximum depth or minimum leaf size . Prevents overﬁtting by avoiding overly complex models . Post pruning : Applied after the tree is fully grown . Removes branches that contribute little to classiﬁcation accuracy . Reduces model complexity , enhancing generalization to new data . 14 Example about pruning in decision tree 15 Example about pruning in decision tree - ‘ ) ~=min . Num . Objects : 30 4l Example about pruning in decision tree 15 Example about pruning in'), Document(metadata={}, page_content='in decision tree 15 Example about pruning in decision tree 16 Max depth = 3 Example about pruning in decision tree Multi . Agent = O : ; Max depth = 3 lidepth > 2 : class 0 | ; depth < = 2 : 1,1 Multi . P = 1 : class 0 | Multil . P = 0 : | breadth < = 6 : class 0 | breadth > 6 : | | Repeated . Access < = 0.322 : class 0 | | Repeated . Access > 0.322 : class 1 Multi(Agent=1 : = © | total . Pages < = 81 : class 0 | total . Pages > 81 : class 1 Multi . Agent = 1 : | total . Pages < = 81 : class 0'), Document(metadata={}, page_content='. Agent = 1 : | total . Pages < = 81 : class 0 | total . Pages > 81 : class 1 ‘ ; Multi . Agent = 0 : class 0 Example about pruning in decision tree 16 Max depth = 3 Outline ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 17  of  and selection CO gm ico  ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 17  : Estimate model performance on data not used during training . Ensure robust model evaluation .  a separate test set , not involved in model building , for unbiased evaluation .'), Document(metadata={}, page_content='in model building , for unbiased evaluation .  split data into training and test sets . Use the test set to estimate generalization error . Cross -  data into multiple subsets ; train and test the model on different subsets for a comprehensive performance estimate . 18  : [ ) [ ) Estimate model performance on data not used during training . Ensure robust model evaluation .  [ ) Utilize a separate test set , not involved in model building , for unbiased evaluation .  [ ) [ ) Randomly split data'), Document(metadata={}, page_content='evaluation .  [ ) [ ) Randomly split data into training and test sets . Use the test set to estimate generalization error . Cross -  [ ) Divide data into multiple subsets ; train and test the model on different subsets for a comprehensive performance estimate . a FR SS yy BAD ?  : Estimate model performance on data not used during training . Ensure robust model evaluation .  a separate test set , not involved in model building , for unbiased evaluation .  split data into training and test sets'), Document(metadata={}, page_content='.  split data into training and test sets . Use the test set to estimate generalization error . Cross -  data into multiple subsets ; train and test the model on different subsets for a comprehensive performance estimate . 18  technique to partition data into training ( D.train ) and testing ( D.test ) sets .  error rate on D.test ( errtest ) as a measure of generalization error .  decide the split ratio . Commonly two - thirds training and one - third testing . Trade - offs Balancing D.train'), Document(metadata={}, page_content='- third testing . Trade - offs Balancing D.train size for model learning and D.test size for reliable error estimation .  reliability by repeating the process and averaging error rates . 19  technique to partition data into training ( D.train ) and testing ( D.test ) sets .  DATASET © Calculate error rate on D.test ( errtest ) as a measure of generalization error .  | © Commonly two - thirds training and one - third testing . | i Trade - offs  © Balancing D.train size for model learning and'), Document(metadata={}, page_content='© Balancing D.train size for model learning and D.test size for reliable error estimation .  reliability by repeating the process and averaging error rates . 19  technique to partition data into training ( D.train ) and testing ( D.test ) sets .  error rate on D.test ( errtest ) as a measure of generalization error .  decide the split ratio . Commonly two - thirds training and one - third testing . Trade - offs Balancing D.train size for model learning and D.test size for reliable error'), Document(metadata={}, page_content='model learning and D.test size for reliable error estimation .  reliability by repeating the process and averaging error rates . 19 Model selection and validation set Achieve an optimal balance between model complexity and performance .  can be measured by the ratio of leaf nodes to training instances . Limitation of  : Training error rate is insufficient for effective model selection .  : Essential for assessing generalization error .  : Combine complexity with validation set performance to'), Document(metadata={}, page_content='complexity with validation set performance to select the most effective model . 20 Model selection and validation set Achieve an optimal balance between model complexity and performance .  can be measured by the ratio of leaf A retest nodes to training instances . Limitation of  :  error rate is insufficient for effective  : . . . .  for assessing generalization error .  : Combine complexity with validation set performance to select the most effective model . 20 Model selection and validation'), Document(metadata={}, page_content='model . 20 Model selection and validation set Achieve an optimal balance between model complexity and performance .  can be measured by the ratio of leaf nodes to training instances . Limitation of  : Training error rate is insufficient for effective model selection .  : Essential for assessing generalization error .  : Combine complexity with validation set performance to select the most effective model . 20 Cross validation Cross validation helps to avoid the split baise . Divide data into k'), Document(metadata={}, page_content=\"to avoid the split baise . Divide data into k equal folds . Each instance is used exactly once for error . calculation . The error is calculated based on : What if some classes do n't appear in some folds ? 21 Cross validation Cross validation helps to avoid the split baise . Divide data into k equal folds . Each instance is used exactly once for error . i = : calculation . HB  error is calculated based on : Run 2 | [ J  D1 eum ) m3 |l . Ur . LE Cet = — = — — _ = © N What if some classes do n't\"), Document(metadata={}, page_content=\"Cet = — = — — _ = © N What if some classes do n't appear in some folds ? 21 Cross validation Cross validation helps to avoid the split baise . Divide data into k equal folds . Each instance is used exactly once for error . calculation . The error is calculated based on : What if some classes do n't appear in some folds ? 21 Cross validation Cross validation helps to avoid the split baise . Divide data into k equal folds . Each instance is used exactly once for error calculation . The error is\"), Document(metadata={}, page_content=\"exactly once for error calculation . The error is calculated based on : What if some classes do n't appear in some folds ? Number of errors in one fold . 22 Cross validation Cross validation helps to avoid the split baise . Divide data into k equal folds . Each instance is used exactly once for error calculation . The error is calculated based on : Run 2 ys ( ay Number of errors pun3 erriest = in one fold . N What if some classes do n't appear in some folds ? S ; S Ss = HB  [ _ ]  22 Cross\"), Document(metadata={}, page_content=\"in some folds ? S ; S Ss = HB  [ _ ]  22 Cross validation Cross validation helps to avoid the split baise . Divide data into k equal folds . Each instance is used exactly once for error calculation . The error is calculated based on : What if some classes do n't appear in some folds ? Number of errors in one fold . 22 Cross validation  equal representation of classes in each partition . Leave - One -  A special case where each instance is used once as a test set . K = N  cross - validation with\"), Document(metadata={}, page_content='as a test set . K = N  cross - validation with different partitions provides robust error estimates . 23 Cross validation  [ ) Ensures equal representation of classes in each partition . Leave - One -  [ ) [ ) A special case where each instance is used once as a test set . K = N  [ ) Repeating cross - validation with different partitions provides robust error estimates . Stratified K - fold  ( K = 5 )  1 Round 2 Round 3 Round 4 Round 5 Keep the distribution of classes in each fold [ [ )  [  23'), Document(metadata={}, page_content='distribution of classes in each fold [ [ )  [  23 Cross validation  equal representation of classes in each partition . Leave - One -  A special case where each instance is used once as a test set . K = N  cross - validation with different partitions provides robust error estimates . 23 Classification evaluation metrics  of correctly predicted instances to total instances .  of true positives to total predicted positives . Recall ( Sensitivity ) Ratio of true positives to actual positives . F1'), Document(metadata={}, page_content='Ratio of true positives to actual positives . F1  mean of precision and recall .  tool categorizing true and false positives and negatives . 24 Classification evaluation metrics  of correctly predicted instances to total instances . Predicted 0  of true positives to total predicted positives . Recall ( Sensitivity ) Ratio of true positives to actual positives . TN F1Score Harmonic mean of precision and recall .  tool categorizing true and false positives and negatives . 24 Classification'), Document(metadata={}, page_content='false positives and negatives . 24 Classification evaluation metrics  of correctly predicted instances to total instances .  of true positives to total predicted positives . Recall ( Sensitivity ) Ratio of true positives to actual positives . F1  mean of precision and recall .  tool categorizing true and false positives and negatives . 24 Classification evaluation metrics 25 Classification evaluation metrics  . ..  ( FN )  ( TP ) r nk TP iia ( TP + FN )  :  ( FP ) ;  1 , IE  ( TN ) TN ype I'), Document(metadata={}, page_content='( TP + FN )  :  ( FP ) ;  1 , IE  ( TN ) TN ype I Error aa  TP+TN TP ( TP + FP ) ( TN + FN ) TN ( TP + TN + FP + FN ) 25 Classification evaluation metrics 25 Outline ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 26  of  and selection m@eceio Conclusion 26 Outline ❏ Characteristics of  ❏  ❏  and selection ❏ Conclusion 26  , the journey in data science is a continuous battle against overfitting and underfitting . Stay vigilant ! 27  , the journey in data science is a continuous battle'), Document(metadata={}, page_content='journey in data science is a continuous battle against overfitting and underfitting . Stay vigilant ! 27  , the journey in data science is a continuous battle against overfitting and underfitting . Stay vigilant ! 27'), Document(metadata={}, page_content='1 & 1  1 &  1 & 1 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  2 Outline LJ Overview of Clustering LJ  LJ K - means Clustering 1  LJ DBSCAN Clustering LJ }  ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  2 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  3 Outline L } Overview of Clustering LJ  LJ K - means Clustering 1  LJ DBSCAN Clustering LJ }  ❏ Overview of Clustering ❏  ❏ K - means'), Document(metadata={}, page_content='LJ }  ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  3 What is Clustering ? Given a set of objects , place them in groups such that : The objects in a group are similar ( or related ) to one another and different from ( or unrelated to ) the objects in other groups 4 What is ? m= Given a set of objects , place them in groups such that : The objects in a group are similar ( or related ) to one another and different from ( or unrelated to ) the objects in other'), Document(metadata={}, page_content='from ( or unrelated to ) the objects in other groups What is ? Given a set of objects , place them in groups such that : The objects in a group are similar ( or related ) to one another and different from ( or unrelated to ) the objects in other groups 4 Think of some practical applications of clustering ? 5 Think of some practical applications of clustering ? Think of some practical applications of clustering ? 5 Applications of  : Discover customer segments for targeted marketing Information'), Document(metadata={}, page_content='segments for targeted marketing Information retrieval : Document clustering Land use : Identifying similar land use areas in an Earth database Biology : Taxonomy levels ( kingdom to species ) City planning : Grouping houses by type , value , and location Earthquake studies : Clustering observed epicenters along fault lines Climate : Analyzing atmospheric and ocean patterns Economic science : Market research 6 Applications of  : Discover customer segments for targeted marketing Information'), Document(metadata={}, page_content='segments for targeted marketing Information retrieval : Document clustering Land use : Identifying similar land use areas in an Earth database Biology : Taxonomy levels ( kingdom to species ) City planning : Grouping houses by type , value , and location Earthquake studies : Clustering observed epicenters along fault lines Climate : Analyzing atmospheric and ocean patterns Economic science : Market research Applications of  : Discover customer segments for targeted marketing Information'), Document(metadata={}, page_content='segments for targeted marketing Information retrieval : Document clustering Land use : Identifying similar land use areas in an Earth database Biology : Taxonomy levels ( kingdom to species ) City planning : Grouping houses by type , value , and location Earthquake studies : Clustering observed epicenters along fault lines Climate : Analyzing atmospheric and ocean patterns Economic science : Market research 6 Clustering as Preprocessing tool Summarization : Preprocessing for classification ,'), Document(metadata={}, page_content=': Preprocessing for classification , regression , PCA , and association analysis Compression : Image processing using vector quantization Finding K - nearest  search to one or a small number of clusters Outlier detection Outliers are often viewed as those “ far away ” from any cluster 7 Clustering as Preprocessing tool Summarization : » Preprocessing for classification , regression , PCA , and association analysis Compression : » Image processing using vector quantization Finding K - nearest'), Document(metadata={}, page_content='using vector quantization Finding K - nearest Neighbors » Localizing search to one or a small number of clusters Outlier detection » Outliers are often viewed as those “ far away ” from any cluster Clustering as Preprocessing tool Summarization : Preprocessing for classification , regression , PCA , and association analysis Compression : Image processing using vector quantization Finding K - nearest  search to one or a small number of clusters Outlier detection Outliers are often viewed as'), Document(metadata={}, page_content='Outlier detection Outliers are often viewed as those “ far away ” from any cluster 7 What is a good clustering and what are the factors that contribute to it ? 8 What is a good clustering and what are the factors that contribute to it ? What is a good clustering and what are the factors that contribute to it ? 8 What is a  ? ( 1 ) A good clustering method will produce high - quality clusters high intra - class similarity : cohesive within clusters low inter - class similarity : distinctive'), Document(metadata={}, page_content='low inter - class similarity : distinctive between clusters The quality of a clustering method depends on : the similarity measure used by the method the implementation of the clustering method the ability to discover some or all of the hidden patterns 9 What is a  ? ( 1 ) : clustering method will produce high - quality clusters » high intra - class similarity : cohesive within clusters . low inter - class similarity : distinctive between clusters . The quality of a clustering method depends on'), Document(metadata={}, page_content='. The quality of a clustering method depends on : . the similarity measure used by the method . the implementation of the clustering method . the ability to discover some or all of the hidden patterns What is a  ? ( 1 ) A good clustering method will produce high - quality clusters high intra - class similarity : cohesive within clusters low inter - class similarity : distinctive between clusters The quality of a clustering method depends on : the similarity measure used by the method the'), Document(metadata={}, page_content=': the similarity measure used by the method the implementation of the clustering method the ability to discover some or all of the hidden patterns 9 What is a  ? ( 2 ) Dissimilarity / Similarity metric Similarity is expressed in terms of a distance function d(i , j ) The definitions of distance functions depend on the attribute type : boolean , categorical , interval - scaled , ordinal ratio , and vector variables Weights should be associated with different attributes based on the domain'), Document(metadata={}, page_content='with different attributes based on the domain application and data semantics Quality of clustering There is a “ quality ” function that measures the “ goodness ” of a cluster It is hard to define “ similar enough ” or “ good enough ” due to subjectivity 10 What is a  ? ( 2 ) : Dissimilarity / Similarity metric Similarity is expressed in terms of a distance function d(i , / ) The definitions of distance functions depend on the attribute type : boolean , categorical , interval - scaled , ordinal'), Document(metadata={}, page_content=', categorical , interval - scaled , ordinal ratio , and vector variables Weights should be associated with different attributes based on the domain application and data semantics = Quality of clustering There is a “ quality ” function that measures the “ goodness ” of a cluster It is hard to define “ similar enough ” or “ good enough ” due to subjectivity 10 What is a  ? ( 2 ) Dissimilarity / Similarity metric Similarity is expressed in terms of a distance function d(i , j ) The definitions of'), Document(metadata={}, page_content='a distance function d(i , j ) The definitions of distance functions depend on the attribute type : boolean , categorical , interval - scaled , ordinal ratio , and vector variables Weights should be associated with different attributes based on the domain application and data semantics Quality of clustering There is a “ quality ” function that measures the “ goodness ” of a cluster It is hard to define “ similar enough ” or “ good enough ” due to subjectivity 10 Considerations for  criteria'), Document(metadata={}, page_content='to subjectivity 10 Considerations for  criteria Single - level Hierarchical partitioning ( often , multi - level partitioning is desirable ) Separation of clusters Exclusive ( .g . , one customer belongs to only one region ) Non - exclusive ( .g . , one document may belong to more than one class ) Similarity measure Distance - based ( .g . , Euclidean , road network , vector ) Connectivity - based ( .g . , density or contiguity ) Clustering space Full space ( often when low dimensional )'), Document(metadata={}, page_content='space Full space ( often when low dimensional ) Subspaces ( often in high - dimensional clustering ) 11 11 Considerations for  criteria « Single - level » Hierarchical partitioning ( often , multi - level partitioning is desirable ) Separation of clusters » Exclusive ( .g . , one customer belongs to only one region ) » Non - exclusive ( .g . , one document may belong to more than one class ) Similarity measure » Distance - based ( .g . , Euclidean , road network , vector ) » Connectivity -'), Document(metadata={}, page_content=', road network , vector ) » Connectivity - based ( .g . , density or contiguity ) Clustering space » Full space ( often when low dimensional ) » Subspaces ( often in high - dimensional clustering ) 11 Considerations for  criteria Single - level Hierarchical partitioning ( often , multi - level partitioning is desirable ) Separation of clusters Exclusive ( .g . , one customer belongs to only one region ) Non - exclusive ( .g . , one document may belong to more than one class ) Similarity measure'), Document(metadata={}, page_content='to more than one class ) Similarity measure Distance - based ( .g . , Euclidean , road network , vector ) Connectivity - based ( .g . , density or contiguity ) Clustering space Full space ( often when low dimensional ) Subspaces ( often in high - dimensional clustering ) 11 11 Notion of a Cluster can be Ambiguous 12 Notion of a Cluster can be  many clusters ? Notion of a Cluster can be Ambiguous 12 Notion of a Cluster can be Ambiguous 13 Notion of a Cluster can be Ambiguous “ 8 + *  acre o. O %'), Document(metadata={}, page_content='a Cluster can be Ambiguous “ 8 + *  acre o. O % ® fe ) ° . ee @ . vv i 9 ° ee Vv oo How many clusters ?  r. E A aa * ] A +  A Vv + a AA A Vv ° AA Vv oo  13 Notion of a Cluster can be Ambiguous 13 Requirements and  and use the different clusters  all the data instead of samples Deal with different types of attributes Numerical , binary , categorical , ordinal , linked , and a mixture of these Constraint - based clustering User may give inputs on constraints Use domain knowledge to determine'), Document(metadata={}, page_content='on constraints Use domain knowledge to determine input parameters  to deal with noisy data and outliers Ability to detect clusters of any shape 14 14 Requirements and Challenges . Interpretability » Explain and use the different clusters = Scalability » Clustering all the data instead of samples : Deal with different types of attributes » Numerical , binary , categorical , ordinal , linked , and a mixture of these : Constraint - based clustering » User may give inputs on constraints » Use'), Document(metadata={}, page_content='» User may give inputs on constraints » Use domain knowledge to determine input parameters . Others » Ability to deal with noisy data and outliers : Ability to detect clusters of any shape 14 Requirements and  and use the different clusters  all the data instead of samples Deal with different types of attributes Numerical , binary , categorical , ordinal , linked , and a mixture of these Constraint - based clustering User may give inputs on constraints Use domain knowledge to determine input'), Document(metadata={}, page_content='Use domain knowledge to determine input parameters  to deal with noisy data and outliers Ability to detect clusters of any shape 14 14 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  15 Outline L } Overview of Clustering LJ  LJ K - means Clustering 1  LJ DBSCAN Clustering LJ }  ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  15  16 16  16  16 16 Partitional vs  17 Nested clusters Non - nested clusters Partitional vs  5 © Nested'), Document(metadata={}, page_content='Non - nested clusters Partitional vs  5 © Nested clusters Non - nested clusters Partitional vs  17 Nested clusters Non - nested clusters  approach : Construct various partitions and then evaluate them by some criterion Typical methods : K - means , K - medoids , CLARANS Hierarchical approach : Create a hierarchical decomposition of the set of data using some criterion Typical methods : , , BIRCH , CAMELEON Density - based approach : Based on connectivity and density functions ( detect regions'), Document(metadata={}, page_content='and density functions ( detect regions where points are concentrated ) Typical methods : DBSCAN , OPTICS , Den .  - based : A model is hypothesized for each of the clusters and tries to find the best fit Typical methods : EM , SOM , COBWEB 18 18  approach : : Construct various partitions and then evaluate them by some criterion » Typical methods : K - means , K - medoids , CLARANS Hierarchical approach : : Create a hierarchical decomposition of the set of data using some criterion » Typical'), Document(metadata={}, page_content='of the set of data using some criterion » Typical methods : , , BIRCH , CAMELEON Density - based approach : : Based on connectivity and density functions ( detect regions where points are concentrated ) » Typical methods : DBSCAN , OPTICS , Den .  - based : » Amodel is hypothesized for each of the clusters and tries to find the best fit » Jypical methods : EM , SOM , COBWEB 18  approach : Construct various partitions and then evaluate them by some criterion Typical methods : K - means , K -'), Document(metadata={}, page_content='some criterion Typical methods : K - means , K - medoids , CLARANS Hierarchical approach : Create a hierarchical decomposition of the set of data using some criterion Typical methods : , , BIRCH , CAMELEON Density - based approach : Based on connectivity and density functions ( detect regions where points are concentrated ) Typical methods : DBSCAN , OPTICS , Den .  - based : A model is hypothesized for each of the clusters and tries to find the best fit Typical methods : EM , SOM , COBWEB 18'), Document(metadata={}, page_content='best fit Typical methods : EM , SOM , COBWEB 18 18 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  19 Outline LJ Overview of Clustering LJ  [ ) K - means Clustering 1  LJ DBSCAN Clustering LJ }  ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  19  : Partitioning a database D of n objects into a set of K clusters . K - Means algorithm is an example of a partitional clustering algorithm . Example of clustering data points with K=3 :'), Document(metadata={}, page_content='. Example of clustering data points with K=3 : 20 20  : Partitioning a database D of n objects into a set of K clusters . K - Means algorithm is an example of a partitional clustering algorithm . Example of clustering data points with K=3 : sp After clusterin oot 8 20  : Partitioning a database D of n objects into a set of K clusters . K - Means algorithm is an example of a partitional clustering algorithm . Example of clustering data points with K=3 : 20 20 Which objective function should be'), Document(metadata={}, page_content='K=3 : 20 20 Which objective function should be used ? 21 Which objective function should be used ? Which objective function should be used ? 21  A common objective function is minimize the Sum of  ( SSE ) SSE is used with the Euclidean distance measure x is a data point in cluster Ci and mi is the centroid or medoid for cluster  each point x , the error is the distance to the nearest cluster center mi To get SSE , we square these errors and sum them . SSE improves in each iteration until it'), Document(metadata={}, page_content='them . SSE improves in each iteration until it reaches a local or global minima . 22  objective function is minimize the Sum of  ( SSE ) SSE is used with the Euclidean distance measure K SSE = , S dist ’ ( m , , x ) i = l xe . C ; » x. X isa data point in cluster C , and m , is the centroid or medoid for cluster C , « For each point x , the error is the distance to the nearest cluster center m , » To get SSE , we square these errors and sum them . » SSE improves in each iteration until it'), Document(metadata={}, page_content='them . » SSE improves in each iteration until it reaches a local or global minima . 22  A common objective function is minimize the Sum of  ( SSE ) SSE is used with the Euclidean distance measure x is a data point in cluster Ci and mi is the centroid or medoid for cluster  each point x , the error is the distance to the nearest cluster center mi To get SSE , we square these errors and sum them . SSE improves in each iteration until it reaches a local or global minima . 22 K -  number of'), Document(metadata={}, page_content='a local or global minima . 22 K -  number of clusters K must be specified as input Each cluster is represented with a centroid ( i .. center point , .g . the mean ) In the first iteration , the K centroids are K random points ( objects ) from the data Each point in the data is assigned to the cluster with the closest centroid The centroid of each cluster is updated at each iteration The algorithm keeps iterating until the centroid do n’t change 23 23 K -  number of clusters K must be specified'), Document(metadata={}, page_content='23 23 K -  number of clusters K must be specified as input Each cluster is represented with a centroid ( i .. center point , .g . the mean ) In the first iteration , the K centroids are K random points ( objects ) from the data Each point in the data is assigned to the cluster with the closest centroid The centroid of each cluster is updated at each iteration The algorithm keeps iterating until the centroid do n’t change 1 : Select K points as the initial centroids . 2 : repeat 3 : Form K'), Document(metadata={}, page_content='as the initial centroids . 2 : repeat 3 : Form K clusters by assigning all points to the closest centroid . 4 : Recompute the centroid of each cluster . 5 : until The centroids do n’t change 23 K -  number of clusters K must be specified as input Each cluster is represented with a centroid ( i .. center point , .g . the mean ) In the first iteration , the K centroids are K random points ( objects ) from the data Each point in the data is assigned to the cluster with the closest centroid The'), Document(metadata={}, page_content='to the cluster with the closest centroid The centroid of each cluster is updated at each iteration The algorithm keeps iterating until the centroid do n’t change 23 23 Example of K - Means ( K=2 ) 24 ) = 2 Means ( K Example of K 24 Example of K - Means ( K=2 ) 24 Example of K - Means ( K=3 ) 25 Example of K - Means ( K=3 ) ° ~*~ | a &  * . . ej y 7 g , or xeon * B ° * Ersee | 25 Iteration 2 ° i ? oss ¥ “ f. F K = ea LI wm , 6 ie ° ot % | = , * * 5 ° PAG ! ie Bote Bo3- a ° “ grees B66 0 » + 4 .'), Document(metadata={}, page_content='5 ° PAG ! ie Bote Bo3- a ° “ grees B66 0 » + 4 . ee | . # & * = “ 4 a = = a. S a =] 3 15 05 05 -15 -2 1.5 05 05 “ 15 15 05 05 A 15 x Iteration 6 * ° . 4 ov + % + to6 + x Iteration 4 * oe ° KS ° 1.5 05 05 -15 -2 1.5 05 05 -15 1.5 05 05 -15 a. J Example of K - Means ( K=3 ) 25 Strength and Weakness of K -  : Fast : O(tkn ) n : number of objects , k : number of clusters , t : number of iterations Normally : k , t < < n  to specify k , the number of clusters , in advance The random choice of the'), Document(metadata={}, page_content='of clusters , in advance The random choice of the first k centroids may result in different clustering Applicable only to objects in a continuous n - dimensional space Often terminates at a local optimal Sensitive to noisy data and outliers Not suitable to discover clusters with non - convex shapes 26 26 Strength and Weakness of K -  : Fast : O(tkn ) » nm : number of objects , k : number of clusters , t ’ number of iterations = Normally : k , t<<n Weakness » Need to specify k , the number of'), Document(metadata={}, page_content='t<<n Weakness » Need to specify k , the number of clusters , in advance » The random choice of the first k centroids may result in different clustering « Applicable only to objects in a continuous n - dimensional space « Often terminates at a local optimal » Sensitive to noisy data and outliers « Not suitable to discover clusters with non - convex shapes 26 Strength and Weakness of K -  : Fast : O(tkn ) n : number of objects , k : number of clusters , t : number of iterations Normally : k , t <'), Document(metadata={}, page_content=', t : number of iterations Normally : k , t < < n  to specify k , the number of clusters , in advance The random choice of the first k centroids may result in different clustering Applicable only to objects in a continuous n - dimensional space Often terminates at a local optimal Sensitive to noisy data and outliers Not suitable to discover clusters with non - convex shapes 26 26 How to improve the K - Means algorithm ? 27 How to improve the K - Means algorithm ? How to improve the K - Means'), Document(metadata={}, page_content='- Means algorithm ? How to improve the K - Means algorithm ? 27 Variations of the K -  of the variants of the k - means differ in : Selection of the initial k means Dissimilarity calculations Strategies to calculate cluster means Handling categorical data with k - modes : Replacing means of clusters with modes Using new dissimilarity measures to deal with categorical objects Using a frequency - based method to update modes of clusters A mixture of categorical and numerical data : k - prototype'), Document(metadata={}, page_content='of categorical and numerical data : k - prototype method 28 28 Variations of the K -  of the variants of the k - means differ in : = Selection of the initial k. K means « Dissimilarity calculations . Strategies to calculate cluster means Handling categorical data with k - modes : « = Replacing means of clusters with modes « Using new dissimilarity measures to deal with categorical objects . Using a frequency - based method to update modes of clusters = Amixture of categorical and numerical data'), Document(metadata={}, page_content='= Amixture of categorical and numerical data : k - prototype method 28 Variations of the K -  of the variants of the k - means differ in : Selection of the initial k means Dissimilarity calculations Strategies to calculate cluster means Handling categorical data with k - modes : Replacing means of clusters with modes Using new dissimilarity measures to deal with categorical objects Using a frequency - based method to update modes of clusters A mixture of categorical and numerical data : k -'), Document(metadata={}, page_content='A mixture of categorical and numerical data : k - prototype method 28 28 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  29 Outline LJ Overview of Clustering LJ  [ ) K - means Clustering 1  LJ DBSCAN Clustering LJ }  29 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  29'), Document(metadata={}, page_content='2 & 1  2 &  2 & 1 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  2 Outline LJ Overview of Clustering LJ  LJ K - means Clustering { J  LJ DBSCAN Clustering { y  ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  2  produce a set of nested - clusters . It does not have to assume any particular number of clusters . It may correspond to meaningful taxonomies ( .g . , biological taxonomy , animal kingdom , phylogeny reconstruction , … )'), Document(metadata={}, page_content=', animal kingdom , phylogeny reconstruction , … ) .  3 Nested clusters  produce a set of nested - clusters . It does not have to assume any particular number of clusters . It may correspond to meaningful taxonomies ( .g . , biological taxonomy , animal kingdom , phylogeny reconstruction , ... ) . Nested clusters  produce a set of nested - clusters . It does not have to assume any particular number of clusters . It may correspond to meaningful taxonomies ( .g . , biological taxonomy , animal'), Document(metadata={}, page_content='taxonomies ( .g . , biological taxonomy , animal kingdom , phylogeny reconstruction , … ) .  3 Nested clusters The set of nested clusters can be organized as a hierarchical tree . The hierarchical tree of clusters is called a dendrogram , which records the sequences of merges or splits Different clustering of the data can be obtained by cutting the dendrogram at the desired level , then each connected component forms a cluster  5 nested clusters of 6 data points 4 distance  = m The set of'), Document(metadata={}, page_content='of 6 data points 4 distance  = m The set of nested clusters can be organized as a hierarchical tree . = The hierarchical tree of clusters is called a dendrogram , which records the sequences of merges or splits = Different clustering of the data can be obtained by cutting the dendrogram at the desired level , then each connected component forms a cluster 0.2 ° = a distance ° 0.05 — — _ { 1 3 2 5 4 6 Dendrogram 5 nested clusters of 6 data points The set of nested clusters can be organized as a'), Document(metadata={}, page_content='The set of nested clusters can be organized as a hierarchical tree . The hierarchical tree of clusters is called a dendrogram , which records the sequences of merges or splits Different clustering of the data can be obtained by cutting the dendrogram at the desired level , then each connected component forms a cluster  5 nested clusters of 6 data points 4 distance Types of  : Start with the points as individual clusters At each step , merge the closest pair of clusters until only one cluster ('), Document(metadata={}, page_content='closest pair of clusters until only one cluster ( or k clusters ) left Popular algorithm : AGNES (  ) Divisive : Start with one , all - inclusive cluster At each step , split the least cohesive clusters until each cluster contains an individual point ( or there are k clusters ) Popular algorithm : DIANA (  ) Hierarchical algorithms use a proximity matrix ( similarity or distance ) Merge or split one cluster at a time 5 Types of  : Agglomerative : . Start with the points as individual clusters .'), Document(metadata={}, page_content='. Start with the points as individual clusters . At each step , merge the closest pair of clusters until only one cluster ( or k clusters ) left . Popular algorithm : AGNES (  ) : Divisive : . Start with one , all - inclusive cluster . At each step , split the least cohesive clusters until each cluster contains an individual point ( or there are k clusters ) . Popular algorithm : DIANA (  ) » Hierarchical algorithms use a proximity matrix ( similarity or distance ) = Merge or split one cluster'), Document(metadata={}, page_content='or distance ) = Merge or split one cluster at a time Types of  : Start with the points as individual clusters At each step , merge the closest pair of clusters until only one cluster ( or k clusters ) left Popular algorithm : AGNES (  ) Divisive : Start with one , all - inclusive cluster At each step , split the least cohesive clusters until each cluster contains an individual point ( or there are k clusters ) Popular algorithm : DIANA (  ) Hierarchical algorithms use a proximity matrix ('), Document(metadata={}, page_content='Hierarchical algorithms use a proximity matrix ( similarity or distance ) Merge or split one cluster at a time 5 Agglomerative vs Divisive 6 Agglomerative vs  eo _ , 2 , P= g Agglomerative vs Divisive 6  : Successively merge the closest clusters Basic algorithm : 6 . Until only a single cluster remains ( or k clusters left ) Key operation is the computation of the proximity of two clusters : Different approaches to defining the distance between clusters distinguish the different algorithms ('), Document(metadata={}, page_content='clusters distinguish the different algorithms ( Min , Max , etc . ) 7  .  : Successively merge the closest clusters . Basic algorithm : Compute the proximity matrix Let each data point be a cluster  the two closest clusters Update the proximity matrix Until only a single cluster remains ( or k clusters left ) Our WN > . Key operation is the computation of the proximity of two clusters : . Different approaches to defining the distance between clusters distinguish the different algorithms ( Min ,'), Document(metadata={}, page_content='distinguish the different algorithms ( Min , Max , etc . )  : Successively merge the closest clusters Basic algorithm : 6 . Until only a single cluster remains ( or k clusters left ) Key operation is the computation of the proximity of two clusters : Different approaches to defining the distance between clusters distinguish the different algorithms ( Min , Max , etc . ) 7 How to measure the distance between two clusters ? 8 How to measure the distance between two clusters ? How to measure the'), Document(metadata={}, page_content='between two clusters ? How to measure the distance between two clusters ? 8 How to  -  ? p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  9 How to  -  p4 | p5 p1 Distance ? + > p2 p3 p4 MIN Pe MAX ‘  to  -  ? p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  9 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  10 How to  -  p4 | p5 p1 p2 p3 p4 MIN P ° MAX ‘  10 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  10 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  11 How to  -  p4 | p5 p1 p2 p3 p4'), Document(metadata={}, page_content='p5 . . . . . .  11 How to  -  p4 | p5 p1 p2 p3 p4 MIN i MAX ‘  11 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  11 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  12 How to  -  p4 | p5 p1 p2 p3 p4 MIN Pe MAX ‘  ¥ proximity(p,,P ;) pje . Cluster ; pj < Cluster , roximity(Cluster , , Cluster , ) = P ty ( i \" ) | Cluster , | x | Cluster ; |  12 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  12 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  13 How to  -  p4 | p5 p1 p2 p3'), Document(metadata={}, page_content='p4 p5 . . . . . .  13 How to  -  p4 | p5 p1 p2 p3 p4 MIN P ° MAX ‘  13 How to  -  p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  13 Inter -  1 . Min ( Single link ): smallest distance between an element in one cluster and an element in the other , dist(Ki , Kj ) = min(tip , tjq ) 2 . Max ( Complete link ): largest distance between an element in one cluster and an element in the other , dist(Ki , Kj ) = max(tip , tjq ) 3 .  : avg distance between an element in one cluster and an element in the other'), Document(metadata={}, page_content='in one cluster and an element in the other , dist(Ki , Kj ) = avg(tip , tjq ) 4 . Centroid : distance between the centroids of two clusters , dist(Ki , Kj ) = dist(Ci , Cj ) X X 14 14 1 . Inter -  xy Min ( Single link ): smallest distance between an element in one cluster and an element in the other , dist(K , , K ) = min(t , , . , t. ) Max ( Complete link ): largest distance between an element in one cluster and an element in the other , dist(K , , K ) = max(t , , , t. )  : avg distance'), Document(metadata={}, page_content='dist(K , , K ) = max(t , , , t. )  : avg distance between an element in one cluster and an element in the other , dist(K , , K ) = avg(t , , . t ) Centroid : distance between the centroids of two clusters , dist(K , , K ) = dist(C , , C ) ) 14 Inter -  1 . Min ( Single link ): smallest distance between an element in one cluster and an element in the other , dist(Ki , Kj ) = min(tip , tjq ) 2 . Max ( Complete link ): largest distance between an element in one cluster and an element in the other'), Document(metadata={}, page_content=\"in one cluster and an element in the other , dist(Ki , Kj ) = max(tip , tjq ) 3 .  : avg distance between an element in one cluster and an element in the other , dist(Ki , Kj ) = avg(tip , tjq ) 4 . Centroid : distance between the centroids of two clusters , dist(Ki , Kj ) = dist(Ci , Cj ) X X 14 14 Now that we 've understood how to measure the distance between two clusters , let 's go back to the steps of the  algorithm . 15 Now that we 've understood how to measure the distance between two\"), Document(metadata={}, page_content=\"how to measure the distance between two clusters , let 's go back to the steps of the  algorithm . Now that we 've understood how to measure the distance between two clusters , let 's go back to the steps of the  algorithm . 15 Agglomerative clustering : Steps 1 and 2 Start with clusters of individual points and a proximity matrix p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  16 Agglomerative clustering : Steps 1 and 2 = Start with clusters of individual points and a proximity matrix O O O O O O O\"), Document(metadata={}, page_content='points and a proximity matrix O O O O O O O O . O -  OO O p1 p2 p3 p4 pd p10 p11 p12 Agglomerative clustering : Steps 1 and 2 Start with clusters of individual points and a proximity matrix p1 p3 p5 p4 p2 p1 p2 p3 p4 p5 . . . . . .  16  some merging steps , we have some clusters C1 C4 C2 C5 C3 C2 C1 C1 C3 C5 C4 C2 C3 C4 C5  17  = u After some merging steps , we have some clusters C1 C2 C3 C4 C5  ror bi re 17  some merging steps , we have some clusters C1 C4 C2 C5 C3 C2 C1 C1 C3 C5 C4 C2 C3 C4'), Document(metadata={}, page_content='C1 C4 C2 C5 C3 C2 C1 C1 C3 C5 C4 C2 C3 C4 C5  17 Step 4 Merge the two closest clusters ( C2 and C5 ) and update the matrix C1 C4 C2 C5 C3 C2 C5 18 step 4 = m Merge the two closest clusters ( C2 and C5 ) and update the matrix | 7 ) coke oo ’ rte ° Step 4 Merge the two closest clusters ( C2 and C5 ) and update the matrix C1 C4 C2 C5 C3 C2 C5 18 Step 5 Now , the question is “ how do we update the proximity matrix ? ” C1 C4 C2 U C5 C3 19 step 5 = Now , the question is “ how do we update the'), Document(metadata={}, page_content='5 = Now , the question is “ how do we update the proximity matrix ? ” op | | | ce p3 p4 po p10 pti p12 19 Step 5 Now , the question is “ how do we update the proximity matrix ? ” C1 C4 C2 U C5 C3 19 Answer : we update the proximity matrix using the different approaches to defining the distance between clusters ( Min , Max , etc . ) Note : to compute the distance between an individual data point and a cluster , we consider that data point itself as a cluster 20 Answer : we update the proximity'), Document(metadata={}, page_content='as a cluster 20 Answer : we update the proximity matrix using the different approaches to defining the distance between clusters ( Min , Max , etc . ) Note : to compute the distance between an individual data point and a cluster , we consider that data point itself as a cluster 20 Answer : we update the proximity matrix using the different approaches to defining the distance between clusters ( Min , Max , etc . ) Note : to compute the distance between an individual data point and a cluster , we'), Document(metadata={}, page_content='an individual data point and a cluster , we consider that data point itself as a cluster 20  1 2 3 4 5 6 1 2 3 4 5  : MIN  : MIN  1 2 3 4 5 6 1 2 3 4 5  : MIN Strength of MIN  detects clusters of any shape by focusing only on the nearest points between clusters , ignoring overall shape . Captures irregularly shaped clusters effectively without assuming specific geometrical forms like elliptical shapes . 22 Strength of MIN  detects clusters of any shape by focusing only on the nearest points'), Document(metadata={}, page_content='any shape by focusing only on the nearest points between clusters , ignoring overall shape . Captures irregularly shaped clusters effectively without assuming specific geometrical forms like elliptical shapes . 22 Strength of MIN  detects clusters of any shape by focusing only on the nearest points between clusters , ignoring overall shape . Captures irregularly shaped clusters effectively without assuming specific geometrical forms like elliptical shapes . 22 Limitations of MIN  effect :'), Document(metadata={}, page_content=\"shapes . 22 Limitations of MIN  effect : Merges two clusters due to closely paired points , leading to a chain of combined clusters . Noise sensitivity : A single point can alter the cluster 's shape .  23 Limitations of MIN @ cs , od ef 8 ° Ld oe i ) sts . soe : man ae eh ° wo fe ccs y ‘ ” ° @ oe ry  ¢ & t ¢ Seas ‘ oe oes oe @ ° oe “ fr oe ars ° ° fe “ & > * é °  . ys & , acy O Chaining effect : Merges two clusters due SG . - Sc } to closely paired points , leading to a chain ~ rah of combined\"), Document(metadata={}, page_content=\"points , leading to a chain ~ rah of combined clusters . é . oe ” ~ . oe , © Noise sensitivity : A single point can alter the cluster 's shape .  23 Limitations of MIN  effect : Merges two clusters due to closely paired points , leading to a chain of combined clusters . Noise sensitivity : A single point can alter the cluster 's shape .  23  1 2 3 4 5 6 1 2 5 3 4  : MAX  : MAX 0.4- 0.35- 0.3- 0.25- 0.2- 0.15- 0.17 0.05-  1 2 3 4 5 6 1 2 5 3 4  : MAX Strength of MAX  to Noise : Less affected by\"), Document(metadata={}, page_content='MAX Strength of MAX  to Noise : Less affected by noise because it looks at the farthest points between clusters , forming compact groups less likely to be influenced by outliers . 25 x. X < = Qu O — C — 41 0 77 ° ie ) oe H @ 8 eo ) C )  farthest points between clusters , forming compact groups less likely to Robustness to Noise : Less affected by noise because it looks at the be influenced by outliers . 25 Strength of MAX  to Noise : Less affected by noise because it looks at the farthest'), Document(metadata={}, page_content='by noise because it looks at the farthest points between clusters , forming compact groups less likely to be influenced by outliers . 25 Limitations of MAX  to break large clusters into smaller , more distinct ones . Biased towards globular clusters 26 tations of MAX imi L © Se ° oo , 6 ‘ 3 © wy reget ’ ° owets Ve , @ cod 2 5 $ Se % Mee 3 . # 7 ° ° » Ld % Ce 20 aed ae eee ” 3 % % . @ aoe 2 eet Fe . O ” we « , * te So oe ’ f ° sy & ° . ot % 73 ° ° oe \" ? ha he ok 3 ° ad ee , a ° os cy 8 * . , of'), Document(metadata={}, page_content='oe \" ? ha he ok 3 ° ad ee , a ° os cy 8 * . , of % *  to break large clusters into smaller , more distinct ones . O Biased towards globular clusters O 26 Limitations of MAX  to break large clusters into smaller , more distinct ones . Biased towards globular clusters 26  1 2 3 4 5 6 1 2 5 3 4  :  :  1 2 3 4 5 6 1 2 5 3 4  :  :  between Single and  reduces the influence of noisy data points  towards globular clusters because the average distance favors clusters with compact , closely located'), Document(metadata={}, page_content='favors clusters with compact , closely located points 28  :  between Single and  reduces the influence of noisy data points  towards globular clusters because the average distance favors clusters with compact , closely located points 28  :  between Single and  reduces the influence of noisy data points  towards globular clusters because the average distance favors clusters with compact , closely located points 28  : Space and  N is the number of data points or objects . Space : O(N2 ) O(N² )'), Document(metadata={}, page_content='of data points or objects . Space : O(N2 ) O(N² ) because the proximity matrix has N² entries for distances between N points . Time : O(N3 ) Find the min distance of the matrix O(N2 ) * N iterations ⇒ O(N³ ) Complexity can be reduced to O(N2 log(N ) ) Accelerate finding the minimum using a heap … . 29  : Space and  N is the number of data points or objects . Space : O(N ’ ) m O(N ’ ) because the proximity matrix has N ? entries for distances between N points . Time : O(N * ) m Find the min'), Document(metadata={}, page_content='between N points . Time : O(N * ) m Find the min distance of the matrix O(N ’ ) * N iterations > O(N ® ) = Complexity can be reduced to O(N ? log(N ) ) = Accelerate finding the minimum using a heap .... 29  : Space and  N is the number of data points or objects . Space : O(N2 ) O(N² ) because the proximity matrix has N² entries for distances between N points . Time : O(N3 ) Find the min distance of the matrix O(N2 ) * N iterations ⇒ O(N³ ) Complexity can be reduced to O(N2 log(N ) ) Accelerate'), Document(metadata={}, page_content='can be reduced to O(N2 log(N ) ) Accelerate finding the minimum using a heap … . 29 Strength of  not have to assume any particular number of clusters . Any desired number of clusters can be obtained by ‘ cutting ’ the dendrogram at the proper level . They may correspond to meaningful taxonomies Example in biological sciences ( .g . , animal kingdom , phylogeny reconstruction , … ) 30 Strength of  = Donothave to assume any particular number of clusters . « Any desired number of clusters can be'), Document(metadata={}, page_content='. « Any desired number of clusters can be obtained by ‘ cutting ’ the dendrogram at the proper level . « = They may correspond to meaningful taxonomies « Example in biological sciences ( .g . , animal kingdom , phylogeny reconstruction , ... ) Strength of  not have to assume any particular number of clusters . Any desired number of clusters can be obtained by ‘ cutting ’ the dendrogram at the proper level . They may correspond to meaningful taxonomies Example in biological sciences ( .g . ,'), Document(metadata={}, page_content='Example in biological sciences ( .g . , animal kingdom , phylogeny reconstruction , … ) 30 Weakness of  a decision is made to combine two clusters , it can not be undone Do not scale well : time complexity of O(n3 ) , n is the number of objects No global objective function is directly minimized Different schemes have problems with one or more of the following : Sensitivity to noise Difficulty handling clusters of different sizes and non - globular shapes Breaking large clusters Improvements :'), Document(metadata={}, page_content='shapes Breaking large clusters Improvements : Integration of hierarchical and distance - based clustering Example of Algorithms : BIRCH , CHAMELEON 31 Weakness of  » Once a decision is made to combine two clusters , it can not be undone = Donot scale well : time complexity of O(n ) , nis the number of objects » No global objective function is directly minimized » Different schemes have problems with one or more of the following : » Sensitivity to noise » Difficulty handling clusters of'), Document(metadata={}, page_content='to noise » Difficulty handling clusters of different sizes and non - globular shapes » Breaking large clusters Improvements : Integration of hierarchical and distance - based clustering » Example of Algorithms : BIRCH , CHAMELEON 31 Weakness of  a decision is made to combine two clusters , it can not be undone Do not scale well : time complexity of O(n3 ) , n is the number of objects No global objective function is directly minimized Different schemes have problems with one or more of the'), Document(metadata={}, page_content='schemes have problems with one or more of the following : Sensitivity to noise Difficulty handling clusters of different sizes and non - globular shapes Breaking large clusters Improvements : Integration of hierarchical and distance - based clustering Example of Algorithms : BIRCH , CHAMELEON 31'), Document(metadata={}, page_content='3 & 1  3 &  3 & 1 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  2 Outline L } Overview of Clustering LJ  LJ K - means Clustering LJ  . ) DBSCAN Clustering LJ }  ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  2 Density - based  - based methods use density to discover clusters of any shape . Density means the concentration of data points in a given region . Clusters are regions of high density separated by regions of low density'), Document(metadata={}, page_content='high density separated by regions of low density . Major features : Discover clusters of arbitrary shape Handle noise One scan only No need to define the number of clusters in advance Density - based  - based methods use density to discover clusters of any shape . Density means the concentration of data points in a given region . Clusters are regions of high density separated by regions of low density . Major features : = Discover clusters of arbitrary shape = Handle noise m One scan only ='), Document(metadata={}, page_content='arbitrary shape = Handle noise m One scan only = Noneed to define the number of clusters in advance Density - based  - based methods use density to discover clusters of any shape . Density means the concentration of data points in a given region . Clusters are regions of high density separated by regions of low density . Major features : Discover clusters of arbitrary shape Handle noise One scan only No need to define the number of clusters in advance DBSCAN Clustering DBSCAN : Density -  of'), Document(metadata={}, page_content='advance DBSCAN Clustering DBSCAN : Density -  of Applications with  defines a cluster as a maximal set of density - connected points . Density is the number of data points within a certain radius . The parameters of this algorithm are : ε the maximum radius of the neighborhood Min . Pts the minimal number of point in a dense neighborhood . DBSCAN Clustering DBSCAN : Density -  of Applications with  defines a cluster as a maximal set of density - connected points . Density is the number of data'), Document(metadata={}, page_content='connected points . Density is the number of data points within a certain radius . The parameters of this algorithm are : » € the maximum radius of the neighborhood » Min . Pts the minimal number of point in a dense neighborhood . DBSCAN Clustering DBSCAN : Density -  of Applications with  defines a cluster as a maximal set of density - connected points . Density is the number of data points within a certain radius . The parameters of this algorithm are : ε the maximum radius of the neighborhood'), Document(metadata={}, page_content='are : ε the maximum radius of the neighborhood Min . Pts the minimal number of point in a dense neighborhood . ε - neighborhood The neighborhood within a radius ε of a given object is called the ε - neighborhood of the object . € -neighborhood The neighborhood within a radius € of a given object is called the -neighborhood of the object . ε - neighborhood The neighborhood within a radius ε of a given object is called the ε - neighborhood of the object . Core objects If the ε - neighborhood of'), Document(metadata={}, page_content='object . Core objects If the ε - neighborhood of an object contains at least a minimum number of objects Min . Pts , then the object is called a core object . Core objects If the ¢-neighborhood of an object contains at least a minimum number of objects Min . Pts , then the object is called a core object . Min . Pts = 5 Core objects If the ε - neighborhood of an object contains at least a minimum number of objects Min . Pts , then the object is called a core object . DBSCAN : Core , Border , and'), Document(metadata={}, page_content='a core object . DBSCAN : Core , Border , and  A core point has at least a specified number of points ( Min . Pts ) within ε A border point is not a core point , but is in the neighborhood of a core point A noise point is any point that is not a core point or a border point DBSCAN : Core , Border , and  point has at least a specified number of points ( Min . Pts ) within Aborder point is not a core point , but is in the neighborhood of a core point A noise point is any point that is not a core'), Document(metadata={}, page_content='A noise point is any point that is not a core point or a border point DBSCAN : Core , Border , and  A core point has at least a specified number of points ( Min . Pts ) within ε A border point is not a core point , but is in the neighborhood of a core point A noise point is any point that is not a core point or a border point  ( 1 / 2 ) Given a set of objects D , we say that an object p is directly density - reachable from an object q if : “ p is within the ε - neighborhood of q , and q is a'), Document(metadata={}, page_content='is within the ε - neighborhood of q , and q is a core object . ”  ( 1 / 2 ) Given a set of objects D , we say that an object p is directly density - reachable from an object q if : “ p is within the € -neighborhood of q , and q is a core object . ” Min . Pts= 5  ( 1 / 2 ) Given a set of objects D , we say that an object p is directly density - reachable from an object q if : “ p is within the ε - neighborhood of q , and q is a core object . ”  ( 2 / 2 ) Given a set of objects D , an object p is'), Document(metadata={}, page_content='2 / 2 ) Given a set of objects D , an object p is density - reachable from an object q with respect to ε and Min . Pts if : “ There is a chain of objects p1 , … , pn , where p1 = q and pn = p such that pi+1 is directly density - reachable from pi with respect to ε and Min . Pts , for 1 ≤ i ≤ n and pi ∊ D. ”  ( 2 | 2 ) Given a set of objects D , an object p is density - reachable from an object q with respect to € and Min . Pts if : “ There is a chain of objects p , , ... , p , , where p , = q'), Document(metadata={}, page_content='chain of objects p , , ... , p , , where p , = q and p , = p such that p , , , is directly density - reachable from p , with respect to € and Min . Pts , for 1 Si < n and p , € D. ” ( \\\\/ ’ Min . Pts = 5  ( 2 / 2 ) Given a set of objects D , an object p is density - reachable from an object q with respect to ε and Min . Pts if : “ There is a chain of objects p1 , … , pn , where p1 = q and pn = p such that pi+1 is directly density - reachable from pi with respect to ε and Min . Pts , for 1 ≤ i ≤'), Document(metadata={}, page_content='pi with respect to ε and Min . Pts , for 1 ≤ i ≤ n and pi ∊ D. ”  a set of objects D , an object p is density - connected to an object q with respect to ε and Min . Pts if : “ There is an object ∊ D such that both p and q are density - reachable from with respect to ε and Min . Pts ”  a set of objects D , an object p is density - connected to an object q with respect to € and Min . Pts if : “ There is an object € D such that both p and q are density - reachable from with respect to € and Min .'), Document(metadata={}, page_content='- reachable from with respect to € and Min . Pts ” Min . Pts = 5  a set of objects D , an object p is density - connected to an object q with respect to ε and Min . Pts if : “ There is an object ∊ D such that both p and q are density - reachable from with respect to ε and Min . Pts ” Density - based Clusters A density - based cluster is a set of density - connected points that is maximal with respect to density - reachability . Every object not contained in any cluster is considered noise .'), Document(metadata={}, page_content='contained in any cluster is considered noise . Density - based Clusters A density - based cluster is a set of density - connected points that is maximal with respect to density - reachability . Every object not contained in any cluster is considered noise . Density - based Clusters A density - based cluster is a set of density - connected points that is maximal with respect to density - reachability . Every object not contained in any cluster is considered noise . DBSCAN Algorithm : Intuition'), Document(metadata={}, page_content='considered noise . DBSCAN Algorithm : Intuition 12 “ By recursively exploring the neighborhood of core points within the ε - distance threshold and incorporating reachable points into clusters , the DBSCAN algorithm identifies dense regions in the dataset while also detecting outliers ( noise points ) that do not fit within these dense regions . ” DBSCAN Algorithm : Intuition “ By recursively exploring the neighborhood of core points within the € -distance threshold and incorporating reachable'), Document(metadata={}, page_content='€ -distance threshold and incorporating reachable points into clusters , the DBSCAN algorithm identifies dense regions in the dataset while also detecting outliers ( noise points ) that do not fit within these dense regions . ” DBSCAN Algorithm : Intuition 12 “ By recursively exploring the neighborhood of core points within the ε - distance threshold and incorporating reachable points into clusters , the DBSCAN algorithm identifies dense regions in the dataset while also detecting outliers ('), Document(metadata={}, page_content='in the dataset while also detecting outliers ( noise points ) that do not fit within these dense regions . ” DBSCAN Algorithm 13 1 . Choose ε ( a positive number ) and Min . Points ( a natural number ) . 2 . Select an arbitrary point P from the dataset . 3 . Check if point P is a core point . If yes , form a cluster including P. 4 . Recursively add core points within the ε - neighborhood of the already added points to the cluster . 5 . After fully expanding a cluster , select a new unvisited'), Document(metadata={}, page_content='expanding a cluster , select a new unvisited point and repeat the process ( from point 2 to 4 ) . 6 .  : Assign each border point to one of the clusters of its ε - neighborhood core points . 7 .  : Label points that are neither core points nor border points as noise . - YY BS DBSCAN  € ( a positive number ) and Min . Points ( a natural number ) . Select an arbitrary point P from the dataset . Check if point P is a core point . If yes , form a cluster including P. Recursively add core points'), Document(metadata={}, page_content='cluster including P. Recursively add core points within the -neighborhood of the already added points to the cluster . After fully expanding a cluster , select a new unvisited point and repeat the process ( from point 2 to 4 ) .  : Assign each border point to one of the clusters of its € -neighborhood core points .  : Label points that are neither core points nor border points as noise . 13 DBSCAN Algorithm 13 1 . Choose ε ( a positive number ) and Min . Points ( a natural number ) . 2 . Select'), Document(metadata={}, page_content='Min . Points ( a natural number ) . 2 . Select an arbitrary point P from the dataset . 3 . Check if point P is a core point . If yes , form a cluster including P. 4 . Recursively add core points within the ε - neighborhood of the already added points to the cluster . 5 . After fully expanding a cluster , select a new unvisited point and repeat the process ( from point 2 to 4 ) . 6 .  : Assign each border point to one of the clusters of its ε - neighborhood core points . 7 .  : Label points that'), Document(metadata={}, page_content='core points . 7 .  : Label points that are neither core points nor border points as noise . DBSCAN vs K - Means ( 1 / 2 ) 14 DBSCAN vs K - Means ( 1 | 2 ) DBSCAN © ) AY a k - means © ) fi J . a DBSCAN vs K - Means ( 1 / 2 ) 14 DBSCAN vs K - Means ( 2 / 2 ) 15 K - Means algorithm with different values of K and shapes of data : https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ DBSCAN algorithm with different shapes of data :'), Document(metadata={}, page_content='DBSCAN algorithm with different shapes of data : https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ DBSCAN vs K - Means ( 2 | 2 ) K - Means algorithm with different values of K and shapes of data : https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ DBSCAN algorithm with different shapes of data : https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ DBSCAN vs K - Means ( 2 / 2 ) 15 K - Means algorithm with different values of K and shapes of data :'), Document(metadata={}, page_content='with different values of K and shapes of data : https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ DBSCAN algorithm with different shapes of data : https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ DBSCAN :  .  guidelines for setting Min . Pts : Larger datasets require a larger Min . Pts value . Min . Pts must be chosen at least 3 . In noisier datasets , choose a larger Min . Pts value . Min . Pts should generally be ≥ the dimensionality of the dataset . Example'), Document(metadata={}, page_content='be ≥ the dimensionality of the dataset . Example : Min . Pts = 2 * number of dimensions Domain knowledge is crucial to select an appropriate Min . Pts value . 16 DBSCAN :  .  guidelines for setting Min . Pts : Larger datasets require a larger Min . Pts value . Min . Pts must be chosen at least 3 . In noisier datasets , choose a larger Min . Pts value . Min . Pts should generally be 2 the dimensionality of the dataset . = Example : Min . Pts = 2 * number of dimensions Domain knowledge is crucial'), Document(metadata={}, page_content='number of dimensions Domain knowledge is crucial to select an appropriate Min . Pts value . 16 DBSCAN :  .  guidelines for setting Min . Pts : Larger datasets require a larger Min . Pts value . Min . Pts must be chosen at least 3 . In noisier datasets , choose a larger Min . Pts value . Min . Pts should generally be ≥ the dimensionality of the dataset . Example : Min . Pts = 2 * number of dimensions Domain knowledge is crucial to select an appropriate Min . Pts value . 16 DBSCAN : Determining ε'), Document(metadata={}, page_content=\"Min . Pts value . 16 DBSCAN : Determining ε using K -  the average distance of each point to its K ( K = Min . Pts - 1 ) nearest neighbors . Plot these K - distances in ascending order . The ' knee ' in the plot represents a threshold where a sharp change in distance occurs . This point is indicative of the optimal ε value . Helps to distinguish between core , border , and noise points in the data 17 DBSCAN : Determining € using K -  the average distance of each point to its K ( K = Min . Pts -\"), Document(metadata={}, page_content=\"distance of each point to its K ( K = Min . Pts - 1 ) nearest neighbors . Plot these K - distances in ascending order . The ' knee ' in the plot represents a threshold where a sharp change in distance occurs . This point is indicative of the optimal € value . Helps to distinguish between core , border , and noise points in the data 17 DBSCAN : Determining ε using K -  the average distance of each point to its K ( K = Min . Pts - 1 ) nearest neighbors . Plot these K - distances in ascending\"), Document(metadata={}, page_content=\"neighbors . Plot these K - distances in ascending order . The ' knee ' in the plot represents a threshold where a sharp change in distance occurs . This point is indicative of the optimal ε value . Helps to distinguish between core , border , and noise points in the data 17 DBSCAN : Determining ε and Min .  : for points in a cluster , their Kth nearest neighbors are at close distance Noise points have the Kth nearest neighbor at farther distance So , plot sorted distance of every point to its\"), Document(metadata={}, page_content='So , plot sorted distance of every point to its Kth nearest neighbor DBSCAN : Determining € and Min .  : for points in a cluster , their Kth nearest neighbors are at close distance Noise points have the Kth nearest neighbor at farther distance So , plot sorted distance of every point to its Kth nearest neighbor 4th  i i i 0 500 1000 1500 2000 2500 3000  to Distance of 4th  DBSCAN : Determining ε and Min .  : for points in a cluster , their Kth nearest neighbors are at close distance Noise'), Document(metadata={}, page_content='Kth nearest neighbors are at close distance Noise points have the Kth nearest neighbor at farther distance So , plot sorted distance of every point to its Kth nearest neighbor DBSCAN : Advantages and  : Can handle clusters of different shapes and sizes Resistant to noise and outliers Does n’t require predefined number of clusters . Disadvantages : Sensitivity to the two parameters ε and Min .  with varying density clusters Not suitable for high - dimensional data due to the curse of'), Document(metadata={}, page_content=\"for high - dimensional data due to the curse of dimensionality DBSCAN : Advantages and  : m Can handle clusters of different shapes and sizes m Resistant to noise and outliers = Does n't require predefined number of clusters . Disadvantages : = Sensitivity to the two parameters € and Min . Pts = Difficulty with varying density clusters = Not suitable for high - dimensional data due to the curse of dimensionality DBSCAN : Advantages and  : Can handle clusters of different shapes and sizes\"), Document(metadata={}, page_content='Can handle clusters of different shapes and sizes Resistant to noise and outliers Does n’t require predefined number of clusters . Disadvantages : Sensitivity to the two parameters ε and Min .  with varying density clusters Not suitable for high - dimensional data due to the curse of dimensionality Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  20 Outline L } Overview of Clustering LJ  LJ K - means Clustering LJ  . ) DBSCAN Clustering LJ  20 Outline ❏'), Document(metadata={}, page_content='LJ  . ) DBSCAN Clustering LJ  20 Outline ❏ Overview of Clustering ❏  ❏ K - means Clustering ❏  ❏ DBSCAN Clustering ❏  20'), Document(metadata={}, page_content='Clustering ( part 4 ) & 1 Clustering ( part 4 ) & Clustering ( part 4 ) & 1 Outline ❏ Clustering evaluation ❏ Why cluster evaluation ? ❏ Types of cluster evaluation measures ❏ Unsupervised evaluation ❏ Cohesion vs Separation ❏  ❏ Supervised evaluation ❏ Entropy ❏ Precision , Recall , F - measure 2  evaluation [ ) Why cluster evaluation ? ‘ i Types of cluster evaluation measures 11 Unsupervised evaluation [ ) Cohesion vs Separation 4  1 Supervised evaluation 1 )  , Recall , F - measure Outline ❏'), Document(metadata={}, page_content='evaluation 1 )  , Recall , F - measure Outline ❏ Clustering evaluation ❏ Why cluster evaluation ? ❏ Types of cluster evaluation measures ❏ Unsupervised evaluation ❏ Cohesion vs Separation ❏  ❏ Supervised evaluation ❏ Entropy ❏ Precision , Recall , F - measure 2 Why cluster evaluation ? Generate a random data points . Data without any structure Question : What is the result of applying K - Means with K=3 ? The following link can be used : K -  3 Why cluster evaluation ? Generate a random data'), Document(metadata={}, page_content='3 Why cluster evaluation ? Generate a random data points . Data without any structure Question : What is the result of applying K - Means with K=3 ? The following link can be used : K -  fo Why cluster evaluation ? Generate a random data points . Data without any structure Question : What is the result of applying K - Means with K=3 ? The following link can be used : K -  3 Why cluster evaluation ? Generate a random data points . Data without any structure Clusters found in  ! ! The following'), Document(metadata={}, page_content='structure Clusters found in  ! ! The following link can be used : K -  4 Why cluster evaluation ? Generate a random data points . Data without any structure Clusters found in  ! ! The following link can be used : K -  cluster evaluation ? Generate a random data points . Data without any structure Clusters found in  ! ! The following link can be used : K -  4 Why cluster evaluation ? To avoid Detecting clusters in random  whether non - random structure exists in the data . To evaluate  how well'), Document(metadata={}, page_content='exists in the data . To evaluate  how well the clustering aligns with the data without external reference . To compare with external known patterns Comparing clustering results to externally known information , .g . , class labels . To compare different Clusterings and algorithms Evaluating and comparing different sets of clusters for quality . 5 Why cluster evaluation ? To avoid Detecting clusters in random  whether non - random structure exists in the data . To evaluate  how well the'), Document(metadata={}, page_content='exists in the data . To evaluate  how well the clustering aligns with the data without external reference . To compare with external known patterns Comparing clustering results to externally known information , .g . , class labels . To compare different Clusterings and algorithms © Evaluating and comparing different sets of clusters for quality . Why cluster evaluation ? To avoid Detecting clusters in random  whether non - random structure exists in the data . To evaluate  how well the'), Document(metadata={}, page_content='exists in the data . To evaluate  how well the clustering aligns with the data without external reference . To compare with external known patterns Comparing clustering results to externally known information , .g . , class labels . To compare different Clusterings and algorithms Evaluating and comparing different sets of clusters for quality . 5 “ The validation of clustering structures is the most difficult and frustrating part of cluster analysis . Without a strong effort in this direction ,'), Document(metadata={}, page_content='. Without a strong effort in this direction , cluster analysis will remain a black art accessible only to those true believers who have experience and great courage . ” Algorithms for  , and 6 “ The validation of clustering structures is the most difficult and frustrating part of cluster analysis . Without a strong effort in this direction , cluster analysis will remain a black art accessible only to those true believers who have experience and great courage . ” Algorithms for  , and Dubes “'), Document(metadata={}, page_content='great courage . ” Algorithms for  , and Dubes “ The validation of clustering structures is the most difficult and frustrating part of cluster analysis . Without a strong effort in this direction , cluster analysis will remain a black art accessible only to those true believers who have experience and great courage . ” Algorithms for  , and 6 Types of cluster evaluation measures Unsupervised ( Internal ): measure the goodness of a clustering structure without respect to external information .'), Document(metadata={}, page_content='without respect to external information . The ground truth is not available . Examples : Cohesion , separation , SSE ,  . Supervised ( External ) : measure the extent to which cluster labels match externally supplied class labels . The ground truth is available . Examples : Entropy , Precision , Recall , F - measure . 7 Types of cluster evaluation measures Unsupervised ( Internal ): measure the goodness of a clustering structure without respect to external information . The ground truth is not'), Document(metadata={}, page_content='to external information . The ground truth is not available . Examples : Cohesion , separation , SSE ,  . Supervised ( External ) : measure the extent to which cluster labels match externally supplied class labels . The ground truth is available . Examples : Entropy , Precision , Recall , F - measure . Types of cluster evaluation measures Unsupervised ( Internal ): measure the goodness of a clustering structure without respect to external information . The ground truth is not available .'), Document(metadata={}, page_content='information . The ground truth is not available . Examples : Cohesion , separation , SSE ,  . Supervised ( External ) : measure the extent to which cluster labels match externally supplied class labels . The ground truth is available . Examples : Entropy , Precision , Recall , F - measure . 7 Outline ❏ Clustering evaluation ❏ Why cluster evaluation ? ❏ Types of cluster evaluation measures ❏ Unsupervised evaluation ❏ Cohesion vs Separation ❏  ❏ Supervised evaluation ❏ Entropy ❏ Precision ,'), Document(metadata={}, page_content='❏ Supervised evaluation ❏ Entropy ❏ Precision , Recall , F - measure 8 Outline 11 Clustering evaluation [ ) Why cluster evaluation ? ) Types of cluster evaluation measures W@ Unsupervised evaluation [ ) Cohesion vs Separation 4  1 Supervised evaluation 1 )  , Recall , F - measure Outline ❏ Clustering evaluation ❏ Why cluster evaluation ? ❏ Types of cluster evaluation measures ❏ Unsupervised evaluation ❏ Cohesion vs Separation ❏  ❏ Supervised evaluation ❏ Entropy ❏ Precision , Recall , F -'), Document(metadata={}, page_content='evaluation ❏ Entropy ❏ Precision , Recall , F - measure 8 Cohesion vs  cohesion ( Compactness ) Measure how closely related object in a cluster .  how distinct or well- separated a cluster is from other clusters . 9 Cohesion vs  cohesion ( Compactness ) Measure how closely related object in a cluster .  how distinct or well- separated a cluster is from other clusters . Cohesion vs  cohesion ( Compactness ) Measure how closely related object in a cluster .  how distinct or well- separated a'), Document(metadata={}, page_content='in a cluster .  how distinct or well- separated a cluster is from other clusters . 9 Graph -  graph where the weights are the distances between data points . Cohesion : Sum of proximities in a cluster . Separation : Sum of proximities between two clusters . 10 Graph -  graph where the weights are the distances between data points . Cohesion : Sum of proximities in a cluster . cohesion(C ;) = > proximity(x , y ) x€e . C ; yer ; Separation : Sum of proximities between two clusters .'), Document(metadata={}, page_content=': Sum of proximities between two clusters . separation(C;,Cj ) = S — proximity(x , y ) vec cohesion separation Graph -  graph where the weights are the distances between data points . Cohesion : Sum of proximities in a cluster . Separation : Sum of proximities between two clusters . 10 Prototype -  a clusters using their centroids . Cohesion : Sum of proximities to the cluster centroid . Separation : Sum of proximites between centroids . Between two centroids Between a cluster centroid and the'), Document(metadata={}, page_content='two centroids Between a cluster centroid and the global centroid 11 Prototype -  a clusters using their centroids . Cohesion : Sum of proximities to the cluster centroid . cohesion(C ;) = > proximity(x , ¢ ;) x. E€C ; Separation : Sum of proximites between centroids . Between two centroids separation(C;,C ;) = proximity(c;,c ;) Between a cluster centroid and the global centroid separation(C ;) = proximity(ci , c ) Prototype -  a clusters using their centroids . Cohesion : Sum of proximities to'), Document(metadata={}, page_content='centroids . Cohesion : Sum of proximities to the cluster centroid . Separation : Sum of proximites between centroids . Between two centroids Between a cluster centroid and the global centroid 11 Prototype -  a clusters using their centroids . Cohesion : Sum of proximities to the cluster centroid . Separation : Sum of proximites between centroids . Between two centroids Between a cluster centroid and the global centroid SSE is the sum of prototype based cohesion of all clusters . 12 Prototype -'), Document(metadata={}, page_content='based cohesion of all clusters . 12 Prototype -  a clusters using their centroids . Cohesion : Sum of proximities to the cluster centroid . cohesion(C ;) = S — proximity(x , ¢ ;) x. EC ; Between a cluster centroid and the global centroid separation(C ;) = proximity(ci , c ) Prototype -  a clusters using their centroids . Cohesion : Sum of proximities to the cluster centroid . Separation : Sum of proximites between centroids . Between two centroids Between a cluster centroid and the global'), Document(metadata={}, page_content='Between a cluster centroid and the global centroid SSE is the sum of prototype based cohesion of all clusters . 12  coefficient combines cohesion and separation . For an individual point i a = average distance of i to the points in its cluster b = min ( average distance of i to points in another cluster ) The silhouette coefficient for a point is s = ( b – a ) / max(a , b ) Value can vary between -1 and 1 . The closer to 1 the better . 13  coefficient combines cohesion and separation . For an'), Document(metadata={}, page_content='combines cohesion and separation . For an individual point i © a = average distance of i to the points in its cluster fe b = min ( average distance of if to points in another cluster ) C , [ The silhouette coefficient for a point is = s = ( b — a ) / max(a , b ) Value can vary between -1 and 1 . The closer to 1 the better .  coefficient combines cohesion and separation . For an individual point i a = average distance of i to the points in its cluster b = min ( average distance of i to points in'), Document(metadata={}, page_content='b = min ( average distance of i to points in another cluster ) The silhouette coefficient for a point is s = ( b – a ) / max(a , b ) Value can vary between -1 and 1 . The closer to 1 the better . 13 Outline ❏ Clustering evaluation ❏ Why cluster evaluation ? ❏ Types of cluster evaluation measures ❏ Unsupervised evaluation ❏ Cohesion vs Separation ❏  ❏ Supervised evaluation ❏ Entropy ❏ Precision , Recall , F - measure 14 Outline 11 Clustering evaluation [ ) Why cluster evaluation ? ‘ i Types of'), Document(metadata={}, page_content='[ ) Why cluster evaluation ? ‘ i Types of cluster evaluation measures 11 Unsupervised evaluation [ ) Cohesion vs Separation 4  MH Supervised evaluation 1 )  , Recall , F - measure Outline ❏ Clustering evaluation ❏ Why cluster evaluation ? ❏ Types of cluster evaluation measures ❏ Unsupervised evaluation ❏ Cohesion vs Separation ❏  ❏ Supervised evaluation ❏ Entropy ❏ Precision , Recall , F - measure 14  measures the extent to which the clustering structure matches external class labels . Pure'), Document(metadata={}, page_content='structure matches external class labels . Pure cluster is cluster that contain only one class label . We measure the purity of a cluster using the entropy . How to  for Evaluation : Calculate entropy for each cluster . Sum the entropies to get an overall measure . Lower values indicate better alignment with external class labels . Pure cluster Impure cluster 15  measures the extent to which the clustering structure matches external class labels . Pure cluster is cluster that contain only one'), Document(metadata={}, page_content='. Pure cluster is cluster that contain only one class label . We measure the purity of a cluster using the entropy . How to  for Evaluation : Calculate entropy for each cluster . Sum the entropies to get an overall measure . Lower values indicate better alignment with external class labels . K L NE So = ) va SLk * Sik = ) — PLk logs Plk 1 A Pure cluster Impure cluster  measures the extent to which the clustering structure matches external class labels . Pure cluster is cluster that contain only'), Document(metadata={}, page_content='. Pure cluster is cluster that contain only one class label . We measure the purity of a cluster using the entropy . How to  for Evaluation : Calculate entropy for each cluster . Sum the entropies to get an overall measure . Lower values indicate better alignment with external class labels . Pure cluster Impure cluster 15 Entropy 16 Entropy S , = 0 a R a i Ww & $ uo Zoe AN m. M S _ = 1.585 3 LEE a8 a8 2/6 2/6 2/6 x AN ” 16 — Prx loge Pix a 1 ys ar A i tk c 5 , Entropy 16 Precision , Recall , F'), Document(metadata={}, page_content='ar A i tk c 5 , Entropy 16 Precision , Recall , F - measure Precision : The fraction of a cluster i that consists of objects of a specified class . Recall : The extent to which a cluster contains all objects of a specified class . F - measure : A combination of precision and recall that measures the extent to which a cluster contains only objects of a particular class and all objects of that class . 17 Precision , Recall , F - measure Precision : The fraction of a cluster / that consists of'), Document(metadata={}, page_content=': The fraction of a cluster / that consists of objects of a specified class . a Number of examples of class 7 in cluster 7 Psa ) Size of cluster 7 Recall : The extent to which a cluster contains all objects of a specified class . fentiii sh Number of examples of class j in cluster 7 ee Number of examples of class j F - measure : A combination of precision and recall that measures the extent to which a cluster contains only objects of a particular class and all objects of that class . 2 x'), Document(metadata={}, page_content='class and all objects of that class . 2 x Precision ( , 7 ) x Recall(i , 7 ) Fj . 4 ) — ( 7 ) Precision(i , j ) + Recall(é , j ) Precision , Recall , F - measure Precision : The fraction of a cluster i that consists of objects of a specified class . Recall : The extent to which a cluster contains all objects of a specified class . F - measure : A combination of precision and recall that measures the extent to which a cluster contains only objects of a particular class and all objects of that'), Document(metadata={}, page_content='of a particular class and all objects of that class . 17'), Document(metadata={}, page_content='& & & Outline 1- Data 1- Data 1-  is Data ? Data set : collection of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  is Data ? Data set : collection of objects and their attributes Attribute : property or'), Document(metadata={}, page_content='and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as _ variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance Objects oo NO a k. B WNY = a. K fo }  125 K 100 K 70 K 120 K 95 K 60 K 220 K 85 K 75 K 90 K  is Data ? Data set : collection of objects and their attributes Attribute :'), Document(metadata={}, page_content='of objects and their attributes Attribute : property or characteristic of an object Examples : eye color of a person , temperature , etc . Attribute is also known as variable , field , characteristic , dimension , or feature Object : collection of attributes Object is also known as record , point , case , sample , entity , or instance  of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) ,'), Document(metadata={}, page_content='taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples :'), Document(metadata={}, page_content=', medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Types of  ( Categories ) Examples : ID numbers , eye color , zip codes Ordinal (  ) Examples : Rankings ( .g . , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) Interval (  ,  ) Examples : Calendar dates , temperatures in Celsius or  (  ,  ) Examples :'), Document(metadata={}, page_content=', temperatures in Celsius or  (  ,  ) Examples : Temperature in Kelvin , length , counts , elapsed time ( .g . , time to run a race ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  (= , # ) Ordinal _ Distinctness ( = , # ) Order ( < > ) Interval _ Distinctness ( = , # ) Order ( < > )  ( + , - ) Ratio _ Distinctness ( = , # ) Order ( < , > )  ( + , - )  ( * , / ) Properties of  ( = , ≠ )  ( = ,'), Document(metadata={}, page_content=', - )  ( * , / ) Properties of  ( = , ≠ )  ( = , ≠ ) Order ( < > )  ( = , ≠ ) Order ( < > )  ( + , - )  ( = , ≠ ) Order ( < , > )  ( + , - )  ( * , / ) Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits'), Document(metadata={}, page_content='values , practically measured with finite digits Represented as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents .  as integers . Note : Binary attributes are a special case of discrete attributes . © ®  : Values are real numbers . Discrete v / s Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables . Discrete vs.'), Document(metadata={}, page_content='as floating - point variables . Discrete vs.  : Values from a finite or countably infinite set . Examples : Zip codes , counts , or words in documents . Represented as integers . Note : Binary attributes are a special case of discrete attributes .  : Values are real numbers . Examples : Temperature , height , weight . Real values , practically measured with finite digits Represented as floating - point variables .  asymmetric attributes , only the presence ( non - zero value ) matters .'), Document(metadata={}, page_content=\"only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n't buy most of the same products ? ”  attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions :\"), Document(metadata={}, page_content=\"appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n't buy most of the same products ? ”  asymmetric attributes , only the presence ( non - zero value ) matters . Examples : Words present in documents : Focus on words that appear . Items present in customer transactions : Emphasize purchased items . Real -  : In a grocery store encounter , would we say : “ Our purchases\"), Document(metadata={}, page_content=\"store encounter , would we say : “ Our purchases are similar because we did n't buy most of the same products ? ”  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data 's size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over\"), Document(metadata={}, page_content=\"challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data 's size .  centrality and dispersion in the data .  of  ( Number of Attributes ) High - dimensional data presents unique challenges .  the importance of presence over absence .  can vary based on the scale of measurement .  type of analysis often depends on the data 's size .  centrality and dispersion in the data . Types of  : records with fixed\"), Document(metadata={}, page_content='in the data . Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Person : © Relational records © Data matrix ... To | _ [  |i | | _ _ Alvaro ] Valencia__F~ norelation [ 4 | | [ fom — _ ] Car : [ cart | Model | Year |__Value | Pers 1D |_1o1_| [ 1973 | 100000 =| [ 102 |  | 1965 | 330000 ] 0 _ | [ 04 | | 2005 | 150000 ] 4 _ | ['), Document(metadata={}, page_content='| 330000 ] 0 _ | [ 04 | | 2005 | 150000 ] 4 _ | [ 0s [ renaut [ 1998 | — 2000 [ 3 _ _ ] 106 — [ _ Renautt [ 2007 | — 7o00 | 3 ] [ ior [ smart 199 ] 2000 ] 2 ] Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image'), Document(metadata={}, page_content='networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records © Data matrix ...  and  network Social or information networks ...  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …'), Document(metadata={}, page_content='…  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  : records with fixed attributes [ ) [ ) [ ) Relational records Data matrix ...  and Networks [ ) [ ) [ ) Transportation network Social or information networks ...  ( Sequence ) Data [ ) [ ) [ ) Video : sequence of image  sequence ... -| | J 34 j \\\\ i | Mi | \\\\ is ai py | c ot +1 \" ve 4 Lt jell 1s 202 J WI Vi YS 154 | if  \\\\ 1 WA 10 | Hi 54 1 10 40 50 60 70 80 0 9  of  : records'), Document(metadata={}, page_content='10 | Hi 54 1 10 40 50 60 70 80 0 9  of  : records with fixed attributes Relational records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images Types of  records © Data matrix ... /  and  network Streets ° Social or information networks ...  ( Sequence )  : sequence of image  sequence ...'), Document(metadata={}, page_content='( Sequence )  : sequence of image  sequence ...  RGB  images  of  records Data matrix …  and  network Social or information networks …  ( Sequence )  : sequence of image  sequence …  RGB  images 2- Data preprocessing 2- Data preprocessing 2- Data preprocessing What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data'), Document(metadata={}, page_content='cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression What is  ? —  cleaning'), Document(metadata={}, page_content='reduction Data compression What is  ? —  cleaning Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration Integration of multiple databases , data cubes , or files Data transformation and data discretization  reduction ( covered in the next chapter ) Dimensionality reduction Data compression  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied'), Document(metadata={}, page_content='. – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  adversely affects data processing efforts . Data r ° sy Quality ES 5Fe are r = Example : Poor data can result in wrong loan decisions . — Some credit - worthy'), Document(metadata={}, page_content='in wrong loan decisions . — Some credit - worthy candidates are denied loans — More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : — Noise and outliers — Wrong data — Fake data — Missing values — Duplicate data  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are'), Document(metadata={}, page_content='decisions . – Some credit - worthy candidates are denied loans – More loans are given to individuals that default What types of data quality issues exist ? and how can we identify them ? What can we do about these problems ? Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a'), Document(metadata={}, page_content='values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : © Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  in Objects : Extraneous elements affecting data integrity'), Document(metadata={}, page_content=': Extraneous elements affecting data integrity . Noise in Attributes : Modification of original attribute values . Examples : Distorted voice on a poor phone line . \" Snow \" on a television screen . Erroneous entries caused by data entry errors or system glitches .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios ,'), Document(metadata={}, page_content=': Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . * Case 1 : Outliers as Noise : © Outliers can be noise that disrupts data analysis . : Case 2 : Outliers as the Focus : © Incertain scenarios , outliers are the primary focus of analys © Credit card fraud'), Document(metadata={}, page_content='the primary focus of analys © Credit card fraud detection  detection . © &  : ae Explore the reasons behind the presence of outliers .  objects with characteristics significantly different from the majority in the dataset . Case 1 : Outliers as Noise : Outliers can be noise that disrupts data analysis . Case 2 : Outliers as the Focus : In certain scenarios , outliers are the primary focus of analysis . Credit card fraud detection Intrusion detection .  : Explore the reasons behind the presence'), Document(metadata={}, page_content='.  : Explore the reasons behind the presence of outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or'), Document(metadata={}, page_content=', enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . Regression : Smooth data through regression functions . Use other attributes to predict'), Document(metadata={}, page_content='functions . Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . Semi - supervised : Combine automated and human inspection to identify noise and outliers .  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables Estimate missing values'), Document(metadata={}, page_content='data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis Missing values  .  A / S 21171 PC 17599 STON / O2 . 3101282 113803 $ 3.1 C123 373450 8.05 Reasons for missing values 3 Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) 330877 8.4583 [ ) [ ) [ ) [ ) Handling'), Document(metadata={}, page_content='children ) 330877 8.4583 [ ) [ ) [ ) [ ) Handling missing values Eliminate data objects or variables Estimate missing values = Example : time series of temperature = Example : census results Ignore the missing value during analysis  for missing values Information is not collected ( .g . , people decline to give their age and weight ) Attributes may not be applicable to all cases ( .g . , annual income is not applicable to children ) Handling missing values Eliminate data objects or variables'), Document(metadata={}, page_content='values Eliminate data objects or variables Estimate missing values Example : time series of temperature Example : census results Ignore the missing value during analysis  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately .'), Document(metadata={}, page_content='unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . 2 How to handle duplicate data \\\\ Remove duplicate data objects . }  : When and Why ? prviemn ders Customers with multiple accounts may unintentionally accumulate points separately . © Keeping duplicate data ensures they'), Document(metadata={}, page_content='. © Keeping duplicate data ensures they receive all earned benefits .  of identical or nearly identical data objects . Common when merging data from diverse sources . Example : Identical individuals with multiple email addresses . How to handle duplicate data Remove duplicate data objects .  : When and Why ? Customers with multiple accounts may unintentionally accumulate points separately . Keeping duplicate data ensures they receive all earned benefits .  : Scaling data to a standard range ('), Document(metadata={}, page_content='benefits .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  : Scaling data to a standard range ( .g . , 0 to 1 ) . Discretization : Converting continuous data into'), Document(metadata={}, page_content='Discretization : Converting continuous data into discrete categories . Sampling : Selecting a subset to represent a larger population .  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ):  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max'), Document(metadata={}, page_content='for many data mining algorithms .  . Min - max normalization : to [ mew_min , , new_max , ] ; v — min : . ; vi = — — — — — _ _ ( new _ max:—new _ min:)+new _ min : max : — min : Z - score normalization ( yu : mean , : standard deviation ): _ X 7b o. O z  ensures that variables are on a consistent scale . Normalization is crucial for many data mining algorithms .  . Min - max normalization : to [ new_min . A , new_max . A ] Z - score normalization ( μ : mean , σ : standard deviation ): Min - max'), Document(metadata={}, page_content='( μ : mean , σ : standard deviation ): Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses 100 Min - max normalization : 3 59 vo Guarantees all attributes will have the Fs exact same scale . = 0 — — 0 20'), Document(metadata={}, page_content='will have the Fs exact same scale . = 0 — — 0 20 40 60 80 Does not handle outliers well . Years old Z - score normalization : .  using min - max normalizatio Handles outliers . 3 Vv B ® E10 ; E 107 eo ° S Does not produce normalized data & fe * . 2 | 2 a ye with the exact same scale . E 0.54 a. A ene 8 - © ° 8 = ob , 0 corde i. J ® * ee g COT — 2 0.0 0.2 0.4 0.6 0.8  old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will'), Document(metadata={}, page_content='normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale . Min - max'), Document(metadata={}, page_content='data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses pa lo } 50 Number of rooms Min - max normalization : Guarantees all attributes will have the 0 Miumalp . exact same scale . 0 20 40 60 80 . Years old Does not handle outliers well . 3 2Z - score normalization : s  using z - score normalizatio Handles outliers . 5 24 ° 33 } @ Does not produce normalized data z ol x : with the exact same scale . 3 Pee ny YS g T T T 2 22 = 3 4 5 = Years old ('), Document(metadata={}, page_content='. 3 Pee ny YS g T T T 2 22 = 3 4 5 = Years old ( normalized ) Min - max normalization VS Z - score normalization Min - max normalization : Guarantees all attributes will have the exact same scale . Does not handle outliers well . Z - score normalization : Handles outliers . Does not produce normalized data with the exact same scale .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in'), Document(metadata={}, page_content='number of categories . Discretization is used in both unsupervised and supervised settings . Discretization [ ie ae or L , CONTINUOUS DISCRETE VALUE VALUE Converting a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  a continuous attribute into an ordinal attribute . A potentially infinite number of values are mapped to a small number of'), Document(metadata={}, page_content='number of values are mapped to a small number of categories . Discretization is used in both unsupervised and supervised settings .  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  -'), Document(metadata={}, page_content=': top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  : Top - down split Histogram analysis : Top - down split Clustering analysis : top - down split or bottom - up merge  - tree analysis : top - down split Correlation analysis : bottom - up merge Note : All the methods can be applied recursively  is selecting a subset of data from a larger dataset to make it more manageable for'), Document(metadata={}, page_content='a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is'), Document(metadata={}, page_content='bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time -'), Document(metadata={}, page_content='of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for'), Document(metadata={}, page_content='a larger dataset to make it more manageable for analysis while maintaining its representativeness . We use sampling because obtaining the entire dataset of interest is : Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . Challenges : Ensuring the sample is representative of the population . Addressing potential bias in the sampling process . Sampling is'), Document(metadata={}, page_content='bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights . Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sample size h and | decision in researc itica ize is acr iate sample si ing an appropria Select analysis . eg ee Pe oe lee # PSs et g. S Shheaal ote o. P gor hae atk A Siee sed FS ? Sample size Selecting an'), Document(metadata={}, page_content='hae atk A Siee sed FS ? Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally'), Document(metadata={}, page_content='draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Random sampling with replacement Sampling methods Random sampling without replacement Simple random sampling Cluster sampling Origian ! data Equal probability of selecting any particular item Sampling without replacement ciel cana Once an object is selected , it is removed from the population Sampling with replacement Aselected object is not removed from the population Stratified sampling'), Document(metadata={}, page_content='removed from the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) Sampling methods Simple random sampling Equal probability of selecting any particular item Sampling without replacement Once an object is selected , it is removed from the population Sampling with replacement A selected object is not removed from the population Stratified sampling Partition ( or'), Document(metadata={}, page_content='the population Stratified sampling Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i .. , approximately the same percentage of the data ) 3- Similarity and  3- Similarity and  3- Similarity and  and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit .'), Document(metadata={}, page_content='Often starts at 0 and varies in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : © Quantifies data object likeness . © Higher values indicate greater similarity . © Typically within the range [ 0 , 1 ] .  : © Quantifies data object differences . * Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . —  : ©'), Document(metadata={}, page_content='at 0 and varies in the upper limit . —  : © Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  : Quantifies data object likeness . Higher values indicate greater similarity . Typically within the range [ 0 , 1 ] .  : Quantifies data object differences . Lower values indicate greater similarity . Often starts at 0 and varies in the upper limit . Proximity : Refers to either'), Document(metadata={}, page_content='in the upper limit . Proximity : Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real -'), Document(metadata={}, page_content='Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : © Non - Negativity : m d(x , y ) 20 for all x and y. m d(x , y ) = 0 if and only if x = y. Symmetry : m d(x , y ) = d(y , x ) for all x and y.  : m d(x , Z ) S$ d(x , y ) + d(y , Z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric'), Document(metadata={}, page_content='that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  t is a metric if it satisfies these properties : Non - Negativity : d(x , y ) ≥ 0 for all x and y. d(x , y ) = 0 if and only if x = y. Symmetry : d(x , y ) = d(y , x ) for all x and y.  : d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. Metrics ensure that distances align with real - world geometric properties Metrics guarantee'), Document(metadata={}, page_content='- world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data'), Document(metadata={}, page_content='and consistency of similarity measures in data analysis . Properties of a  : Oo § ( x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : Oo § ( x , y ) = S(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a  : s(x , y ) = 1 ('), Document(metadata={}, page_content='analysis . Properties of a  : s(x , y ) = 1 ( or maximum similarity ) only if x = y. Note : This property may not always hold , .g . , cosine similarity . Symmetry : s(x , y ) = s(y , x ) for all x and y. Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for'), Document(metadata={}, page_content='all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger similarities . Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects .'), Document(metadata={}, page_content='dissimilarities .  between data objects . Valuable for clustering , recommendation systems , ... Often symmetric , with higher values indicating stronger similarities . EEE Similarity and dissimilarity matrix  between all data objects in a dataset . Useful for clustering and nearest neighbor algorithms . Symmetric , with values reflecting dissimilarities .  between data objects . Valuable for clustering , recommendation systems , … Often symmetric , with higher values indicating stronger'), Document(metadata={}, page_content=', with higher values indicating stronger similarities . Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( applicable to numerical vectors ) : number of'), Document(metadata={}, page_content='( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) d(x , y ) — 7 ? : number of attributes . Uk , ye : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) : number of attributes . , , : : kth attributes for objects x and y , respectively . Standardization'), Document(metadata={}, page_content='objects x and y , respectively . Standardization is necessary , if scales differ . Example :  matrix Example :  matrix 4 - 2 @p ! Ft ot | le | 03 ad | opt | of 2.828 ] 3.162 ] 5.099 ] _ a | pr | agosto aia ] 3.162 22 | ps | 3.62 ] iat io 0 , | ps [ | 5090 ] 3162 ) a ] Example :  matrix  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the'), Document(metadata={}, page_content='and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) a 1 / r a(x , y ) = { Solow — al k=1 Generalization of  . r : parameter n : number of attributes x , and y , are , respectively , the k * \" attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) Generalization of  . r : parameter n : number of attributes xk and yk are ,'), Document(metadata={}, page_content='n : number of attributes xk and yk are , respectively , the kth attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also'), Document(metadata={}, page_content='distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . siaciniaed  ( r = 2 ): The most commonly used distance metric . Measures the straight -'), Document(metadata={}, page_content='used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used'), Document(metadata={}, page_content='bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ('), Document(metadata={}, page_content=': Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths .  vector example : Hamming'), Document(metadata={}, page_content='in grid - like paths .  vector example : Hamming distance counts differing bits . @  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm or L » norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid -'), Document(metadata={}, page_content='. Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm or L∞ norm distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal'), Document(metadata={}, page_content='Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as'), Document(metadata={}, page_content='in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits . &  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r — ~ ): Also called Lmax norm , L~ » or chebyshev distance . Calculates the maximum difference between any component of vectors . Appropriate when movement is'), Document(metadata={}, page_content='of vectors . Appropriate when movement is unrestricted in any direction .  of  ( r = 1 ): Also known as Manhattan , taxicab , or L1 norm distance . Ideal for measuring distances in grid - like paths . Binary vector example : Hamming distance counts differing bits .  ( r = 2 ): The most commonly used distance metric . Measures the straight - line distance in Euclidean space .  ( r → ∞ ): Also called Lmax norm , L∞ or chebyshev distance . Calculates the maximum difference between any component of'), Document(metadata={}, page_content='the maximum difference between any component of vectors . Appropriate when movement is unrestricted in any direction . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors )  ( applicable to numerical vectors ) n A - B » AWB fe n di y/ B ; i=1 i=1'), Document(metadata={}, page_content='vectors ) n A - B » AWB fe n di y/ B ; i=1 i=1 A .B is dot product of the two vectors cos(9 ) It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . y Values are between -1 and 1 : -1 ( completely dissimilar ) © 1 ( perfect similarity ) . Omeans orthogonal ( no similarity ) . A . B is dot product of the two vectors It is cosine of the angle between two vectors Non - sensitive to magnitudes , focusing on orientation . Values are between -1 and 1 :'), Document(metadata={}, page_content='on orientation . Values are between -1 and 1 : -1 ( completely dissimilar ) 1 ( perfect similarity ) . 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors ) Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It'), Document(metadata={}, page_content='in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Linear correlation ( applicable to numerical vectors ) Perfect positive correlation > ( x%i- * ) ( i- ) > ( xi- * ) ? ( i -¥ ) ? Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . paren Omeans'), Document(metadata={}, page_content=') 1 ( perfect correlation ) . paren Omeans orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Measure the linear relationship between two variables . Evaluates how well one variable predicts another one . Values are between -1 and 1 : -1 ( perfect inverse correlation ) 1 ( perfect correlation ) . 0 means orthogonal ( no linear relationship ) . Commonly used in statistical analysis and data'), Document(metadata={}, page_content='. Commonly used in statistical analysis and data exploration . It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Distances and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  and similarity examples Proximity measures for numerical vectors'), Document(metadata={}, page_content='Proximity measures for numerical vectors  correlation Proximity measures for binary vectors  ( SMC )  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the'), Document(metadata={}, page_content='1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( 4 , t+ Son ) or thio * Soo t. A ) Jo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0 Soo = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) f01 = the number of'), Document(metadata={}, page_content=') / ( f01 + f10 + f00 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10'), Document(metadata={}, page_content='of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ OO ” non - zero attributes . tis designed for asymmetric binary attributes . J = Su / ( fo thot fi ) fo , = the number of attributes where x was 0 and y was 1 Ji ) = the number of attributes where x was 1 and y was 0'), Document(metadata={}, page_content='number of attributes where x was 1 and y was 0 Jog = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) f01 = the number of attributes where x was 0 and y was 1 f10 = the number of attributes where x was 1 and y was 0 f00 = the number of attributes where x was'), Document(metadata={}, page_content='was 0 f00 = the number of attributes where x was 0 and y was 0 f11 = the number of attributes where x was 1 and y was 1 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 Jaccard = 0 Example : SMC vs  x= 1000000000 y= 0000001001 SMC = 0.7 = 0 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 f01 = 2 f10 = 1 f00 = 7 f11 = 0 SMC = 0.7 = 0 How to Choose the  ? Choice of the right proximity measure depends on the domain'), Document(metadata={}, page_content='right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right'), Document(metadata={}, page_content='. . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  ° Similarity : Documents are considered similar if they use high number of common words .  in Celsius of  :  © Similarity : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time .  :  : Measures the linear relationship'), Document(metadata={}, page_content='time .  :  : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain  presence  :  : Documents are considered similar if they use high number of common words .  in Celsius of  :  : Two locations are considered similar if their temperatures are similar in magnitude .  of Temperature ( Celsius )  :  : Two time series are considered similar if their \" shape \" is similar , i .. , they vary in the same way over time'), Document(metadata={}, page_content=', i .. , they vary in the same way over time .  :  : Measures the linear relationship between two variables . . Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute : Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute :  : 1 te=—9 % Nominal ue S= ) 9 vey  ( values mapped to integers 0 ton—1 , | s=1-—d where n is the number of values )'), Document(metadata={}, page_content=', | s=1-—d where n is the number of values ) Interval or Ratio = lo s=—-d , s=7q . Q , s= \" d — min_d max - d — min_d s = l1- Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute'), Document(metadata={}, page_content='( EDA ) &  ( EDA ) &  ( EDA ) & 2  ( EDA ) ❑ EDA is a set of statistical and visualization techniques . ❑ Used for seeing what the data can tell us before the preprocessing and modeling : ❑ Understand the data and summarize its keys properties . ❑ Discover noisy data and outliers . ❑ Comprehend the distribution of the data . ❑ Decide which set of data cleaning techniques to be applied . ❑ EDA is cross - classified in two ways : 1 . The method is either non - graphical or graphical . 2 . The'), Document(metadata={}, page_content='is either non - graphical or graphical . 2 . The method is either univariate or multivariate ( usually just bivariate ) .  ( EDA ) L ) EDA is a set of statistical and visualization techniques . L ) Used for seeing what the data can tell us before the preprocessing and modeling : _ ) Understand the data and summarize its keys properties . Discover noisy data and outliers . Comprehend the distribution of the data . Decide which set of data cleaning techniques to be applied . Ovwv . UO LL ) EDA is'), Document(metadata={}, page_content='techniques to be applied . Ovwv . UO LL ) EDA is cross - classified in two ways : 1 . The method is either non - graphical or graphical . 2 . The method is either univariate or multivariate ( usually just bivariate ) . 2  ( EDA ) ❑ EDA is a set of statistical and visualization techniques . ❑ Used for seeing what the data can tell us before the preprocessing and modeling : ❑ Understand the data and summarize its keys properties . ❑ Discover noisy data and outliers . ❑ Comprehend the distribution'), Document(metadata={}, page_content='data and outliers . ❑ Comprehend the distribution of the data . ❑ Decide which set of data cleaning techniques to be applied . ❑ EDA is cross - classified in two ways : 1 . The method is either non - graphical or graphical . 2 . The method is either univariate or multivariate ( usually just bivariate ) .  ( EDA )  ( EDA )  ( EDA ) 4 1 : Statics of Data ❑ Population vs Sample ❑ Measuring the  ❑ Mean , Median , and Mode ❑ Measuring the distribution of data ❑ Variance and Standard deviation ❑'), Document(metadata={}, page_content='of data ❑ Variance and Standard deviation ❑ Analysis of two variables ❑ Covariance and Correlation 1 : Statics of Data QO ~Population vs Sample O Measuring the  L ) Mean , Median , and Mode O Measuring the distribution of data _ } Variance and Standard deviation QO Analysis of two variables L ) Covariance and Correlation 4 1 : Statics of Data ❑ Population vs Sample ❑ Measuring the  ❑ Mean , Median , and Mode ❑ Measuring the distribution of data ❑ Variance and Standard deviation ❑ Analysis of'), Document(metadata={}, page_content='❑ Variance and Standard deviation ❑ Analysis of two variables ❑ Covariance and Correlation 5 Population vs Sample ❑ Population ❑ The entire group that you want to draw conclusions about . ❑ Sample ❑ Subset of the population , used when the population size is too large to analyze ❑ A sample is an unbiased subset that best represents the entire population . Population vs Sample Q Population ( ) The entire group that you want to draw conclusions about . OQ Sample OY = Subset of the population ,'), Document(metadata={}, page_content='about . OQ Sample OY = Subset of the population , used when the population size is too large to analyze ( } = Asample is an unbiased subset that best represents the entire population . 5 Population vs Sample ❑ Population ❑ The entire group that you want to draw conclusions about . ❑ Sample ❑ Subset of the population , used when the population size is too large to analyze ❑ A sample is an unbiased subset that best represents the entire population . 6 Measuring the  : ( 1 ) Mean ❑ Mean ('), Document(metadata={}, page_content='. 6 Measuring the  : ( 1 ) Mean ❑ Mean ( algebraic measure ) ( sample vs. population ): n is sample size and N is population size . ❑ Weighted arithmetic mean : ❑ Trimmed mean : Chopping extreme values ( .g . , olympics gymnastics score computation ) Measuring the  : ( 1 ) Mean Q Mean ( algebraic measure ) ( sample vs. population ): n. N nis sample size and N is population size . x= i x. v WM j = l _ 2 * o. N Q Weighted arithmeticmean : — ~= ! — _ _ _ Q Trimmed mean : Chopping extreme values ('), Document(metadata={}, page_content='_ _ _ Q Trimmed mean : Chopping extreme values ( .g . , olympics gymnastics score computation ) 6 Measuring the  : ( 1 ) Mean ❑ Mean ( algebraic measure ) ( sample vs. population ): n is sample size and N is population size . ❑ Weighted arithmetic mean : ❑ Trimmed mean : Chopping extreme values ( .g . , olympics gymnastics score computation ) 7 Measuring the  : ( 2 ) Median ❑ Median is the middle value in a data set when values are ordered . ❑ How to calculate ❑ Sort data in ascending order . ❑'), Document(metadata={}, page_content=\"to calculate ❑ Sort data in ascending order . ❑ Repeat values according to their frequency . ❑ If there 's an odd number of data points , the median is the middle value . ❑ If there 's an even number of data points , the median is the average of the two middle values . ❑ Why use the Median ❑ Resistant to extreme outliers . ❑ Useful for skewed distributions . Measuring the  : ( 2 ) Median Q Median is the middle value in a data set when values are ordered . a Howto calculate a Sort data in\"), Document(metadata={}, page_content=\"are ordered . a Howto calculate a Sort data in ascending order . a Repeat values according to their frequency . a If there 's an odd number of data points , the median is the middle value . a If there 's an even number of data points , the median is the average of the two middle values . a Why use the Median ag Resistant to extreme outliers . ag Useful for skewed distributions . 7 Measuring the  : ( 2 ) Median ❑ Median is the middle value in a data set when values are ordered . ❑ How to\"), Document(metadata={}, page_content=\"in a data set when values are ordered . ❑ How to calculate ❑ Sort data in ascending order . ❑ Repeat values according to their frequency . ❑ If there 's an odd number of data points , the median is the middle value . ❑ If there 's an even number of data points , the median is the average of the two middle values . ❑ Why use the Median ❑ Resistant to extreme outliers . ❑ Useful for skewed distributions . 8 Example1 : Calculating the median ❑ N = 4 + 5 + 3 +1 = 13 ❑ Median is in the position\"), Document(metadata={}, page_content='N = 4 + 5 + 3 +1 = 13 ❑ Median is in the position ceil(13/2 ) = 7 ❑ The median is the value 1 0 0 0 0 1 1 1 1 1 2 2 2 3 Example1 : Calculating the median 1 2 3 OQ N=4 + 54 + 3 + 1= 13 Q Median is in the position ceil(13/2 ) = 7 Q The median is the value 1 8 Example1 : Calculating the median ❑ N = 4 + 5 + 3 +1 = 13 ❑ Median is in the position ceil(13/2 ) = 7 ❑ The median is the value 1 0 0 0 0 1 1 1 1 1 2 2 2 3 9 Example 2 : Calculating the median ❑ N = 4 + 5 + 3 +1 = 13 ❑ Median is in the'), Document(metadata={}, page_content='median ❑ N = 4 + 5 + 3 +1 = 13 ❑ Median is in the position ceil(13/2 ) = 7 ❑ The median bin is the value 10 - 20 How to find the median value ?  0 5 - 10 4 1 10 - 20 5 2 20 - 50 3 3 50 - 55 1 0 0 0 0 1 1 1 1 1 2 2 2 3 Example 2 : Calculating the median  0 5 - 10 4 1 10 - 20 5 2 20 - 50 3 3 50 - 55 1 0 0 0 0 4 4(41)1 4 2 2 2 OQ N=4 + 5 + 3 +1=13 Q Median is in the position ceil(13/2 ) = 7 Q The median bin is the value 10 - 20 How to find the median value ? 9 Example 2 : Calculating the median ❑'), Document(metadata={}, page_content='value ? 9 Example 2 : Calculating the median ❑ N = 4 + 5 + 3 +1 = 13 ❑ Median is in the position ceil(13/2 ) = 7 ❑ The median bin is the value 10 - 20 How to find the median value ?  0 5 - 10 4 1 10 - 20 5 2 20 - 50 3 3 50 - 55 1 0 0 0 0 1 1 1 1 1 2 2 2 3 10 Median calculation for grouped data ❏ L : the lower class boundary of the median bin . ❏ n : the total number of values . ❏ B : cumulative frequency of the bins before the median bin . ❏ G : the frequency of the median bin . ❏ W : the group'), Document(metadata={}, page_content='the frequency of the median bin . ❏ W : the group width .  0 5 - 10 4 1 10 - 20 5 2 20 - 50 3 3 50 - 55 1 C ] C ] Median calculation for grouped data  = L + “ “ ) to Pp ) L : the lower class boundary of the median bin . n : the total number of values . B : cumulative frequency of the bins before the median bin . G : the frequency of the median bin . W : the group width . w Age 5 - 10 10 - 20 20 - 50 50 - 55 Frequency 4 5 3 10 Median calculation for grouped data ❏ L : the lower class boundary of'), Document(metadata={}, page_content='grouped data ❏ L : the lower class boundary of the median bin . ❏ n : the total number of values . ❏ B : cumulative frequency of the bins before the median bin . ❏ G : the frequency of the median bin . ❏ W : the group width .  0 5 - 10 4 1 10 - 20 5 2 20 - 50 3 3 50 - 55 1 11 Example 2 : Calculating the median for grouped data ❑ The median bin is the value 10 - 20 ❑ L = 10 ❑ n=13 ❑ B = 4 and G = 5 ❑ w = 20 - 10 = 10 Median value = 10 + ( ( 6.5 - 4)/5 ) * 10 = 15  0 5 - 10 4 1 10 - 20 5 2 20 -'), Document(metadata={}, page_content='- 4)/5 ) * 10 = 15  0 5 - 10 4 1 10 - 20 5 2 20 - 50 3 3 50 - 55 1 Example 2 : Calculating the median for grouped data Age 0 5 - 10  = L + ee ) Bxw 1 10 - 20 2 20 - 50 3 50 - 55 Q The median bin is the value 10 - 20 O L=10 O n=13 O B=4 and . G=5 Q wez=20 - 10 = 10 Median value = 10 + ( ( 6.5 - 4)/5 ) * 10 = 15 Frequency 4 5 3 11 Example 2 : Calculating the median for grouped data ❑ The median bin is the value 10 - 20 ❑ L = 10 ❑ n=13 ❑ B = 4 and G = 5 ❑ w = 20 - 10 = 10 Median value = 10 + ( ('), Document(metadata={}, page_content='G = 5 ❑ w = 20 - 10 = 10 Median value = 10 + ( ( 6.5 - 4)/5 ) * 10 = 15  0 5 - 10 4 1 10 - 20 5 2 20 - 50 3 3 50 - 55 1 12 Measuring the  : ( 3 ) Mode ❑ Mode : Value that occurs most frequently in the data ❑ Unimodal ❑ Empirical formula : ❑ Multimodal ❑ Bimodal ( a ) ❑ Trimodal ( b ) ( a ) ( b ) Measuring the  : ( 3 ) Mode Q Mode : Value that occurs most frequently in the data . / oo Q  > Q Empirical formula : mean — mode = 3 x ( mean — median ) Right skewed distribution : Mean is to the right'), Document(metadata={}, page_content='Right skewed distribution : Mean is to the right density.default(x = dist ) 04 Q Multimodal flel - Q Bimodal ( a ) zr Ew Q Trimodal ( b ) S11 BT ) 20 - 10 0 10 20 , 30 4 2 ° 2 3 N= 400000 Bandwidth = 0.1262 ( a ) ( b ) 12 Measuring the  : ( 3 ) Mode ❑ Mode : Value that occurs most frequently in the data ❑ Unimodal ❑ Empirical formula : ❑ Multimodal ❑ Bimodal ( a ) ❑ Trimodal ( b ) ( a ) ( b ) 13 Symmetric vs.  ❑ Median , mean and mode of symmetric , positively and negatively skewed data'), Document(metadata={}, page_content='symmetric , positively and negatively skewed data Symmetric vs.  Q Median , mean and mode of symmetric , positively and negatively skewed data apol - uelpa  13 13 Symmetric vs.  ❑ Median , mean and mode of symmetric , positively and negatively skewed data 14 Properties of  ← — — — — — Represent data dispersion , spread — — — — — → Represent central tendency Properties of  ( aa Represent data dispersion , spread — — — — — — + 99.7 % of the data are within 3 standard deviations of the mean 95 %'), Document(metadata={}, page_content='are within 3 standard deviations of the mean 95 % within 2 standard deviations 68 % within i < — 1 standard deviation u- — 30 u- — 20 u. L- 14 Properties of  ← — — — — — Represent data dispersion , spread — — — — — → Represent central tendency 15  : Variance and  ❑ sample : s , population : σ ❑ Variance : Measure dispersion around the mean ❑ Standard deviation s ( or σ ) is the square root of variance s2 ( or σ2 ) ❑ Measures dispersion around the mean , but in the same units as the l Note : The'), Document(metadata={}, page_content='mean , but in the same units as the l Note : The subtle difference of formulae for sample vs. population n : the size of the sample N : the size of the population  : Variance and  Q sample : s , population : 0 Q Variance : Measure dispersion around the mean 5 l . — \\\\2 | = 2 LX 2 s = ja -¥ ) = — — [ D% ae At j- i = l n 1<¥ 1 om = 2 — py ) = — we i = l i = l Q Standard deviation s ( or ) is the square root of variance s ? ‘ or ” Q Measures dispersion around the mean , but in the same units as the'), Document(metadata={}, page_content='around the mean , but in the same units as the 15  : Variance and  ❑ sample : s , population : σ ❑ Variance : Measure dispersion around the mean ❑ Standard deviation s ( or σ ) is the square root of variance s2 ( or σ2 ) ❑ Measures dispersion around the mean , but in the same units as the l Note : The subtle difference of formulae for sample vs. population n : the size of the sample N : the size of the population 16 Covariance for  ❑ The goal is to understand relationships between two variables'), Document(metadata={}, page_content='to understand relationships between two variables . ❑ Covariance shows how two variables change together . ❑  : both variables move together . ❑  : variables move in opposite directions . ❑  : bo clear pattern in variable movements . Covariance for  d(x — £ ) ( y ; — 9 ) COVz y = QO The goal is to understand relationships between two variables . QO Covariance shows how two variables change together . 4 )  : both variables move together . 4  : variables move in opposite directions . 4 )  : bo'), Document(metadata={}, page_content='variables move in opposite directions . 4 )  : bo clear pattern in variable movements . 16 Covariance for  ❑ The goal is to understand relationships between two variables . ❑ Covariance shows how two variables change together . ❑  : both variables move together . ❑  : variables move in opposite directions . ❑  : bo clear pattern in variable movements . 17 Covariance for  ❑ Understanding relationships between variables . ❑ Covariance shows how two variables change together . Covariance is useful'), Document(metadata={}, page_content='variables change together . Covariance is useful but sensitive to scale , while correlation addresses this issue by standardizing the measurement . Covariance for  d(x — £ ) ( y ; — 9 ) COVz y = QO Understanding relationships between variables . QO Covariance shows how two variables change together . Covariance is useful but sensitive to scale , while correlation addresses this issue by standardizing the measurement . 17 Covariance for  ❑ Understanding relationships between variables . ❑'), Document(metadata={}, page_content='Understanding relationships between variables . ❑ Covariance shows how two variables change together . Covariance is useful but sensitive to scale , while correlation addresses this issue by standardizing the measurement . 18 Covariance for  ❑  - X1 and X2  : X : ( 2 , 3 , 5 , 4 , 6 ) Y : ( 5 , 8 , 10 , 11 , 14 ) ❑ Calculate the Covariance ❑ E(X ) = ( 2 + 3 + 5 + 4 + 6)/ 5 = 20/5 = 4 ❑ E(Y ) = ( 5 + 8 + 10 + 11 + 14 ) /5 = 48/5 = 9.6 ❑ E(XY ) = ( 2×5 + 3×8 + 5×10 + 4×11 + 6×14)/5 = 42.4 ❑ Cov(X'), Document(metadata={}, page_content='2×5 + 3×8 + 5×10 + 4×11 + 6×14)/5 = 42.4 ❑ Cov(X , Y ) = 42.4 - 4 * 9.6 = 4 ❑ Thus , X and Y rise together since Cov(X , Y)>0 Covariance for  cov(X , Y ) = E[XY ] — ELX]E[Y ] 16 14 Q  - X1 and X2  : 2 uy * X : ( 2 , 3,5 , 4,6 ) Y : ( 5 , 8 , 10 , 11 , 14 ) — | t. T Q Calculate the Covariance ° OO E(X)=(2 + 3 + 5 + 4 + 46)/5=20/5=4 0 O ) E(Y)=(5 + 8 + 10 + 11 + 14 ) /5 = 48/5 = 9.6 OQ E(XY ) = ( 2x5 + 3x8 + 5x10 + 4x11 + 6x14)/5 = 42.4 QO Cov(X , ) = 42.4 - 4 * 9.6 = 4 LJ Thus , X and Y rise'), Document(metadata={}, page_content=', ) = 42.4 - 4 * 9.6 = 4 LJ Thus , X and Y rise together since Cov(X , Y)>0 18 Covariance for  ❑  - X1 and X2  : X : ( 2 , 3 , 5 , 4 , 6 ) Y : ( 5 , 8 , 10 , 11 , 14 ) ❑ Calculate the Covariance ❑ E(X ) = ( 2 + 3 + 5 + 4 + 6)/ 5 = 20/5 = 4 ❑ E(Y ) = ( 5 + 8 + 10 + 11 + 14 ) /5 = 48/5 = 9.6 ❑ E(XY ) = ( 2×5 + 3×8 + 5×10 + 4×11 + 6×14)/5 = 42.4 ❑ Cov(X , Y ) = 42.4 - 4 * 9.6 = 4 ❑ Thus , X and Y rise together since Cov(X , Y)>0 19  ❑ The variance and covariance information for the two variables'), Document(metadata={}, page_content='and covariance information for the two variables can be summarized as 2X2 covariance matrix as : ❑ Generalizing it to d dimensions :  Q The variance and covariance information for the two variables can be summarized as 2X2 covariance matrix as : 2 , 2 5 Oi ij ij wt 2 ji Q Generalizing it to d dimensions : ot Oi = * * Cig 021 a3 \" t+ O2d y= E[(K — p)(X - p ) \" ] = Cdl Fd2 * \" * Gq 19  ❑ The variance and covariance information for the two variables can be summarized as 2X2 covariance matrix as :'), Document(metadata={}, page_content='can be summarized as 2X2 covariance matrix as : ❑ Generalizing it to d dimensions : 20 Correlation between  ❑ Correlation between two variables X1 and X2 is the standard covariance , obtained by normalizing the covariance with the standard deviation of each variable ❑ Sample correlation for two attributes X1 and X2 : n is the number of tuples , µ1 and µ2 are the respective means of X1 and X2 , σ1 and σ2 are the respective standard deviation of X1 and X2 ❑ If ρ12 > 0 : A and B are positively'), Document(metadata={}, page_content='X1 and X2 ❑ If ρ12 > 0 : A and B are positively correlated ( X1 ’s values increase as X2 ’s ) ❑ If ρ12 = 0 : independent ( under the same assumption as discussed in co - variance ) ❑ If ρ12 < 0 : negatively correlated O Correlation between  between two variables X , and X , is the standard covariance , obtained by normalizing the covariance with the standard deviation of each variable On =  . = ° O10 , Joo , a n _ Aw . _ “ ~ Sample correlation for two attributes X , and X , : , , = 212 = dtr'), Document(metadata={}, page_content='for two attributes X , and X , : , , = 212 = dtr @in — a ) Cin = fa ) A ~ \\\\2 why in — fi , ) ? ye Xi2 — fiz ) n is the number of tuples , h , and wp , are the respective means of X , and X , , o. O , and O , are the respective standard deviation of X , and X , If p , , > 0 : A and B are positively correlated ( X , ’s values increase as X , ’s ) If p , , = 0 : independent ( under the same assumption as discussed in co - variance ) If p , , < 0 : negatively correlated 20 Correlation between  ❑'), Document(metadata={}, page_content=': negatively correlated 20 Correlation between  ❑ Correlation between two variables X1 and X2 is the standard covariance , obtained by normalizing the covariance with the standard deviation of each variable ❑ Sample correlation for two attributes X1 and X2 : n is the number of tuples , µ1 and µ2 are the respective means of X1 and X2 , σ1 and σ2 are the respective standard deviation of X1 and X2 ❑ If ρ12 > 0 : A and B are positively correlated ( X1 ’s values increase as X2 ’s ) ❑ If ρ12 = 0 :'), Document(metadata={}, page_content='( X1 ’s values increase as X2 ’s ) ❑ If ρ12 = 0 : independent ( under the same assumption as discussed in co - variance ) ❑ If ρ12 < 0 : negatively correlated 21  (  ) ❑ The correlation matrix is a matrix that shows the correlations between each pair of variables in a dataset  (  ) Q The correlation matrix is a matrix that shows the correlations between each pair of variables in a dataset Hours spent studying Exam score IQ score Hours spent sleeping School rating Hours spent  studying Examscore'), Document(metadata={}, page_content='School rating Hours spent  studying Examscore IQscore sleeping rating 21  (  ) ❑ The correlation matrix is a matrix that shows the correlations between each pair of variables in a dataset 22  of  ❑ Correlation coefficient value range : [ – 1 , 1 ] ❑ A set of scatter plots shows sets of points and their correlation coefficients changing from – 1 to 1 -1.00 -0.90 -0.80 -0.70 -0.60 -0.50 -0.40 -0.20 0.10  of  coefficient value range : A set of scatter plots shows sets of points and their'), Document(metadata={}, page_content='of scatter plots shows sets of points and their correlation coefficients changing from — 1 to 1 22  of  ❑ Correlation coefficient value range : [ – 1 , 1 ] ❑ A set of scatter plots shows sets of points and their correlation coefficients changing from – 1 to 1  ( EDA )  ( EDA )  ( EDA ) 24 2-  ❑ Boxplot ❑ Histogram and Bar chart ❑ Quantile plot ❑ Quantile - quantile ( Q - Q ) plot ❑ Scatter plot ❑ Line chart ❑  plot 2-  and Bar chart Quantile plot Quantile - quantile ( Q - Q ) plot Scatter plot'), Document(metadata={}, page_content='Quantile - quantile ( Q - Q ) plot Scatter plot Line chart  plot 24 2-  ❑ Boxplot ❑ Histogram and Bar chart ❑ Quantile plot ❑ Quantile - quantile ( Q - Q ) plot ❑ Scatter plot ❑ Line chart ❑  plot 25 Measuring the Dispersion of Data : Quartiles & Boxplots ❑ Quartiles : Q1 ( 25th percentile ) , Q3 ( 75th percentile ) ❑ Interquartile range : IQR = Q3 – Q1 ❑ Five number summary : min , Q1 , median , Q3 , max ❑ Box plot : Data is represented with a box ❑ Q1 , Q3 , IQR : The ends of the box are at'), Document(metadata={}, page_content='box ❑ Q1 , Q3 , IQR : The ends of the box are at the first and third quartiles , i .. , the height of the box is IQR ❑ Median ( Q2 ) is marked by a line within the box ❑ Outliers : points beyond a specified threshold ( .g . value higher / lower than 1.5 x IQR ) ❑ Whiskers : two lines outside the box extended to Minimum and  the Dispersion of Data : Quartiles & Boxplots Q Q Q Q Q Quartiles : Q , ( 25 percentile ) , Q , ( 75 \" \" percentile ) fe Interquartile range : IQR = Q,-Q , aus — Five number'), Document(metadata={}, page_content='range : IQR = Q,-Q , aus — Five number summary : min , Q , , median , Q , , max _ — Box plot : Data is represented with a box ™ Q , , Q , , 1QR : The ends of the box are at the first and a > third quartiles , i .. , the height of the box is l|QR Median ( Q , ) is marked by a line within the box : two lines outside the box extended to Minimum and  : points beyond a specified threshold ( .g . value higher / lower than 1.5 x IQR ) 25 Measuring the Dispersion of Data : Quartiles & Boxplots ❑'), Document(metadata={}, page_content='the Dispersion of Data : Quartiles & Boxplots ❑ Quartiles : Q1 ( 25th percentile ) , Q3 ( 75th percentile ) ❑ Interquartile range : IQR = Q3 – Q1 ❑ Five number summary : min , Q1 , median , Q3 , max ❑ Box plot : Data is represented with a box ❑ Q1 , Q3 , IQR : The ends of the box are at the first and third quartiles , i .. , the height of the box is IQR ❑ Median ( Q2 ) is marked by a line within the box ❑ Outliers : points beyond a specified threshold ( .g . value higher / lower than 1.5 x IQR'), Document(metadata={}, page_content='( .g . value higher / lower than 1.5 x IQR ) ❑ Whiskers : two lines outside the box extended to Minimum and Maximum 26 Measuring the Dispersion of Data : detect  the Dispersion of Data : detect  ( IQR )  \" Minimum \" \" Maximum \" ( Q1 - 1.5*IQR )  Q3 ( Q3 + 1.5*IQR ) ( 25th Percentile ) ( 75th Percentile ) 26 Measuring the Dispersion of Data : detect Outliers 27 Histograms vs Bar charts ❑ Histogram : Tabulated frequencies represented by bars . ❑ Bar chart : Categorical data with bars proportional'), Document(metadata={}, page_content='chart : Categorical data with bars proportional to the values they represent . 27 Histograms vs Bar charts Q Histogram : Tabulated frequencies represented by bars . Q Bar chart : Categorical data with bars proportional to the values they represent . 10000 > 8000 - 6000 ~ Numeric val ues 4000 + 2000 -  3683  4 507 40 4 304 Frequency count 20 4 40 60 80 100 Numeric ranges 27 Histograms vs Bar charts ❑ Histogram : Tabulated frequencies represented by bars . ❑ Bar chart : Categorical data with bars'), Document(metadata={}, page_content='bars . ❑ Bar chart : Categorical data with bars proportional to the values they represent . 28 Differences between Histograms and Bar charts : ❑ Histograms show distributions of variables , while bar charts compare variables ❑ Histograms plot binned quantitative / categorical data , while bar charts only plot categorical data ❑ Bars can be reordered in bar charts , but not in histograms ❑ In histograms , it is the area of the bar that denotes the value , not the height as in bar charts .  chart'), Document(metadata={}, page_content='value , not the height as in bar charts .  chart Differences between Histograms and Bar charts : 4 Q Histograms show distributions of variables , while bar 45 s = 6c oan oc a Gs a Ss 3 2 2 charts compare variables Q Histograms plot binned quantitative / categorical data , while bar charts only plot categorical data tooo « 30000 « = S«0000S*«000SCSC « 0  of all Times ( till 2012 Olympics ) Q Bars can be reordered in bar charts , but not in histograms ux QO In histograms , it is the area of the'), Document(metadata={}, page_content='ux QO In histograms , it is the area of the bar that denotes the value , not the height as in bar charts . re Gold @ Silver @ Bronze 28 Differences between Histograms and Bar charts : ❑ Histograms show distributions of variables , while bar charts compare variables ❑ Histograms plot binned quantitative / categorical data , while bar charts only plot categorical data ❑ Bars can be reordered in bar charts , but not in histograms ❑ In histograms , it is the area of the bar that denotes the value ,'), Document(metadata={}, page_content='is the area of the bar that denotes the value , not the height as in bar charts .  chart 29  than Boxplots ❑ The two histograms shown in the left may have the same box plot representation ❑ The same values for : min , Q1 , median , Q3 , max ❑ But they have rather different data distributions  than Boxplots A Q The two histograms shown in the left may have the same box plot representation Q The same values for : min , Q1 , median , Q3 , max Q But they have rather different data distributions 29'), Document(metadata={}, page_content='they have rather different data distributions 29  than Boxplots ❑ The two histograms shown in the left may have the same box plot representation ❑ The same values for : min , Q1 , median , Q3 , max ❑ But they have rather different data distributions 30  ❑ Divide data into buckets and store average ( sum ) for each bucket . ❑ Partitioning rules : ❑ Equal - width : equal bucket range ❑ Equal - frequency ( or equal - depth )  Q Divide data into buckets and store average ( sum ) for each bucket .'), Document(metadata={}, page_content='and store average ( sum ) for each bucket . QO Partitioning rules : QO ~—s Equal - width : equal bucket range 4 ) ~=Equal - frequency ( or equal - depth )  frequency binning 5 6 4 4 3 2 0 [ 10 , 21 ] [ 22 , 33 ] ( 34 , 45 ) [ 46 , 55 ] [ 10 , 16 ] [ 17 , 30 ] 131 , 48 ] [ 49 , 55 ] Count of AGE_bins Count of AGE_bins 30  ❑ Divide data into buckets and store average ( sum ) for each bucket . ❑ Partitioning rules : ❑ Equal - width : equal bucket range ❑ Equal - frequency ( or equal - depth ) 31'), Document(metadata={}, page_content=\"❑ Equal - frequency ( or equal - depth ) 31  : Concepts and  ❑ Benefits ❑ Provides a comprehensive view of the attribute 's distribution . ❑ Helps identify both general trends and outliers . ❑ Construction ❑ For a data { X1 , X2 , … , XN } sorted in increasing order . ❑ fi indicates that approximately fi of the data point have values ≤ xi ❑ Purpose : Visualizes all quantile information for a specific attribute  Q Purpose : Visualizes all quantile information for a specific attribute Q Benefits\"), Document(metadata={}, page_content=\"information for a specific attribute Q Benefits Q ) 140 Provides a comprehensive view of the _ attribute 's distribution . 100 2 80 _ ) Helps identify both general trends and = © 69 outliers . Ss 40 20 LL ) Construction 0 0.000 0.250 0.500 0.750 1.000 _ ) Fora data { X1 , X2 , ... , XN } sorted in f - value increasing order . Qf , indicates that approximately f of the data point have values $ x ,  : Concepts and Techniques 31  : Concepts and  ❑ Benefits ❑ Provides a comprehensive view of the\"), Document(metadata={}, page_content=\"❑ Benefits ❑ Provides a comprehensive view of the attribute 's distribution . ❑ Helps identify both general trends and outliers . ❑ Construction ❑ For a data { X1 , X2 , … , XN } sorted in increasing order . ❑ fi indicates that approximately fi of the data point have values ≤ xi ❑ Purpose : Visualizes all quantile information for a specific attribute 32 Quantile - Quantile ( Q - Q ) Plot ❑ Purpose ❑ Assess the distributional similarity between an attribute and either another attribute or a\"), Document(metadata={}, page_content='an attribute and either another attribute or a theoretical distribution . ❑ Interpreting a Q - Q Plot ❑ If the points closely follow a straight line ⇒ The two distributions are similar . ❑ Deviations from the line indicate differences in distribution . Q Purpose 5 Q_s . Assess the distributional similarity between an attribute and 5 3 Q Q Q Quantile - Quantile ( Q - Q ) Plot either another attribute or a theoretical distribution . “ 23 = 2 = -1~«#O 1 2 3 Normal theoretical quantiles'), Document(metadata={}, page_content='= 2 = -1~«#O 1 2 3 Normal theoretical quantiles Interpreting a Q - Q Plot un = 4 If the points closely follow a straight line = The two 5 83 ° ° distributions are similar . | Deviations from the line indicate differences in distribution . 51 } = -2 1 1 A Normal theoretical quantiles 12 32 Quantile - Quantile ( Q - Q ) Plot ❑ Purpose ❑ Assess the distributional similarity between an attribute and either another attribute or a theoretical distribution . ❑ Interpreting a Q - Q Plot ❑ If the points'), Document(metadata={}, page_content='. ❑ Interpreting a Q - Q Plot ❑ If the points closely follow a straight line ⇒ The two distributions are similar . ❑ Deviations from the line indicate differences in distribution . 33 Scatter plot ❑ Provides a first look at bivariate data to see clusters of points , outliers , etc . ❑ Each pair of values is treated as a pair of coordinates and plotted as points in the plane . Items sold Scatter plot Q Provides a first look at bivariate data to see clusters of points , outliers , etc . Q _ Each'), Document(metadata={}, page_content=\"clusters of points , outliers , etc . Q _ Each pair of values is treated as a pair of coordinates and plotted as points in the plane . ° © 1952 80 a oe ~ * © 1982 es 7 © 2007 t. iw of oe 70 r ad ical ° rey . 's  2 g De foe , ° * ‘ 0 ® ° . 8 oe . grote BH , 5 ° 5 a oe , . & eh ? ” 4 £ 50 * s whe : At ee : . a © % oe Se “ ae use ene , eis wo go “ ef © ° oe \\\\€-9 * rg i ve ee . « @ | 30 * ye . Unit price ( $ ) 2 s 1000 2 5 10k 2 5 100k GDP per Capita 33 Scatter plot ❑ Provides a first look at\"), Document(metadata={}, page_content='Capita 33 Scatter plot ❑ Provides a first look at bivariate data to see clusters of points , outliers , etc . ❑ Each pair of values is treated as a pair of coordinates and plotted as points in the plane . 34 Positively and  ❑ The left half fragment is positively correlated ❑ The right half is negative correlated Positively and  QO The left half fragment is positively correlated QQ The right half is negative correlated 34 Positively and  ❑ The left half fragment is positively correlated ❑ The'), Document(metadata={}, page_content=\"left half fragment is positively correlated ❑ The right half is negative correlated 35  le a ° . - * . ai “ te . mek * Givie ° ° vee bd . 35  36 Line chart ❑ A line chart displays information as a series of data points called ' markers ' . ❑ The markers are connected to each other by straight line segments 36 Line chart QO Aline chart displays information as a series of data points called ' markers ' ’ . QO The markers are connected to each other by straight line segments J P Temperature on 1st\"), Document(metadata={}, page_content=\"by straight line segments J P Temperature on 1st September - = - 20 ¢ . . 18 Temperature ° C 3 Sam 10 am Tiam T2pm Tpm 2 pm 3 pm apm Time 36 Line chart ❑ A line chart displays information as a series of data points called ' markers ' . ❑ The markers are connected to each other by straight line segments 37  of  ❑ A parallel coordinate plot maps each row in the data table as a line , or profile . ❑ Each attribute of a row is represented by a point on the line . 37  of  L ) A parallel coordinate\"), Document(metadata={}, page_content='on the line . 37  of  L ) A parallel coordinate plot maps each row in the data table as a line , or profile . L ) Each attribute of a row is represented by a point on the line . ~—~ ~~Versicolor 8 , \\\\ Virginica i — ~Setosa 7 7 : — — Versicolor 6 ~~Virginica x 6 + a a wo Value ( centimeters ) »  ( centimeters ) es WUT 1 + MALL Ld t / a ) J /miti/ = eapal length sepal width petal length petal width sepal width sepal length petal length petal width 37  of  ❑ A parallel coordinate plot maps each'), Document(metadata={}, page_content='37  of  ❑ A parallel coordinate plot maps each row in the data table as a line , or profile . ❑ Each attribute of a row is represented by a point on the line'), Document(metadata={}, page_content='1 &  1 &  1 &  reduction is a technique in data analysis that simplifies the data by reducing the number of variables or dimensions .  should ensure the following :  retains the most crucial information from high - dimensional spaces .  reduction efficiently removes redundant and irrelevant attributes for the analysis . Transformation into a lower - dimensional space it transforms data into a lower - dimensional space , making it more manageable for analysis and visualization .  reduction is a'), Document(metadata={}, page_content='for analysis and visualization .  reduction is a technique in data analysis that simplifies the data by reducing the number of variables or dimensions .  should ensure the following :  retains the most crucial information from high - dimensional spaces .  © Dimensionality reduction efficiently removes redundant and irrelevant attributes for the analysis . Transformation into a lower - dimensional space it transforms data into a lower - dimensional space , making it more manageable for analysis'), Document(metadata={}, page_content='space , making it more manageable for analysis and visualization .  reduction is a technique in data analysis that simplifies the data by reducing the number of variables or dimensions .  should ensure the following :  retains the most crucial information from high - dimensional spaces .  reduction efficiently removes redundant and irrelevant attributes for the analysis . Transformation into a lower - dimensional space it transforms data into a lower - dimensional space , making it more'), Document(metadata={}, page_content='into a lower - dimensional space , making it more manageable for analysis and visualization . Motivations for  model  and  and  and  quality enhancement Removal of  for  model  and  ©  and  and  quality enhancement Removal of  for  model  and  and  and  quality enhancement Removal of  of sphere VS Volume of cube -1 x 1 1 dimension ( D= 1 ) 2 dimensions ( D= 2 ) 3 dimensions ( D = 3 ) Volume of sphere VS Volume of cube  of  of sphere VS Volume of cube of  dimensionality ( d ) grows , the'), Document(metadata={}, page_content='of cube of  dimensionality ( d ) grows , the hypersphere \\'s volume becomes negligible when compared to the hypercube .  of  4 ! 2 = — — — — — _ . _ > 0asd—- ow . Vaiypercube d2*\"\\'T(d/2 ) As dimensionality ( d ) grows , the hypersphere \\'s volume becomes negligible when compared to the hypercube . Volume of the hypersphere i ] 3 4 5 6 if Number of dimensions  of  dimensionality ( d ) grows , the hypersphere \\'s volume becomes negligible when compared to the hypercube .  of  : when points are'), Document(metadata={}, page_content='to the hypercube .  of  : when points are uniformly generated within a high - dimensional hypercube , most points are much farther from the center than expected .  of  : when points are uniformly generated within a high - dimensional hypercube , most points are much farther from the center than expected .  of  : when points are uniformly generated within a high - dimensional hypercube , most points are much farther from the center than expected .  of  high dimensions , traditional distance'), Document(metadata={}, page_content='.  of  high dimensions , traditional distance metrics like Euclidean distance lose their effectiveness . Points become dispersed , posing challenges for distance - based analysis . The curse of dimensionality impacts diverse data analysis tasks ( nearest - neighbor algorithms , classification , and clustering in high - dimensional spaces … ) .  dimensions , traditional distance metrics like Euclidean distance lose their effectiveness . © Points become dispersed , posing challenges for distance'), Document(metadata={}, page_content='become dispersed , posing challenges for distance - based analysis . The curse of dimensionality impacts diverse data analysis tasks ( nearest - neighbor algorithms , classification , and clustering in high - dimensional spaces ... ) .  high dimensions , traditional distance metrics like Euclidean distance lose their effectiveness . Points become dispersed , posing challenges for distance - based analysis . The curse of dimensionality impacts diverse data analysis tasks ( nearest - neighbor'), Document(metadata={}, page_content='diverse data analysis tasks ( nearest - neighbor algorithms , classification , and clustering in high - dimensional spaces … ) . Taxonomy of  extraction : transforms the original attributes into new ones . Linear methods ( PCA ): Transform the original attributes into a new set of linear attributes . Non - linear methods ( t - SNE ): Transform the original attributes into a new set of non - linear attributes . Feature selection : selects a subset of original attributes . Filter methods : Select'), Document(metadata={}, page_content='of original attributes . Filter methods : Select attributes based on their statistical properties . Wrapper methods : Use the performance of a machine learning model to evaluate the goodness of selected attributes . Taxonomy of  extraction : transforms the original attributes into new ones . Linear methods ( PCA ): Transform the original attributes into a new set of linear attributes . Non - linear methods ( t - SNE ): Transform the original attributes into a new set of non - linear attributes'), Document(metadata={}, page_content='into a new set of non - linear attributes . Feature selection : selects a subset of original attributes . Filter methods : Select attributes based on their statistical properties . Wrapper methods : Use the performance of a machine learning model to evaluate the goodness of selected attributes . Taxonomy of  extraction : transforms the original attributes into new ones . Linear methods ( PCA ): Transform the original attributes into a new set of linear attributes . Non - linear methods ( t -'), Document(metadata={}, page_content='of linear attributes . Non - linear methods ( t - SNE ): Transform the original attributes into a new set of non - linear attributes . Feature selection : selects a subset of original attributes . Filter methods : Select attributes based on their statistical properties . Wrapper methods : Use the performance of a machine learning model to evaluate the goodness of selected attributes .  ( PCA ) What is PCA ? Why PCA ? How to perform PCA ? Applications of PCA  ( PCA ) [ ) [ ) [ ) [ ) What is PCA'), Document(metadata={}, page_content='of PCA  ( PCA ) [ ) [ ) [ ) [ ) What is PCA ? Why PCA ? How to perform PCA ? Applications of PCA  ( PCA ) What is PCA ? Why PCA ? How to perform PCA ? Applications of PCA Tracking motion of a ball on an oscillating spring at regular intervals . 3 cameras record ( x , y ) position from different angles Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] Do we need all six dimensions to study the motion of the spring ? Motivation example : oscillating spring spring at'), Document(metadata={}, page_content='Motivation example : oscillating spring spring at regular intervals . Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] Do we need all six dimensions to study the motion of the spring ? Tracking motion of a ball on an oscillating 3 cameras record ( x , y ) position from different angles Motivation example : oscillating spring camera B Li | | camera C camera A camera A camera B camera C Tracking motion of a ball on an oscillating spring at regular intervals . 3 cameras'), Document(metadata={}, page_content='spring at regular intervals . 3 cameras record ( x , y ) position from different angles Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] Do we need all six dimensions to study the motion of the spring ? Motivation example : oscillating spring Tracking motion of a ball on an oscillating spring at regular intervals . 3 cameras record ( x , y ) position from different angles Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] Physically , we know'), Document(metadata={}, page_content='x. B , y. B , x. C , y. C ] Physically , we know the underlying motion is 1D , along the spring . Motivation example : oscillating spring spring at regular intervals . Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] Physically , we know the underlying motion is 1D , along the spring . Tracking motion of a ball on an oscillating 3 cameras record ( x , y ) position from different angles Motivation example : oscillating spring camera B Li | | camera C camera A camera A'), Document(metadata={}, page_content='spring camera B Li | | camera C camera A camera A camera B camera C Tracking motion of a ball on an oscillating spring at regular intervals . 3 cameras record ( x , y ) position from different angles Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] Physically , we know the underlying motion is 1D , along the spring . Motivation example : oscillating spring Tracking motion of a ball on an oscillating spring at regular intervals . 3 cameras record ( x , y ) position from'), Document(metadata={}, page_content='. 3 cameras record ( x , y ) position from different angles Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] How do we extract this underlying 1D dynamic hidden in 6D recordings ? Motivation example : oscillating spring spring at regular intervals . Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] How do we extract this underlying 1D dynamic hidden in 6D recordings ? Tracking motion of a ball on an oscillating 3 cameras record ( x , y )'), Document(metadata={}, page_content='ball on an oscillating 3 cameras record ( x , y ) position from different angles Motivation example : oscillating spring camera B Li | | camera C camera A camera A camera B camera C Tracking motion of a ball on an oscillating spring at regular intervals . 3 cameras record ( x , y ) position from different angles Each sample is 6D vector : X = [ x. A , y. A , x. B , y. B , x. C , y. C ] How do we extract this underlying 1D dynamic hidden in 6D recordings ? Motivation example : oscillating spring'), Document(metadata={}, page_content='? Motivation example : oscillating spring Correlations is common in  ( to be finished ) Correlation is a common characteristic of real - world datasets , often reflecting complex relationships .  and height correlation in individuals . Spatial pixel correlations in images . Study hours and test score relationships .  can indicate redundancy in the data . Reducing redundancy can enhance the performance of data mining algorithms . Recognizing and addressing correlations is essential for effective'), Document(metadata={}, page_content='correlations is essential for effective data analysis and machine learning applications . Correlations is common in  ( to be finished ) Correlation is acommon characteristic of real - world datasets , often reflecting complex relationships .  and height correlation in individuals . Spatial pixel correlations in images . Study hours and test score relationships .  can indicate redundancy in the data . Reducing redundancy can enhance the performance of data mining algorithms . Recognizing and'), Document(metadata={}, page_content='of data mining algorithms . Recognizing and addressing correlations is essential for effective data analysis and machine learning applications . Correlations is common in  ( to be finished ) Correlation is a common characteristic of real - world datasets , often reflecting complex relationships .  and height correlation in individuals . Spatial pixel correlations in images . Study hours and test score relationships .  can indicate redundancy in the data . Reducing redundancy can enhance the'), Document(metadata={}, page_content=\"in the data . Reducing redundancy can enhance the performance of data mining algorithms . Recognizing and addressing correlations is essential for effective data analysis and machine learning applications . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first\"), Document(metadata={}, page_content=\"principal components , are formed . The first principal component ( PC1 ) captures the most variation . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle Attribute 2 PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Attribute 1 What is  ( PCA ) ? PCA\"), Document(metadata={}, page_content=\"variation . Attribute 1 What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's\"), Document(metadata={}, page_content=\"that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle Attribute 2 PCA rotates the axis to align with the direction of maximum data variability .\"), Document(metadata={}, page_content=\"with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Attribute 1 What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal\"), Document(metadata={}, page_content=\"components , are formed . The first principal component ( PC1 ) captures the most variation . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Using PC1 , we reduce the data 's dimensionality while\"), Document(metadata={}, page_content=\"PC1 , we reduce the data 's dimensionality while retaining most of its variation . What is  ( PCA ) ? PCA 's goal PC1 Transform the original data into new variables that follow the variation in the data . PCA ’ . Attribute 2 Using PC1 , we reduce the data 's dimensionality while retaining most of its variation .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Attribute 1 What is  ( PCA ) ? PCA 's goal\"), Document(metadata={}, page_content=\". Attribute 1 What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Using PC1 , we reduce the data 's dimensionality while retaining most of its variation . What is  ( PCA ) ? PCA 's goal Transform the\"), Document(metadata={}, page_content=\". What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Using PC1 , we reduce the data 's dimensionality while retaining most of its variation . What is  ( PCA ) ? PCA 's goal PC1 Transform the original\"), Document(metadata={}, page_content=\"( PCA ) ? PCA 's goal PC1 Transform the original data into new variables that follow the variation in the data . PCA ’ | Using PC1 , we reduce the data 's Attribute 2 dimensionality while retaining most of its variation .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Attribute 1 What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's\"), Document(metadata={}, page_content=\"that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Using PC1 , we reduce the data 's dimensionality while retaining most of its variation . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA\"), Document(metadata={}, page_content=\"the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Using PC1 , we reduce the data 's dimensionality while retaining most of its variation . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA ’ Using PC1 , we reduce the\"), Document(metadata={}, page_content=\"in the data . PCA ’ Using PC1 , we reduce the data 's dimensionality while retaining most of its variation .  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . What is  ( PCA ) ? PCA 's goal Transform the original data into new variables that follow the variation in the data . PCA 's principle PCA rotates the axis to align with the direction of maximum data variability .  rotation , new axes , called principal\"), Document(metadata={}, page_content=\".  rotation , new axes , called principal components , are formed . The first principal component ( PC1 ) captures the most variation . Using PC1 , we reduce the data 's dimensionality while retaining most of its variation . What is  ( PCA ) ? PCA identifies the directions , called principal components , along which the data varies the most . First principal component captures the most variance within the data . Subsequent components are orthogonal ( uncorrelated ) to the preceding ones .\"), Document(metadata={}, page_content=\"( uncorrelated ) to the preceding ones . Retaining only the most important components reduces the data 's dimensionality . PCA is widely used in Data compression , Visualization , Noise reduction , … What is  ( PCA ) ? PCA identifies the directions , called principal components , along which the data varies the most . © First principal component captures the most variance within the data . Subsequent components are orthogonal ( uncorrelated ) to the preceding ones . x2 Retaining only the most\"), Document(metadata={}, page_content=\"the preceding ones . x2 Retaining only the most important components reduces the data 's dimensionality . . PCA is widely used in Data compression , Visualization , Noise reduction , ... What is  ( PCA ) ? PCA identifies the directions , called principal components , along which the data varies the most . First principal component captures the most variance within the data . Subsequent components are orthogonal ( uncorrelated ) to the preceding ones . Retaining only the most important\"), Document(metadata={}, page_content=\"ones . Retaining only the most important components reduces the data 's dimensionality . PCA is widely used in Data compression , Visualization , Noise reduction , … Objectives of PCA - A  formulation ( 1933 ) Express the maximum variation within the first component . Subsequent principal components express the remaining variation . Minimizing error formulation ( 1901 ) Minimize the sum of projection errors on the first principal component . Successive components should also minimize projection\"), Document(metadata={}, page_content='components should also minimize projection errors . Objectives of PCA - A  formulation Minimizing error formulation ( Hotelling 1933 ) ( 1901 ) Express the maximum variation within the Minimize the sum of projection errors on the first component . first principal component . Subsequent principal components express the Successive components should also minimize remaining variation . projection errors . Objectives of PCA - A  formulation ( 1933 ) Express the maximum variation within the first'), Document(metadata={}, page_content=') Express the maximum variation within the first component . Subsequent principal components express the remaining variation . Minimizing error formulation ( 1901 ) Minimize the sum of projection errors on the first principal component . Successive components should also minimize projection errors . Objectives of PCA - A  D1 is the remaining variance expressed in the component . D2 is the projection error or the lost variance . D3 is the original fixed variance independent to the component .'), Document(metadata={}, page_content='fixed variance independent to the component . Maximizing the remaining variance ( D1 ) is equivalent to minimizing the projection error ( D2 ) Objectives of PCA - A  datapoint 2 2 2 initial — remaining + lost variance variance variance 2 2 all= |Imel/\"+|]a,-melf this is maximize minimize constant this this D1 is the remaining variance expressed in the component . D2 is the projection error or the lost variance . D3 is the original fixed variance independent to the component . Maximizing the'), Document(metadata={}, page_content='independent to the component . Maximizing the remaining variance ( D1 ) is equivalent to minimizing the projection error ( D2 ) Objectives of PCA - A  D1 is the remaining variance expressed in the component . D2 is the projection error or the lost variance . D3 is the original fixed variance independent to the component . Maximizing the remaining variance ( D1 ) is equivalent to minimizing the projection error ( D2 ) PCA  1 :  the mean from each attribute ( column ) to center the data . Scale'), Document(metadata={}, page_content='attribute ( column ) to center the data . Scale attributes if their scales differ . Step 2 :  the covariance matrix , denoted as sigma . Step 3 : Eigenvalues and  the eigenvalues and eigenvectors of the covariance matrix to find principal components . Step 4 :  k < m eigenvectors as principal components . Step 5 :  data by projecting it onto the k principal components PCA  1 :  the mean from each attribute ( column ) to center the data . ° Scale attributes if their scales differ . Step 2 :  the'), Document(metadata={}, page_content='attributes if their scales differ . Step 2 :  the covariance matrix , denoted as sigma . Step 3 : Eigenvalues and  the eigenvalues and eigenvectors of the covariance matrix to find principal components . Step 4 :  k < m eigenvectors as principal components . Step 5 :  data by projecting it onto the k principal components PCA  1 :  the mean from each attribute ( column ) to center the data . Scale attributes if their scales differ . Step 2 :  the covariance matrix , denoted as sigma . Step 3 :'), Document(metadata={}, page_content='covariance matrix , denoted as sigma . Step 3 : Eigenvalues and  the eigenvalues and eigenvectors of the covariance matrix to find principal components . Step 4 :  k < m eigenvectors as principal components . Step 5 :  data by projecting it onto the k principal components The input of the algorithm n : is the number of objects ( rows ) . m : is the number of attributes ( columns ) . The input of the algorithm x. X = ee Xo ... x. | n : is the number of objects ( rows ) . m : is the number of'), Document(metadata={}, page_content='number of objects ( rows ) . m : is the number of attributes ( columns ) . The input of the algorithm n : is the number of objects ( rows ) . m : is the number of attributes ( columns ) .  calculation Subtracting the mean from each attribute centers the data around zero , which makes the covariance formula easier to calculate :  calculation sm — Hie — F Cov(X , Y ) = — — Subtracting the mean from each attribute centers the data around zero , which makes the covariance formula easier to'), Document(metadata={}, page_content=', which makes the covariance formula easier to calculate : nm par . 2 4  calculation Subtracting the mean from each attribute centers the data around zero , which makes the covariance formula easier to calculate :  calculation ( Matrix notation ) This produce a symmetric covariance matrix :  calculation ( Matrix notation ) Var ( Xi ) Cov ( X1,X2 ) ... Cov(X1 , Xm ) a 2 Cov ( X9,X1 ) Var(X2 ) ~~ ... Cov ( X2 , Xm ) Cov ( Xm , X1 ) Cov ( Xm , X2 ) ... Var ( Xm ) This produce a symmetric'), Document(metadata={}, page_content=\"Xm , X2 ) ... Var ( Xm ) This produce a symmetric covariance matrix : i gay Oe zee n—1 x. T x  calculation ( Matrix notation ) This produce a symmetric covariance matrix : Eigenvalues and  in PCA A fundamental concept in PCA for simplifying the variance matrix . Diagonal matrix ( D ) consists of eigenvalues ( λ ) . Eigenvectors are arranged in the matrix P enable data projection and dimensionality reduction . Spectral theorem ensures eigenvectors ' orthogonality . Singular value decomposition (\"), Document(metadata={}, page_content=\"' orthogonality . Singular value decomposition ( SVD ) can be used to do this decomposition . Eigenvalues and  in PCA A fundamental concept in PCA for simplifying the variance matrix . = = PDP ’ Diagonal matrix ( D ) consists of eigenvalues ( A ) . FEigenvectors are arranged in the matrix P enable data projection and dimensionality reduction . Spectral theorem ensures eigenvectors ’ orthogonality . Singular value decomposition ( SVD ) can be used to do this decomposition . > = USsv !\"), Document(metadata={}, page_content=\"can be used to do this decomposition . > = USsv ! Eigenvalues and  in PCA A fundamental concept in PCA for simplifying the variance matrix . Diagonal matrix ( D ) consists of eigenvalues ( λ ) . Eigenvectors are arranged in the matrix P enable data projection and dimensionality reduction . Spectral theorem ensures eigenvectors ' orthogonality . Singular value decomposition ( SVD ) can be used to do this decomposition . Choosing k ( number of principal components ) Select k < n eigenvectors to\"), Document(metadata={}, page_content='components ) Select k < n eigenvectors to make them components Select k components Choosing k ( number of principal components ) Select k < n eigenvectors to make them components | | | | | | U = Jul ) ul 22 . ul ) 20 . ulm ) || | | | | Select k components Ee R ™ xm Choosing k ( number of principal components ) Select k < n eigenvectors to make them components Select k components Choosing k ( number of principal components ) Average squared projection error Total variation in the data Choose k'), Document(metadata={}, page_content='error Total variation in the data Choose k to be smallest value so that “ 99 % of variance is retained ” Choosing k ( number of principal components ) Average squared projection error n ) i = l Choose k to be smallest value so that Total variation in the data 2 + . < ( ) .01 “ 99 % of variance is retained ” Choosing k ( number of principal components ) Average squared projection error Total variation in the data Choose k to be smallest value so that “ 99 % of variance is retained ” Choosing k ('), Document(metadata={}, page_content='“ 99 % of variance is retained ” Choosing k ( number of principal components ) Average squared projection error Total variation in the data Choose k to be smallest value so that “ 99 % of variance is retained ” Choosing k ( number of principal components ) Average squared projection error 1 n , 2 i = l + > ( el Choose k to be smallest value so that Total variation in the data ye ial \" s 9 99 1- S — < 0.01 5 Sy a iat ii ~ a a “ 99 % of variance is retained ” Choosing k ( number of principal'), Document(metadata={}, page_content='is retained ” Choosing k ( number of principal components ) Average squared projection error Total variation in the data Choose k to be smallest value so that “ 99 % of variance is retained ” Choosing k ( number of principal components )  aids in deciding how many principal components to retain(k ) . How to choose k Elbow point in the plot indicates a significant drop in eigenvalues , suggesting an optimal number of components . Choosing k ( number of principal components )  aids in deciding'), Document(metadata={}, page_content='of principal components )  aids in deciding how many principal components to retain(k ) . How to choose k point in the plot indicates a significant drop in eigenvalues , suggesting an optimal number of  components . Choosing k ( number of principal components )  aids in deciding how many principal components to retain(k ) . How to choose k point in the plot indicates a significant drop in eigenvalues , suggesting an optimal number of components .  in  T : Coordinates matrix of data points in'), Document(metadata={}, page_content='.  in  T : Coordinates matrix of data points in PCA components . X : Data matrix ( variables as columns , data points as rows ) . P : Eigenvectors matrix of the covariance matrix . For a dataset with m variables , to compute coordinates in the first k PCA components :  in  T= XU TT : Coordinates matrix of data points in PCA components . X : Data matrix ( variables as columns , data points as rows ) . P : Eigenvectors matrix of the covariance matrix . For a dataset with m variables , to compute'), Document(metadata={}, page_content='. For a dataset with m variables , to compute coordinates in the first k PCA components : T , = XU!:,1 : k  in  T : Coordinates matrix of data points in PCA components . X : Data matrix ( variables as columns , data points as rows ) . P : Eigenvectors matrix of the covariance matrix . For a dataset with m variables , to compute coordinates in the first k PCA components :  in  T : Coordinates matrix of data points in PCA components . X : Data matrix ( variables as columns , data points as rows )'), Document(metadata={}, page_content='( variables as columns , data points as rows ) . P : Eigenvectors matrix of the covariance matrix . For a dataset with m variables , to compute coordinates in the first k PCA components :  in  T= XU TT : Coordinates matrix of data points in PCA components . X : Data matrix ( variables as columns , data points as rows ) . P : Eigenvectors matrix of the covariance matrix . For a dataset with m variables , to compute coordinates in the first k PCA components : T. = XU ,  in  T : Coordinates matrix'), Document(metadata={}, page_content=': T. = XU ,  in  T : Coordinates matrix of data points in PCA components . X : Data matrix ( variables as columns , data points as rows ) . P : Eigenvectors matrix of the covariance matrix . For a dataset with m variables , to compute coordinates in the first k PCA components : Reconstruction of data The matrix is approximation of the centred data . To reconstruct the original data , the mean should be added to each columns . Reconstruction of data T X approx = Tk UX The matrix is approximation'), Document(metadata={}, page_content='T X approx = Tk UX The matrix is approximation of the centred data . To reconstruct the original data , the mean should be added to each columns . Reconstruction of data The matrix is approximation of the centred data . To reconstruct the original data , the mean should be added to each columns . PCA applications : Denoising with PCA  noise in data or images .  is represented as a matrix . PCA identifies principal components . High - variance components retain signal ; low - variance components'), Document(metadata={}, page_content='retain signal ; low - variance components capture noise .  : Image denoising . Enhancing data quality in various fields . PCA applications : Denoising with PCA  noise in data or images .  is represented as a matrix . PCA identifies principal components . High - variance components retain signal : low - variance components capture noise .  : Image denoising . Enhancing data quality in various fields . PCA applications : Denoising with PCA  noise in data or images .  is represented as a matrix .'), Document(metadata={}, page_content='in data or images .  is represented as a matrix . PCA identifies principal components . High - variance components retain signal ; low - variance components capture noise .  : Image denoising . Enhancing data quality in various fields . PCA applications :  with PCA  and authenticate individuals from facial images .  : Eigenvalue and eigenvector decomposition of face images . Reduced - dimension representation . Compare face features for recognition .  systems . Biometric authentication . Video'), Document(metadata={}, page_content='.  systems . Biometric authentication . Video surveillance . PCA applications :  with PCA  and authenticate individuals from facial images .  inne Eigenfaces : Eigenvalue and eigenvector ‘ 0 decomposition of face images . 2 — Reduced - dimension representation . » : — Compare face features for recognition . 7  : 2 Security systems . Biometric authentication . Video surveillance . PCA applications :  with PCA  and authenticate individuals from facial images .  : Eigenvalue and eigenvector'), Document(metadata={}, page_content='facial images .  : Eigenvalue and eigenvector decomposition of face images . Reduced - dimension representation . Compare face features for recognition .  systems . Biometric authentication . Video surveillance . PCA applications :  with PCA  high - dimensional data into a lower - dimensional space for visualization .  PCA reduces data dimensions while preserving data variance . Data points are projected onto a lower - dimensional ( 2D,3D ) subspace .  data analysis . Visualizing'), Document(metadata={}, page_content='( 2D,3D ) subspace .  data analysis . Visualizing multidimensional data in two or three dimensions . Cluster analysis . PCA applications :  with PCA  high - dimensional data into a lower - dimensional space for visualization . original data space  rks PCA a space PCAreduces data dimensions while preserving data variance . Data points are projected onto a Gene 3 lower - dimensional ( 2D,3D ) subspace .  ee Exploratory data analysis . Visualizing multidimensional data in two or three dimensions .'), Document(metadata={}, page_content='data in two or three dimensions . Cluster analysis . PCA applications :  with PCA  high - dimensional data into a lower - dimensional space for visualization .  PCA reduces data dimensions while preserving data variance . Data points are projected onto a lower - dimensional ( 2D,3D ) subspace .  data analysis . Visualizing multidimensional data in two or three dimensions . Cluster analysis . PCA applications :  PCA is used in various fields and applications .  : Identifying unusual data'), Document(metadata={}, page_content='and applications .  : Identifying unusual data patterns .  : Reducing data dimensionality .  : Extracting user preferences .  : Identifying gene expression patterns . PCA applications :  PCAis used in various fields and applications .  : Identifying unusual data patterns .  : Reducing data dimensionality .  : Extracting user preferences .  : Identifying gene expression patterns . o. O Original points ® ) New points @ Reconstruction PCA applications :  PCA is used in various fields and'), Document(metadata={}, page_content='applications :  PCA is used in various fields and applications .  : Identifying unusual data patterns .  : Reducing data dimensionality .  : Extracting user preferences .  : Identifying gene expression patterns'), Document(metadata={}, page_content='2 :  & 1  2 :  &  2 :  & 1  is  ( FSS ) ? Why doing  ? Taxonomy of  methods  methods for FSS Wrapper methods for FSS Comparison of  methods 2  is  ( FSS ) ? Why doing  ? Taxonomy of  methods  a Filter methods for FSS m Wrapper methods for FSS Comparison of  methods  is  ( FSS ) ? Why doing  ? Taxonomy of  methods  methods for FSS Wrapper methods for FSS Comparison of  methods 2 What is  ( FSS ) ?  ( FSS ) is the process of selecting a subset of features from a given feature set . The goal is to'), Document(metadata={}, page_content='from a given feature set . The goal is to optimize an objective function for the chosen features . FSS can be seen as a form of feature extraction . However , the methods and goals differ significantly . Feature extraction : Transforming the existing features into a lower dimensional space . Feature selection : Selecting a subset of the existing features without a transformation . 3 What is  ( FSS ) ?  ( FSS ) is the process of selecting a subset of features from a given feature set . © ---X'), Document(metadata={}, page_content='of features from a given feature set . © ---X The goal is to optimize an objective function for the chosen features . © — @ FSS can be seen as a form of feature extraction . © ---X However , the methods and goals differ significantly . Feature extraction : © XK Transforming the existing features into a lower dimensional space . Feature selection : © — > © Selecting a subset of the existing features without a transformation . What is  ( FSS ) ?  ( FSS ) is the process of selecting a subset of'), Document(metadata={}, page_content='( FSS ) is the process of selecting a subset of features from a given feature set . The goal is to optimize an objective function for the chosen features . FSS can be seen as a form of feature extraction . However , the methods and goals differ significantly . Feature extraction : Transforming the existing features into a lower dimensional space . Feature selection : Selecting a subset of the existing features without a transformation . 3 What is  ( FSS ) ?  : Objective : Find a subset : that'), Document(metadata={}, page_content='is  ( FSS ) ?  : Objective : Find a subset : that optimizes the objective function Optimization : 4 What is  ( FSS ) ?  : X = { a;|i=1, ... ,n } Objective : Find a subset : Yn — 12 % , , % , , ... - , 24 , p27 < P that optimizes the objective function J ( Ym ) Optimization : Y = arg max J(Y , , ) M , tm @--x eo - ® @ @-x @--x eo—- ® What is  ( FSS ) ?  : Objective : Find a subset : that optimizes the objective function Optimization : 4 Why doing  ? Why not stick to feature extraction , as it'), Document(metadata={}, page_content='? Why not stick to feature extraction , as it essentially accomplishes the same task of dimensionality reduction ? 5 Why doing  ? Why not stick to feature extraction , as it essentially accomplishes the same task of dimensionality reduction ? Why doing  ? Why not stick to feature extraction , as it essentially accomplishes the same task of dimensionality reduction ? 5 Why doing  ? Features may be expensive to obtain Evaluate a large number of features ( sensors ) in the test bed and select a'), Document(metadata={}, page_content='features ( sensors ) in the test bed and select a few for the final implementation You may want to extract meaningful rules from your classifier When you transform or project , the measurement units ( length , weight , etc . ) of your features are lost . Features may not be numeric A typical situation in the data mining . Fewer features means fewer parameters for pattern recognition Improved the generalization capabilities and avoid overfitting . Reduced complexity and run - time . 6 Why doing'), Document(metadata={}, page_content='Reduced complexity and run - time . 6 Why doing  ? Features may be expensive to obtain Evaluate a large number of features ( Sensors ) in the test bed and select a few for the final implementation You may want to extract meaningful rules from your classifier When you transform or project , the measurement units ( length , weight , etc . ) of your features are lost . Features may not be numeric Atypical situation in the data mining . Fewer features means fewer parameters for pattern recognition'), Document(metadata={}, page_content='means fewer parameters for pattern recognition Improved the generalization capabilities and avoid overfitting . Reduced complexity and run - time . Why doing  ? Features may be expensive to obtain Evaluate a large number of features ( sensors ) in the test bed and select a few for the final implementation You may want to extract meaningful rules from your classifier When you transform or project , the measurement units ( length , weight , etc . ) of your features are lost . Features may not be'), Document(metadata={}, page_content=') of your features are lost . Features may not be numeric A typical situation in the data mining . Fewer features means fewer parameters for pattern recognition Improved the generalization capabilities and avoid overfitting . Reduced complexity and run - time . 6  selection is challenging ? Number of features = 3 { A1 , A2 , A3 } How many features subsets ? { A1 } , { A2 } , { A3 } { A1 , A2 } , { A1 , A3 } , { A2 , A3 } { A1 , A2 , A3 } 7  selection is challenging ? Number of features = 3 © {'), Document(metadata={}, page_content='is challenging ? Number of features = 3 © { A1 , A2 , A3 } @ How many features subsets ? O { A1 } , { A2 } , { A3 } © { A1 , A2 } , { A1 , A3 } , { A2 , A3 } © { A1 , A2 , A3 }  selection is challenging ? Number of features = 3 { A1 , A2 , A3 } How many features subsets ? { A1 } , { A2 } , { A3 } { A1 , A2 } , { A1 , A3 } , { A2 , A3 } { A1 , A2 , A3 } 7  selection is challenging ? Number of features = n { A1 , A2 , A3 , … , An } How many features subsets ? { A1 } , { A2 } , { A3 } … .. { A1 ,'), Document(metadata={}, page_content='subsets ? { A1 } , { A2 } , { A3 } … .. { A1 , A2 } , { A1 , A3 } , { A2 , A3 } .... { A1 , A2 , A3 } … . … … For n=100 1 267 650 600 228 229 401 496 703 205 376 8  selection is challenging ? Number of features = n © { A1 , A2 , A3 , ... , An } @ How many features subsets ? O { A1 } , { A2 } , { A3 } ..... © { A1 , A2 } , { A1 , A3 } , { A2 , A3 } .... O { A1 , A2 , A3 } .... @ For n=100 1 267 650 600 228 229 401 496 703 205 376  selection is challenging ? Number of features = n { A1 , A2 , A3'), Document(metadata={}, page_content='? Number of features = n { A1 , A2 , A3 , … , An } How many features subsets ? { A1 } , { A2 } , { A3 } … .. { A1 , A2 } , { A1 , A3 } , { A2 , A3 } .... { A1 , A2 , A3 } … . … … For n=100 1 267 650 600 228 229 401 496 703 205 376 8 Taxonomy of  methods 9 Taxonomy of  methods Feature selection methods 1  information  - squared  gain test selection elimination algorithms Taxonomy of  methods 9  feature selection typically uses statistical measures to evaluate features . The most common'), Document(metadata={}, page_content='measures to evaluate features . The most common statistical measures include : Variance : Features with high variance are more likely to be informative and useful for prediction . Correlation : Features that are highly correlated with each other are likely to contain redundant information . Mutual information : Measures the amount of information that two features share . Features with high mutual information are more likely to be predictive of each other . 10  feature selection typically uses'), Document(metadata={}, page_content='each other . 10  feature selection typically uses statistical measures to evaluate features . The most common statistical measures include : Variance : Features with high variance are more likely to be informative and useful for prediction . Correlation : Features that are highly correlated with each other are likely to contain redundant information . Mutual information : Measures the amount of information that two features share . Features with high mutual information are more likely to be'), Document(metadata={}, page_content='high mutual information are more likely to be predictive of each other .  feature selection typically uses statistical measures to evaluate features . The most common statistical measures include : Variance : Features with high variance are more likely to be informative and useful for prediction . Correlation : Features that are highly correlated with each other are likely to contain redundant information . Mutual information : Measures the amount of information that two features share .'), Document(metadata={}, page_content='amount of information that two features share . Features with high mutual information are more likely to be predictive of each other . 10  descriptors with a high percentage of identical values for all objects . Why ? Absence of information in the constant descriptors 11  descriptors with a high percentage of identical values for all objects . Feature 1 | Feature 2 | Feature 3 ) Feature 4 Object 1 4 8 Object 2 17 34 Object 3 4 8 Object 4 15 30 Object 5 8 16 ° Why ? Object 6 15 30 Absence of'), Document(metadata={}, page_content='Object 5 8 16 ° Why ? Object 6 15 30 Absence of information in the constant descriptors  descriptors with a high percentage of identical values for all objects . Why ? Absence of information in the constant descriptors 11  correlated filters Why ? Redundant information in descriptors ( Attribute 2 = 2 * Attribute 1 ) 12  correlated filters Object 1 Feature 3 | Feature 4 Object 2 Object 3 Object 4 Object 5 Redundant information in descriptors ( Attribute 2 = 2 * Attribute 1 ) Object 6 Why ?'), Document(metadata={}, page_content='( Attribute 2 = 2 * Attribute 1 ) Object 6 Why ?  correlated filters Why ? Redundant information in descriptors ( Attribute 2 = 2 * Attribute 1 ) 12 Taxonomy of  methods 13 Taxonomy of  methods Feature selection methods r 1 Mutual information Chi - squared  of  methods 13 Filter methods for FSS Filter methods evaluate features independently and select the most important features based on statistical measures , such as : correlation with the target variable , information gain , or chi - squared'), Document(metadata={}, page_content='variable , information gain , or chi - squared test . How to use filter methods for FSS 1 . Choose a filter metric ( correlation , information gain , or chi - squared test ) . 2 . Evaluate each feature using the chosen filter metric and rank them accordingly . 3 . Select the features with the highest filter metric scores . 14 Filter methods for FSS Filter methods evaluate features independently and select the most important features based on statistical measures , Such as : correlation with the'), Document(metadata={}, page_content='measures , Such as : correlation with the target variable , information gain , or chi - squared test . How to use filter methods for FSS 1 . Choose a filter metric ( correlation , information gain , or chi - squared test ) . 2 . Evaluate each feature using the chosen filter metric and rank them accordingly . 3 . Select the features with the highest filter metric scores . Filter methods for FSS Filter methods evaluate features independently and select the most important features based on'), Document(metadata={}, page_content='and select the most important features based on statistical measures , such as : correlation with the target variable , information gain , or chi - squared test . How to use filter methods for FSS 1 . Choose a filter metric ( correlation , information gain , or chi - squared test ) . 2 . Evaluate each feature using the chosen filter metric and rank them accordingly . 3 . Select the features with the highest filter metric scores . 14 Filter methods :  is a widely used filter method in feature'), Document(metadata={}, page_content=\":  is a widely used filter method in feature selection . It assesses the strength and direction of a relationship between two variables . It measures how well a feature relates to the target variable . High correlation with the target : Features strongly correlated with the target variable are often good candidates for predictive models Low inter - feature correlation : It 's also essential to consider inter - feature correlations to avoid redundancy in the feature set The correlation does not\"), Document(metadata={}, page_content='in the feature set The correlation does not consider the non - linear correlation with the target and the interaction between features 15 Filter methods :  is a widely used filter method in feature selection . It assesses the strength and direction of a relationship between two variables . It measures how well a feature relates to the target variable . High correlation with the target : Features strongly correlated with the target variable are often good candidates for predictive models Low'), Document(metadata={}, page_content=\"often good candidates for predictive models Low inter - feature correlation : © It 's also essential to consider inter - feature correlations to avoid redundancy in the feature set The correlation does not consider the non - linear correlation with the target and the interaction between features Filter methods :  is a widely used filter method in feature selection . It assesses the strength and direction of a relationship between two variables . It measures how well a feature relates to the\"), Document(metadata={}, page_content=\". It measures how well a feature relates to the target variable . High correlation with the target : Features strongly correlated with the target variable are often good candidates for predictive models Low inter - feature correlation : It 's also essential to consider inter - feature correlations to avoid redundancy in the feature set The correlation does not consider the non - linear correlation with the target and the interaction between features 15  colors for positive . Cool colors . for\"), Document(metadata={}, page_content=\"15  colors for positive . Cool colors . for negative . Neutral for no correlation . Helps in visualizing the correlation between attributes 16  eee , Wee ' 0.78 ors 0.82 | 0.87 | 0.88 Warm colors for positive . ie - 0.25 Cool colors . for negative . 0 on ; ae elo 8 Neutral for no correlation . “ i f us sf Helps in visualizing the correlation between attributes  colors for positive . Cool colors . for negative . Neutral for no correlation . Helps in visualizing the correlation between attributes\"), Document(metadata={}, page_content='in visualizing the correlation between attributes 16 Filter methods :  gain is a measure of how much information is gained about the target categorical variable when a dataset is split on a given categorical feature . It is calculated as the difference between the entropy of the target variable before and after the split . What is entropy ? 17 Filter methods :  gain is a measure of how much information is gained about the target categorical variable when a dataset is split on a given'), Document(metadata={}, page_content='variable when a dataset is split on a given categorical feature . It is calculated as the difference between the entropy of the target variable before and after the split . What is entropy ? Filter methods :  gain is a measure of how much information is gained about the target categorical variable when a dataset is split on a given categorical feature . It is calculated as the difference between the entropy of the target variable before and after the split . What is entropy ? 17 What is entropy'), Document(metadata={}, page_content='the split . What is entropy ? 17 What is entropy ? It measures the level of uncertainty , randomness , or disorder in a system or dataset . In the context of data , entropy quantifies the impurity of a set . 18 What is entropy ? It measures the level of uncertainty , randomness , or disorder in a system or dataset . Inthe context of data , entropy quantifies the impurity of a set .  is entropy ? It measures the level of uncertainty , randomness , or disorder in a system or dataset . In the'), Document(metadata={}, page_content=', or disorder in a system or dataset . In the context of data , entropy quantifies the impurity of a set . 18 What is entropy ? It measures the level of uncertainty , randomness , or disorder in a system or dataset . In the context of data , entropy quantifies the impurity of a set . S= { x1,x2 … .xn } , which is the set of elements . 19 What is entropy ? It measures the level of uncertainty , randomness , or disorder in a system or dataset . In the context of data , entropy quantifies the'), Document(metadata={}, page_content='. In the context of data , entropy quantifies the impurity of a set . H(S ) = — ) _ p(ai ) log , p(2 :) i=1 S= { x1,x2 .... xn } , which is the set of elements . What is entropy ? It measures the level of uncertainty , randomness , or disorder in a system or dataset . In the context of data , entropy quantifies the impurity of a set . S= { x1,x2 … .xn } , which is the set of elements . 19 What is entropy ? p(red ) = 6/12=0.5 p(green ) = 6/12=0.5 H(S ) = -(0.5*log(0.5)+0.5*log(0.5 ) ) = -(-0.5 -'), Document(metadata={}, page_content='H(S ) = -(0.5*log(0.5)+0.5*log(0.5 ) ) = -(-0.5 - 0.5 ) H(S ) = 1 20 What is entropy ? H(S ) = — ) ~ p(ai ) logy p(z :) a= ) p(red ) = 6/12=0.5 p(green ) = 6/12=0.5 H(S ) = -(0.5*log(0.5)+0.5*log(0.5 ) ) = -(-0.5 - 0.5 ) H(S ) = 1  is entropy ? p(red ) = 6/12=0.5 p(green ) = 6/12=0.5 H(S ) = -(0.5*log(0.5)+0.5*log(0.5 ) ) = -(-0.5 - 0.5 ) H(S ) = 1 20 What is entropy ? p(red ) = 3/12 p(green ) = 9/12 H(S ) = -((3/12)*log(3/12)+9/12*log(9/12 ) ) H(S ) = 0.811278 21 What is entropy ? H(S ) = — )'), Document(metadata={}, page_content='H(S ) = 0.811278 21 What is entropy ? H(S ) = — ) ~ p(ai ) logy p(z :) a= ) p(red ) = 3/12 p(green ) = 9/12 H(S ) = -((3/12)*log(3/12)+9/12*log(9/12 ) ) H(S ) = 0.811278  is entropy ? p(red ) = 3/12 p(green ) = 9/12 H(S ) = -((3/12)*log(3/12)+9/12*log(9/12 ) ) H(S ) = 0.811278 21 What is entropy ? p(red ) = 0 p(green ) = 1 H(S ) = -(0 + 1*log(1 ) ) H(S ) = 0 22 What is entropy ? H(S ) = — ) ~ p(ai ) logy p(z :) a= ) p(red ) = 0 p(green ) = 1 H(S ) = -(0 + 1*log(1 ) ) H(S ) = 0  is entropy ?'), Document(metadata={}, page_content='H(S ) = -(0 + 1*log(1 ) ) H(S ) = 0  is entropy ? p(red ) = 0 p(green ) = 1 H(S ) = -(0 + 1*log(1 ) ) H(S ) = 0 22 Filter methods :  gain is a measure of how much information is gained about the target categorical variable when a dataset is split on a given categorical feature . It is calculated as the difference between the entropy of the target variable before and after the split . 23 Filter methods :  gain is a measure of how much information is gained about the target categorical variable'), Document(metadata={}, page_content='is gained about the target categorical variable when a dataset is split on a given categorical feature . It is calculated as the difference between the entropy of the target variable before and after the split . IG(X ) = H(S ) — } _ p(ai)H(S| = 2 ) 23 Filter methods :  gain is a measure of how much information is gained about the target categorical variable when a dataset is split on a given categorical feature . It is calculated as the difference between the entropy of the target variable'), Document(metadata={}, page_content=\"between the entropy of the target variable before and after the split . 23 Example : Information gain for Age IG(Age ) = H(Root node ) - p(Age = Recent ) * H(Recent ) - p(Age = Old ) * H(Old ) = 1 - ( ¾)*0.918 – ( ¼)*0 IG(Age ) = 0.3115 24 Example : Information gain for  node ¥ 2instances  x 2instances Entropy = 1  4 , old  n't buy ¥ 2instances ¥ oinstances x 1instances x 1instances  n’t buy Entropy = 0.918 Entropy = 0 IG(Age ) = H(Root node ) - p(Age = Recent ) * H(Recent ) - p(Age = Old ) *\"), Document(metadata={}, page_content='- p(Age = Recent ) * H(Recent ) - p(Age = Old ) * H(Old ) = 1 - ( % 4)*0.918 — ( % ) * 0 IG(Age ) = 0.3115 34 Example : Information gain for Age IG(Age ) = H(Root node ) - p(Age = Recent ) * H(Recent ) - p(Age = Old ) * H(Old ) = 1 - ( ¾)*0.918 – ( ¼)*0 IG(Age ) = 0.3115 24 Example : Information gain for  IG( ) = 1 - 0.5 * 0 - 0.5 * 0 = 1 - 0 IG(Age ) = 1 ( Perfect predictor ) 25 Pure set Pure set Example : Information gain for  n’t buy  n’t buy IG( ) = 1 - 0.5 * 0 - 0.5 * 0 = 1 - 0 IG(Age ) ='), Document(metadata={}, page_content='IG( ) = 1 - 0.5 * 0 - 0.5 * 0 = 1 - 0 IG(Age ) = 1 ( Perfect predictor ) Pure set Pure set 25 Example : Information gain for  IG( ) = 1 - 0.5 * 0 - 0.5 * 0 = 1 - 0 IG(Age ) = 1 ( Perfect predictor ) 25 Pure set Pure set Filter methods for FSS  methods are computationally efficient and scalable to large datasets and high dimensionality as no machine learning model is trained .  of any machine learning algorithm , making them unbiased toward any particular algorithm .  not always identify the'), Document(metadata={}, page_content='particular algorithm .  not always identify the best feature subset for a specific machine learning algorithm because it does not consider the model performance .  not be able to handle complex relationships and interactions between features . 26 Filter methods for FSS  methods are computationally efficient and scalable to large datasets and high dimensionality as no machine learning model is trained .  of any machine learning algorithm , making them unbiased toward any particular algorithm .'), Document(metadata={}, page_content='them unbiased toward any particular algorithm .  not always identify the best feature subset for a specific machine learning algorithm because it does not consider the model performance .  not be able to handle complex relationships and interactions between features . 26 Filter methods for FSS  methods are computationally efficient and scalable to large datasets and high dimensionality as no machine learning model is trained .  of any machine learning algorithm , making them unbiased toward any'), Document(metadata={}, page_content='algorithm , making them unbiased toward any particular algorithm .  not always identify the best feature subset for a specific machine learning algorithm because it does not consider the model performance .  not be able to handle complex relationships and interactions between features . 26 How to use filter methods for FSS  as an initial step Filter methods serve as an excellent initial step in the feature selection process . Particularly valuable when dealing with large datasets .'), Document(metadata={}, page_content=\"valuable when dealing with large datasets . Hybridization is key It 's advisable to combine filter methods with other feature selection techniques . Wrapper methods and embedded methods complement filter methods effectively . 27 How to use filter methods for FSS  as an initial step Filter methods serve as an excellent initial step in the feature selection process . Particularly valuable when dealing with large datasets . Hybridization is key It 's advisable to combine filter methods with other\"), Document(metadata={}, page_content=\"'s advisable to combine filter methods with other feature selection techniques . Wrapper methods and embedded methods complement filter methods effectively . 27 How to use filter methods for FSS  as an initial step Filter methods serve as an excellent initial step in the feature selection process . Particularly valuable when dealing with large datasets . Hybridization is key It 's advisable to combine filter methods with other feature selection techniques . Wrapper methods and embedded methods\"), Document(metadata={}, page_content='techniques . Wrapper methods and embedded methods complement filter methods effectively . 27 Taxonomy of  methods 28 Taxonomy of  methods Feature selection methods r Mutual wl  - squared  gain test selection elimination Genetic algorithms 28 Taxonomy of  methods 28 Wrapper methods for FSS Wrapper methods are feature selection methods that evaluate features by using a machine learning model as a black box . The model is trained on different subsets of features , and the subset that produces the'), Document(metadata={}, page_content='of features , and the subset that produces the best performance is selected . How to use filter methods for FSS  : The search algorithm generates various feature subsets for evaluation  : Each subset is input to the model and its performance is evaluated via model performance  : A selection criterion ( .g . accuracy ) guides the choice of the best - performing feature subset Iteration : The process is repeated with different subsets until an optimal set is found 29 Wrapper methods for FSS'), Document(metadata={}, page_content='optimal set is found 29 Wrapper methods for FSS Wrapper methods are feature selection methods that evaluate features by using a machine learning model as a black box . The model is trained on different subsets of features , and the subset that produces the best performance is selected . How to use filter methods for FSS  : The search algorithm generates various feature subsets for evaluation  : Each subset is input to the model and its performance is evaluated via model performance  : A'), Document(metadata={}, page_content='is evaluated via model performance  : A selection criterion ( .g . accuracy ) guides the choice of the best - performing feature subset Iteration : The process is repeated with different subsets until an optimal set is found 29 Wrapper methods for FSS Wrapper methods are feature selection methods that evaluate features by using a machine learning model as a black box . The model is trained on different subsets of features , and the subset that produces the best performance is selected . How to'), Document(metadata={}, page_content='the best performance is selected . How to use filter methods for FSS  : The search algorithm generates various feature subsets for evaluation  : Each subset is input to the model and its performance is evaluated via model performance  : A selection criterion ( .g . accuracy ) guides the choice of the best - performing feature subset Iteration : The process is repeated with different subsets until an optimal set is found 29 30  feature set feature set SR \" wae ] | SA Classification model'), Document(metadata={}, page_content='feature set SR \" wae ] | SA Classification model Classification model 30 Search strategy and objective function  :  : A strategy to select candidate subsets Exhaustive evaluation of all subsets is impossible . A search strategy navigates the vast feature subset space efficiently  : Function to evaluate these candidates Evaluates candidate feature subsets Quantitative measure of the \" goodness \" or quality of a subset Feedback from the objective function to guide the search strategy 31 Search'), Document(metadata={}, page_content='function to guide the search strategy 31 Search strategy and objective function Complete feature set  :  : A strategy to select candidate subsets Exhaustive evaluation of all subsets is impossible .  “ Goodness ” Asearch strategy navigates the vast feature subset space efficiently Feature subset Objective function Final feature subset PR algorithm  : Function to evaluate these candidates Evaluates candidate feature subsets Quantitative measure of the \" goodness \" or quality of a subset Feedback'), Document(metadata={}, page_content='the \" goodness \" or quality of a subset Feedback from the objective function to guide the search strategy 31 Search strategy and objective function  :  : A strategy to select candidate subsets Exhaustive evaluation of all subsets is impossible . A search strategy navigates the vast feature subset space efficiently  : Function to evaluate these candidates Evaluates candidate feature subsets Quantitative measure of the \" goodness \" or quality of a subset Feedback from the objective function to'), Document(metadata={}, page_content='a subset Feedback from the objective function to guide the search strategy 31 Search strategy and objective functions Forward selection Add features step by step , choosing the one that improve the model the most at each iteration . Backward elimination Remove features step by step , choosing the one that does not improve the model the at each iteration .  ( GA ) An optimization technique , inspired by natural selection , used to evolve feature subsets for improved model performance . 32 Search'), Document(metadata={}, page_content='for improved model performance . 32 Search strategy and objective functions Forward selection Add features step by step , choosing the one that improve the model the most at each iteration . Backward elimination Remove features step by step , choosing the one that does not improve the model the at each iteration .  ( GA ) An optimization technique , inspired by natural selection , used to evolve feature subsets for improved model performance . 32 Search strategy and objective functions Forward'), Document(metadata={}, page_content='Search strategy and objective functions Forward selection Add features step by step , choosing the one that improve the model the most at each iteration . Backward elimination Remove features step by step , choosing the one that does not improve the model the at each iteration .  ( GA ) An optimization technique , inspired by natural selection , used to evolve feature subsets for improved model performance . 32  1 . Start with 0 features a. Reach the number of features . b. Performance'), Document(metadata={}, page_content='a. Reach the number of features . b. Performance threshold . c. Elbow methods . 33  a. b. c. Reach the number of features . Performance threshold . Elbow methods . Start with a model with no variables  LLL Add the most significant variable Model with 1 variable ( ee Keep adding the most significant variable until reaching the stopping rule or running out of variables Model with 2 variables eeu LD  1 . Start with 0 features a. Reach the number of features . b. Performance threshold . c. Elbow'), Document(metadata={}, page_content='of features . b. Performance threshold . c. Elbow methods . 33 Backward elimination a. Reach the number of features . b. Performance threshold . c. Elbow methods . 34 Backward elimination a. b. c. Reach the number of features . Performance threshold . Elbow methods . Start with a model that contains all the variables  1 a Remove the least significant variable Model with 4 variables e008 . 0 Keep removing the least significant variable until reaching the stopping rule or running out of variables'), Document(metadata={}, page_content='the stopping rule or running out of variables Mode with 3 variables ao Backward elimination a. Reach the number of features . b. Performance threshold . c. Elbow methods . 34  ( GA ) for  is a genetic algorithm ? 35  ( GA ) for  - Sy O What is a genetic algorithm ? i Mutation 4 Recombination 35  ( GA ) for  is a genetic algorithm ? 35  ( GA ) for  two solutions Produce good children  change  the best Survival of fittest 36 N.B. In feature selection and GA , we can encode the selected feature'), Document(metadata={}, page_content=\"and GA , we can encode the selected feature using 1 's and the discarded one by 0 's  ( GA ) for  eee a Crossover = . . chromosomes j Combine two solutions encoding { 1100101010 } — > | Produce good children 1011101110 \\\\ 0011011001 > . 1100110001 a. S Mutation roe O Random change \\\\ ezozte7 ] o. O Exploration evaluation ( offspring Selection new | 110010 1110 ) L 101110 1010 ) { 00110 01001 ] i y decoding fitness N.B. In feature selection and GA , we can encode the selected feature using 1 's\"), Document(metadata={}, page_content=\", we can encode the selected feature using 1 's and the discarded one by 0 's population Select the best Survival of fittest roulette wheel _ _ computation 36  ( GA ) for  two solutions Produce good children  change  the best Survival of fittest 36 N.B. In feature selection and GA , we can encode the selected feature using 1 's and the discarded one by 0 's Wrapper methods for FSS  detect the ideal feature subset for a given machine learning algorithm .  are effective in handling relationships\"), Document(metadata={}, page_content='.  are effective in handling relationships between features by evaluating a subset , not a feature .  to be computationally demanding , particularly with large datasets .  may exhibit bias towards the machine learning algorithm used for feature evaluation . 37 Wrapper methods for FSS  detect the ideal feature subset for a given machine learning algorithm .  are effective in handling relationships between features by evaluating a subset , not a feature .  to be computationally demanding ,'), Document(metadata={}, page_content='a feature .  to be computationally demanding , particularly with large datasets .  may exhibit bias towards the machine learning algorithm used for feature evaluation . 37 Wrapper methods for FSS  detect the ideal feature subset for a given machine learning algorithm .  are effective in handling relationships between features by evaluating a subset , not a feature .  to be computationally demanding , particularly with large datasets .  may exhibit bias towards the machine learning algorithm'), Document(metadata={}, page_content=\"bias towards the machine learning algorithm used for feature evaluation . 37 Comparison of  methods Unsupervised methods Advantages : Simple , fast , no need for labels Disadvantages : May miss complex relationships , does n't optimize for prediction Filter methods Advantages : Fast , scalable , algorithm independent Disadvantages : May miss complex relationships , suboptimal for specific algorithms Wrapper methods Advantages : Finds optimal subset for algorithm , handles complex relationships\"), Document(metadata={}, page_content='for algorithm , handles complex relationships Disadvantages : Slow , algorithm bias Forward selection Advantages : Starts with no features , adds features incrementally Disadvantages : suboptimal and does not consider feature interaction . Backward elimination Advantages : Starts with all features , removes features incrementally Disadvantages : Can be slow for high dimensional data Genetic algorithms Advantages : Searches and optimizes feature sets through evolution Disadvantages :'), Document(metadata={}, page_content=\"feature sets through evolution Disadvantages : Computationally intensive but thorough search 38 Comparison of  methods Unsupervised methods Advantages : Simple , fast , no need for labels Disadvantages : May miss complex relationships , does n't optimize for prediction Filter methods Advantages : Fast , scalable , algorithm independent Disadvantages : May miss complex relationships , suboptimal for specific algorithms Wrapper methods Advantages : Finds optimal subset for algorithm , handles\"), Document(metadata={}, page_content=': Finds optimal subset for algorithm , handles complex relationships Disadvantages : Slow , algorithm bias Forward selection Advantages : Starts with no features , adds features incrementally Disadvantages : suboptimal and does not consider feature interaction . Backward elimination Advantages : Starts with all features , removes features incrementally Disadvantages : Can be slow for high dimensional data Genetic algorithms Advantages : Searches and optimizes feature sets through evolution'), Document(metadata={}, page_content=\"and optimizes feature sets through evolution Disadvantages : Computationally intensive but thorough search 38 Comparison of  methods Unsupervised methods Advantages : Simple , fast , no need for labels Disadvantages : May miss complex relationships , does n't optimize for prediction Filter methods Advantages : Fast , scalable , algorithm independent Disadvantages : May miss complex relationships , suboptimal for specific algorithms Wrapper methods Advantages : Finds optimal subset for algorithm\"), Document(metadata={}, page_content='Advantages : Finds optimal subset for algorithm , handles complex relationships Disadvantages : Slow , algorithm bias Forward selection Advantages : Starts with no features , adds features incrementally Disadvantages : suboptimal and does not consider feature interaction . Backward elimination Advantages : Starts with all features , removes features incrementally Disadvantages : Can be slow for high dimensional data Genetic algorithms Advantages : Searches and optimizes feature sets through'), Document(metadata={}, page_content=': Searches and optimizes feature sets through evolution Disadvantages : Computationally intensive but thorough search 38 Comparison of  methods 39 Comparison of  methods Unsupervised methods Filter methods Wrapper methods Forward selection Backward elimination Genetic algorithms Simple &  need for labels  algorithm independent Finds optimal subset for algorithm Handles complex relationships Fast and simple Can be optimal for high dimensional data Thorough search May miss complex relationships'), Document(metadata={}, page_content=\"Thorough search May miss complex relationships Does n't optimize for prediction May miss complex relationships Suboptimal for specific algorithms Slow algorithm bias Can be suboptimal for high- dimensional data Can be slow Computationally intensive 39 Comparison of  methods 39 How to choose a  method ? Size of dataset Filter methods work well for large datasets Wrapper methods work well for small datasets Computational budget Filter methods are fast Wrapper methods are slow Need for\"), Document(metadata={}, page_content='are fast Wrapper methods are slow Need for interpretability Filter methods allow inspection of feature importance Wrapper methods act as black box Domain knowledge Unsupervised methods do not use labels Supervised methods use domain labels Algorithm fit Wrapper methods tailor to specific ML algorithm Filter methods are independent of any algorithm Feature interactions Wrapper methods handle feature interactions Filter methods assess features independently 40 How to choose a  method ? Size of'), Document(metadata={}, page_content='40 How to choose a  method ? Size of dataset Domain knowledge Filter methods work well for large datasets Unsupervised methods do not use labels Wrapper methods work well for small datasets © Supervised methods use domain labels Computational budget Algorithm fit Filter methods are fast Wrapper methods tailor to specific ML algorithm Wrapper methods are slow Filter methods are independent of any algorithm Need for interpretability Feature interactions Filter methods allow inspection of feature'), Document(metadata={}, page_content='Filter methods allow inspection of feature importance Wrapper methods handle feature interactions Wrapper methods act as black box Filter methods assess features independently 40 How to choose a  method ? Size of dataset Filter methods work well for large datasets Wrapper methods work well for small datasets Computational budget Filter methods are fast Wrapper methods are slow Need for interpretability Filter methods allow inspection of feature importance Wrapper methods act as black box Domain'), Document(metadata={}, page_content='Wrapper methods act as black box Domain knowledge Unsupervised methods do not use labels Supervised methods use domain labels Algorithm fit Wrapper methods tailor to specific ML algorithm Filter methods are independent of any algorithm Feature interactions Wrapper methods handle feature interactions Filter methods assess features independently 40  goal is to select an optimal subset of features Main approaches : Unsupervised : Evaluate features independently using statistical measures  : Fast'), Document(metadata={}, page_content='independently using statistical measures  : Fast evaluation of features using metrics like correlation  : Use model performance to search feature subsets , slow but thorough Considerations : Dataset size and dimensionality Computational budget and time constraints Need for interpretability vs performance Domain knowledge of features Relationships between features No one - size - fits - all method - choose based on goals , data , and constraints Hybrid approaches combine strengths of different'), Document(metadata={}, page_content='Hybrid approaches combine strengths of different techniques Feature selection critical step for ML pipelines , balances model performance and efficiency 41  goal is to select an optimal subset of features Main approaches : Unsupervised : Evaluate features independently using statistical measures  : Fast evaluation of features using metrics like correlation  : Use model performance to search feature subsets , slow but thorough Considerations : Dataset size and dimensionality Computational budget'), Document(metadata={}, page_content='size and dimensionality Computational budget and time constraints Need for interpretability vs performance Domain knowledge of features Relationships between features 0o . O000 0 No one - size - fits - all method - choose based on goals , data , and constraints Hybrid approaches combine strengths of different techniques Feature selection critical step for ML pipelines , balances model performance and efficieney  goal is to select an optimal subset of features Main approaches : Unsupervised :'), Document(metadata={}, page_content='of features Main approaches : Unsupervised : Evaluate features independently using statistical measures  : Fast evaluation of features using metrics like correlation  : Use model performance to search feature subsets , slow but thorough Considerations : Dataset size and dimensionality Computational budget and time constraints Need for interpretability vs performance Domain knowledge of features Relationships between features No one - size - fits - all method - choose based on goals , data , and'), Document(metadata={}, page_content='- all method - choose based on goals , data , and constraints Hybrid approaches combine strengths of different techniques Feature selection critical step for ML pipelines , balances model performance and efficiency 41')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from importnb import Notebook\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "with Notebook():\n",
    "    import Preprocessing_data.DM as dm_module  \n",
    "\n",
    "DM_module = dm_module.split_docs_DM\n",
    "\n",
    "print(dm_module.split_docs_DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 12:12:08,486 - INFO - Use pytorch device_name: cpu\n",
      "2025-04-05 12:12:08,488 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-04-05 12:12:25,598 - INFO - Loading faiss with AVX2 support.\n",
      "2025-04-05 12:12:26,090 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-04-05 12:12:26,174 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = FAISS.from_documents(DM, embedding_model)\n",
    "\n",
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_20160\\907170105.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "2025-04-05 15:23:32,918 - INFO - Use pytorch device_name: cpu\n",
      "2025-04-05 15:23:32,920 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-04-05 15:24:16,818 - INFO - Loading faiss with AVX2 support.\n",
      "2025-04-05 15:24:16,936 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-04-05 15:24:16,952 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = FAISS.from_documents(DM_module, embedder)\n",
    "\n",
    "vector_store.save_local(\"vector_db/DM_vector_store\")\n",
    "print(\"Vector database updated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
