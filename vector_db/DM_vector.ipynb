{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../Preprocessing_data\"))  \n",
    "\n",
    "import pdf_loader_pre  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists: True\n",
      "Contents: ['data_mining.py', 'DM.ipynb', 'pdf_loader_pre.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the path exists\n",
    "path_to_check = os.path.abspath(os.path.join('..', 'Preprocessing_data'))\n",
    "print(f\"Path exists: {os.path.exists(path_to_check)}\")\n",
    "print(f\"Contents: {os.listdir(path_to_check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[': Part 1 \\n &  : Part 1 &  : Part 1 & Outline 1- Data 1- Data 1-  is Data ? \\n ● \\n Data set : collection of objects and their attributes \\n ● \\n Attribute : property or characteristic of an object \\n ○ \\n Examples : eye color of a person , temperature , etc . \\n ● \\n Attribute is also known as variable , field , characteristic , dimension , or feature \\n ● \\n Object : collection of attributes ● \\n Object is also known as record , point , case , sample , entity , or instance \\n Attributes \\n  is Data ? e Data set : collection of objects and their attributes e Attribute : property or characteristic of an object \\n o Examples : eye color of a person , temperature , etc . \\n e Attribute is also known as _ variable , field , \\n characteristic , dimension , or feature Object : collection of attributes \\n Object is also known as record , point , case , \\n sample , entity , or instance Objects oo NO a kB WNY = aK \\n fo }  \\n Married \\n Single \\n Married \\n Divorced \\n Married \\n Divorced \\n  125 K \\n 100 K \\n 70 K \\n 120 K \\n 95 K \\n 60 K \\n 220 K \\n 85 K \\n 75 K \\n 90 K Taxable \\n  \\n No \\n  is Data ? ● \\n Data set : collection of objects and their attributes ● \\n Attribute : property or characteristic of an object ○ \\n Examples : eye color of a person , temperature , etc . \\n ● \\n Attribute is also known as variable , field , characteristic , dimension , or feature ● \\n Object : collection of attributes ● \\n Object is also known as record , point , case , sample , entity , or instance  of Attributes \\n ● \\n Nominal ( Categories ) \\n ○ \\n Examples : ID numbers , eye color , zip codes \\n ● \\n Ordinal (  ) \\n ○ \\n Examples : Rankings ( e.g. , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) \\n ● \\n Interval (  ,  ) \\n ○ \\n Examples : Calendar dates , temperatures in Celsius or Fahrenheit \\n ● \\n Ratio (  ,  ) \\n ○ \\n Examples : Temperature in Kelvin , length , counts , elapsed time ( e.g. , time to run a race ) Types of Attributes e Nominal ( Categories ) o Examples : ID numbers , eye color , zip codes e Ordinal (  ) \\n o Examples : Rankings ( e.g. , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , \\n medium , short ) e Interval (  ,  ) o Examples : Calendar dates , temperatures in Celsius or Fahrenheit e Ratio (  ,  ) o Examples : Temperature in Kelvin , length , counts , elapsed time ( e.g. , time to run a race ) Types of Attributes ● \\n Nominal ( Categories ) ○ \\n Examples : ID numbers , eye color , zip codes ● \\n Ordinal (  ) ○ \\n Examples : Rankings ( e.g. , taste of potato chips on a scale from 1 - 10 ) , grades , height ( tall , medium , short ) ● \\n Interval (  ,  ) ○ \\n Examples : Calendar dates , temperatures in Celsius or Fahrenheit ● \\n Ratio (  ,  ) ○ \\n Examples : Temperature in Kelvin , length , counts , elapsed time ( e.g. , time to run a race ) Properties of  \\n ● \\n Nominal \\n ○ \\n Distinctness ( = , ≠ ) \\n ● \\n Ordinal \\n ○ \\n Distinctness ( = , ≠ ) \\n ○ \\n Order ( < > ) \\n ● \\n Interval \\n ○ \\n Distinctness ( = , ≠ ) \\n ○ \\n Order ( < > ) \\n ○ \\n  ( + , - ) \\n ● \\n Ratio \\n ○ \\n Distinctness ( = , ≠ ) \\n ○ \\n Order ( < , > ) \\n ○ \\n  ( + , - ) \\n ○ \\n  ( * , / ) Properties of  e Nominal \\n o Distinctness (= , # ) \\n e Ordinal o _ Distinctness ( = , # ) \\n o Order ( < > ) \\n e Interval \\n o _ Distinctness ( = , # ) \\n o Order ( < > ) \\n o  ( + , - ) e Ratio \\n o _ Distinctness ( = , # ) \\n o Order ( < , > ) \\n o  ( + , - ) \\n o  ( * , / ) Properties of  ● \\n Nominal ○ \\n Distinctness ( = , ≠ ) \\n ● \\n Ordinal ○ \\n Distinctness ( = , ≠ ) \\n ○ \\n Order ( < > ) \\n ● \\n Interval ○ \\n Distinctness ( = , ≠ ) \\n ○ \\n Order ( < > ) \\n ○ \\n  ( + , - ) \\n ● \\n Ratio ○ \\n Distinctness ( = , ≠ ) \\n ○ \\n Order ( < , > ) \\n ○ \\n  ( + , - ) \\n ○ \\n  ( * , / ) Discrete vs.  \\n ● \\n  : Values from a finite or countably infinite set . \\n ○ \\n Examples : Zip codes , counts , or words in documents . \\n ● \\n Represented as integers . \\n ● \\n Note : Binary attributes are a special case of discrete attributes . \\n ● \\n  : Values are real numbers . \\n ○ \\n Examples : Temperature , height , weight . \\n ● \\n Real values , practically measured with finite digits \\n ● \\n Represented as floating - point variables . Discrete vs.  e  : Values from a finite or \\n countably infinite set . o Examples : Zip codes , counts , or words in documents . Discrete \\n Represented as integers . e \\n Note : Binary attributes are a special case of \\n discrete attributes . © \\n ® \\n e  : Values are real numbers . Discrete v / s o Examples : Temperature , height , weight . \\n Real values , practically measured with finite digits \\n Represented as floating - point variables . Discrete vs.  ● \\n  : Values from a finite or countably infinite set . ○ \\n Examples : Zip codes , counts , or words in documents . \\n ● \\n Represented as integers . \\n ● \\n Note : Binary attributes are a special case of discrete attributes . ● \\n  : Values are real numbers . ○ \\n Examples : Temperature , height , weight . \\n ● \\n Real values , practically measured with finite digits \\n ● \\n Represented as floating - point variables .  \\n ● \\n In asymmetric attributes , only the presence ( non - zero value ) matters . \\n ○ \\n Examples : Words present in documents : Focus on words that appear . \\n ○ \\n Items present in customer transactions : Emphasize purchased items . \\n ● \\n Real -  : \\n In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  e Inasymmetric attributes , only the presence ( non - zero value ) matters . \\n o Examples : Words present in documents : Focus on words that appear . o Items present in customer transactions : Emphasize purchased items . e Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  ● \\n In asymmetric attributes , only the presence ( non - zero value ) matters . ○ \\n Examples : Words present in documents : Focus on words that appear . ○ \\n Items present in customer transactions : Emphasize purchased items . ● \\n Real -  : In a grocery store encounter , would we say : “ Our purchases are similar because we did n\\'t buy most of the same products ? ”  of Data \\n ● \\n Dimensionality ( Number of Attributes ) \\n ○ \\n High - dimensional data presents unique challenges . \\n ● \\n Sparsity \\n ○ \\n Emphasizes the importance of presence over absence . \\n ● \\n Resolution \\n ○ \\n Patterns can vary based on the scale of measurement . \\n ● \\n Size \\n ○ \\n The type of analysis often depends on the data \\'s size . \\n ● \\n Distribution \\n ○ \\n Considers centrality and dispersion in the data .  of Data e Dimensionality ( Number of Attributes ) \\n o High - dimensional data presents unique challenges . e Sparsity \\n o Emphasizes the importance of presence over absence . e Resolution \\n o Patterns can vary based on the scale of measurement . e Size \\n o The type of analysis often depends on the data \\'s size . e Distribution \\n o Considers centrality and dispersion in the data .  of Data ● \\n Dimensionality ( Number of Attributes ) ○ \\n High - dimensional data presents unique challenges . ● \\n Sparsity ○ \\n Emphasizes the importance of presence over absence . ● \\n Resolution ○ \\n Patterns can vary based on the scale of measurement . ● \\n Size ○ \\n The type of analysis often depends on the data \\'s size . ● \\n Distribution ○ \\n Considers centrality and dispersion in the data . Types of  \\n ● \\n  : records with fixed attributes \\n ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  \\n ● \\n Graphs and Networks \\n ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  \\n ● \\n Ordered ( Sequence ) Data \\n ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … \\n ● \\n  \\n ○ \\n RGB Images ○ \\n Satellite images Types of  e  : records with fixed attributes Person : © Relational records \\n © Data matrix ... To | _ [ o  |i | | _ _ Alvaro ] Valencia__F~ norelation \\n [ 4 | | [ fom — _ ] Car : [ cart | Model | Year |__Value | Pers 1D \\n |_1o1_| [ 1973 | 100000 =| \\n [ 102 |  | 1965 | 330000 ] 0 _ | [ 04 | | 2005 | 150000 ] 4 _ | \\n [ 0s [ renaut [ 1998 | — 2000 [ 3 _ _ ] \\n 106 — [ _ Renautt [ 2007 | — 7o00 | 3 ] \\n [ ior [ smart 199 ] 2000 ] 2 ] Types of  ● \\n  : records with fixed attributes ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  ● \\n Graphs and Networks ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  ● \\n Ordered ( Sequence ) Data ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … ● \\n  ○ \\n RGB Images ○ \\n Satellite images Types of  \\n ● \\n  : records with fixed attributes \\n ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  \\n ● \\n Graphs and Networks \\n ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  \\n ● \\n Ordered ( Sequence ) Data \\n ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … \\n ● \\n  \\n ○ \\n RGB Images ○ \\n Satellite images Types of  e  : records with fixed attributes \\n o Relational records \\n o © Data matrix ... \\n o  e Graphs and Networks \\n o Transportation network \\n o Social or information networks ... \\n o  of  ● \\n  : records with fixed attributes ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  ● \\n Graphs and Networks ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  ● \\n Ordered ( Sequence ) Data ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … ● \\n  ○ \\n RGB Images ○ \\n Satellite images Types of  \\n ● \\n  : records with fixed attributes \\n ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  \\n ● \\n Graphs and Networks \\n ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  \\n ● \\n Ordered ( Sequence ) Data \\n ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … \\n ● \\n  \\n ○ \\n RGB Images ○ \\n Satellite images Types of  : records with fixed attributes [ e ) \\n [ e ) \\n [ e ) Relational records \\n Data matrix ... \\n  and Networks [ e ) \\n [ e ) \\n [ e ) Transportation network \\n Social or information networks ... \\n  ( Sequence ) Data [ e ) \\n [ e ) \\n [ e ) Video : sequence of image \\n  \\n Temporal sequence ... -| | \\n J \\n 34 j \\\\ i \\n | Mi | \\\\ \\n is ai py | c \\n ot +1 \" ve 4 Lt jell 1s 202 \\n J WI Vi YS \\n 154 | if  \\n \\\\ 1 WA \\n 10 | Hi \\n 54 \\n 1 10 40 50 60 70 80 0 9 \\n  of  ● \\n  : records with fixed attributes ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  ● \\n Graphs and Networks ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  ● \\n Ordered ( Sequence ) Data ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … ● \\n  ○ \\n RGB Images ○ \\n Satellite images Types of  \\n ● \\n  \\n ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  \\n ● \\n Graphs and Networks \\n ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  \\n ● \\n Ordered ( Sequence ) Data \\n ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … \\n ● \\n  \\n ○ \\n RGB Images ○ \\n Satellite images Types of  e  \\n o Relational records \\n o © Data matrix ... o / \\n Administrative \\n e Graphs and  \\n o Transportation network Streets \\n ° o Social or information networks ... Vector \\n o  \\n Parcels \\n e Ordered ( Sequence ) Data \\n o Video : sequence of image  \\n o  \\n o Temporal sequence ... Raster \\n Elevation e  \\n o RGB Images \\n o Satellite images  of  ● \\n  ○ \\n Relational records \\n ○ \\n Data matrix … \\n ○ \\n  ● \\n Graphs and Networks ○ \\n Transportation network \\n ○ \\n Social or information networks … \\n ○ \\n  ● \\n Ordered ( Sequence ) Data ○ \\n Video : sequence of image \\n ○ \\n  \\n ○ \\n Temporal sequence … ● \\n  ○ \\n RGB Images ○ \\n Satellite images 2- Data preprocessing 2- Data preprocessing 2- Data preprocessing What is  ? —  cleaning \\n ● \\n Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies \\n Data integration \\n ● \\n Integration of multiple databases , data cubes , or files \\n Data transformation and data discretization \\n ● \\n Normalization \\n ● \\n Discretization \\n ● \\n Sampling \\n Data reduction ( covered in the next chapter ) \\n ● \\n Dimensionality reduction ● \\n Data compression What is  ? —  cleaning \\n e Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies \\n Data integration \\n e Integration of multiple databases , data cubes , or files \\n Data transformation and data discretization \\n e Normalization \\n e Discretization \\n e Sampling \\n Data reduction ( covered in the next chapter ) \\n e Dimensionality reduction e Data compression What is  ? —  cleaning ● \\n Handle missing data , smooth noisy data , identify / remove outliers , and resolve inconsistencies Data integration ● \\n Integration of multiple databases , data cubes , or files Data transformation and data discretization ● \\n Normalization ● \\n Discretization ● \\n  reduction ( covered in the next chapter ) ● \\n Dimensionality reduction ● \\n Data compression  \\n ●  adversely affects data processing efforts . \\n Example : Poor data can result in wrong loan decisions . \\n – Some credit - worthy candidates are denied loans \\n – More loans are given to individuals that default \\n ● What types of data quality issues exist ? and how can we identify them ? \\n ● What can we do about these problems ? ● Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data  e  adversely affects data processing efforts . Data r ° \\n sy \\n Quality \\n ES 5Fe \\n are \\n r \\n = Example : Poor data can result in wrong loan decisions . \\n — Some credit - worthy candidates are denied loans \\n — More loans are given to individuals that default e What types of data quality issues exist ? and how can we identify them ? \\n e What can we do about these problems ? e Examples of data quality problems : \\n — Noise and outliers \\n — Wrong data \\n — Fake data \\n — Missing values — Duplicate data  ●  adversely affects data processing efforts . Example : Poor data can result in wrong loan decisions . – Some credit - worthy candidates are denied loans \\n – More loans are given to individuals that default ● What types of data quality issues exist ? and how can we identify them ? ● What can we do about these problems ? ● Examples of data quality problems : – Noise and outliers – Wrong data – Fake data – Missing values – Duplicate data Noise \\n ● \\n Noise in Objects : Extraneous elements affecting data integrity . \\n ● \\n Noise in Attributes : Modification of original attribute values . \\n ● \\n Examples : \\n ○ \\n Distorted voice on a poor phone line . \\n ○ \\n \" Snow \" on a television screen . \\n ○ \\n Erroneous entries caused by data entry errors or system glitches . Noise e Noise in Objects : Extraneous \\n elements affecting data integrity . e Noise in Attributes : Modification of \\n original attribute values . e Examples : \\n © Distorted voice on a poor phone line . \\n o \" Snow \" on a television screen . \\n o Erroneous entries caused by data entry errors or system glitches . Noise ● \\n Noise in Objects : Extraneous elements affecting data integrity . ● \\n Noise in Attributes : Modification of original attribute values . ● \\n Examples : ○ \\n Distorted voice on a poor phone line . ○ \\n \" Snow \" on a television screen . ○ \\n Erroneous entries caused by data entry errors or system glitches . Outliers \\n ● \\n Data objects with characteristics significantly different from the majority in the dataset . \\n ● \\n Case 1 : Outliers as Noise : \\n ○ \\n Outliers can be noise that disrupts data analysis . \\n ● \\n Case 2 : Outliers as the Focus : \\n ○ \\n In certain scenarios , outliers are the primary focus of analysis . \\n ○ \\n Credit card fraud detection ○ \\n Intrusion detection . \\n ● \\n  : \\n ○ \\n Explore the reasons behind the presence of outliers . Outliers e Data objects with characteristics significantly different from the majority in the dataset . * e Case 1 : Outliers as Noise : © sai o Outliers can be noise that disrupts data analysis . : \\n e Case 2 : Outliers as the Focus : © Incertain scenarios , outliers are the primary focus of analys © o Credit card fraud detection Sts o Intrusion detection . © & Ret \\n e  : ae o Explore the reasons behind the presence of outliers . Outliers ● \\n Data objects with characteristics significantly different from the majority in the dataset . ● \\n Case 1 : Outliers as Noise : ○ \\n Outliers can be noise that disrupts data analysis . \\n ● \\n Case 2 : Outliers as the Focus : ○ \\n In certain scenarios , outliers are the primary focus of analysis . ○ \\n Credit card fraud detection ○ \\n Intrusion detection . \\n ● \\n  : ○ \\n Explore the reasons behind the presence of outliers . How to  ? \\n ● \\n Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . \\n ● \\n Regression : Smooth data through regression functions . \\n ○ \\n Use other attributes to predict the noisy attributes \\n ● \\n Clustering : Identify and eliminate outliers . ● \\n Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? Binning : Sort data into bins , enabling smoothing using means , medians , \\n or boundaries . Regression : Smooth data through regression functions . \\n o Use other attributes to predict the noisy attributes Clustering : Identify and eliminate outliers . \\n Semi - supervised : Combine automated and human inspection to identify noise and outliers . How to  ? ● \\n Binning : Sort data into bins , enabling smoothing using means , medians , or boundaries . ● \\n Regression : Smooth data through regression functions . ○ \\n Use other attributes to predict the noisy attributes ● \\n Clustering : Identify and eliminate outliers . ● \\n Semi - supervised : Combine automated and human inspection to identify noise and outliers .  \\n ● \\n Reasons for missing values \\n ○ \\n Information is not collected ○ \\n ( e.g. , people decline to give their age and weight ) \\n ○ \\n Attributes may not be applicable to all cases ○ \\n ( e.g. , annual income is not applicable to children ) \\n ● \\n Handling missing values \\n ○ \\n Eliminate data objects or variables \\n ○ \\n Estimate missing values \\n ■ \\n Example : time series of temperature \\n ■ \\n Example : census results ○ \\n Ignore the missing value during analysis Missing values  SibSp  \\n A / S 21171 \\n PC 17599 \\n STON / O2 . 3101282 \\n 113803 $ 3.1 C123 373450 8.05 Reasons for missing values 3 \\n Information is not collected ( e.g. , people decline to give their age and weight ) \\n Attributes may not be applicable to all cases \\n ( e.g. , annual income is not applicable to children ) 330877 8.4583 [ e ) \\n [ e ) \\n [ e ) \\n [ e ) Handling missing values \\n o Eliminate data objects or variables \\n o Estimate missing values \\n = Example : time series of temperature \\n = Example : census results \\n o Ignore the missing value during analysis  ● \\n Reasons for missing values ○ \\n Information is not collected ○ \\n ( e.g. , people decline to give their age and weight ) \\n ○ \\n Attributes may not be applicable to all cases ○ \\n ( e.g. , annual income is not applicable to children ) ● \\n Handling missing values ○ \\n Eliminate data objects or variables \\n ○ \\n Estimate missing values ■ \\n Example : time series of temperature \\n ■ \\n Example : census results ○ \\n Ignore the missing value during analysis  \\n ● \\n Occurrence of identical or nearly identical data objects . \\n ● \\n Common when merging data from diverse sources . \\n ○ \\n Example : Identical individuals with multiple email addresses . \\n How to handle duplicate data \\n ● \\n Remove duplicate data objects . \\n ● \\n  : When and Why ? \\n ○ \\n Customers with multiple accounts may unintentionally accumulate points separately . ○ \\n Keeping duplicate data ensures they receive all earned benefits .  e Occurrence of identical or nearly identical data objects . \\n e Common when merging data from diverse sources . o Example : Identical individuals with multiple email addresses . 2 \\n How to handle duplicate data \\\\ \\n e Remove duplicate data objects . } \\n e  : When and Why ? prviemn ders o Customers with multiple accounts may unintentionally \\n accumulate points separately . \\n © Keeping duplicate data ensures they receive all earned benefits .  ● \\n Occurrence of identical or nearly identical data objects . ● \\n Common when merging data from diverse sources . ○ \\n Example : Identical individuals with multiple email addresses . How to handle duplicate data ● \\n Remove duplicate data objects . ● \\n  : When and Why ? ○ \\n Customers with multiple accounts may unintentionally accumulate points separately . ○ \\n Keeping duplicate data ensures they receive all earned benefits .  \\n ● \\n Normalization : Scaling data to a standard range ( e.g. , 0 to 1 ) . \\n ● \\n Discretization : Converting continuous data into discrete categories . \\n ● \\n Sampling : Selecting a subset to represent a larger population .  e Normalization : Scaling data to a standard range ( e.g. , 0 to 1 ) . e Discretization : Converting continuous data into discrete categories . e Sampling : Selecting a subset to represent a larger population .  ● \\n Normalization : Scaling data to a standard range ( e.g. , 0 to 1 ) . ● \\n Discretization : Converting continuous data into discrete categories . ● \\n Sampling : Selecting a subset to represent a larger population . Normalization \\n ● \\n Normalization ensures that variables are on a consistent scale . \\n ● \\n Normalization is crucial for many data mining algorithms . \\n ● \\n  . \\n Min - max normalization : to [ new_minA , new_maxA ] \\n Z - score normalization ( μ : mean , σ : standard deviation ): Normalization e Normalization ensures that variables are on a consistent scale . \\n e Normalization is crucial for many data mining algorithms . \\n e  . Min - max normalization : to [ mew_min , , new_max , ] ; v — min : . ; \\n vi = — — — — — _ _ ( new _ max:—new _ min:)+new _ min : max : — min : \\n Z - score normalization ( yu : mean , o : standard deviation ): \\n _ X 7b \\n oO z Normalization ● \\n Normalization ensures that variables are on a consistent scale . \\n ● \\n Normalization is crucial for many data mining algorithms . \\n ● \\n  . Min - max normalization : to [ new_minA , new_maxA ] Z - score normalization ( μ : mean , σ : standard deviation ): Min - max normalization VS Z - score normalization \\n ● \\n Min - max normalization : \\n ○ \\n Guarantees all attributes will have the exact same scale . \\n ○ \\n Does not handle outliers well . \\n ● \\n Z - score normalization : ○ \\n Handles outliers . \\n ○ \\n Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses e 100 \\n Min - max normalization : 3 59 \\n vo \\n o Guarantees all attributes will have the Fs \\n exact same scale . = 0 — — 0 20 40 60 80 o Does not handle outliers well . Years old Z - score normalization : .  using min - max normalizatio \\n o Handles outliers . 3 \\n Vv \\n B ® \\n E10 e \\n ; E 107 eo e ° S o \\n o Does not produce normalized data & fe e * . 2 \\n | 2 a ye \\n with the exact same scale . E 0.54 aA ene \\n 8 - © ° 8 \\n = ob , 0 corde \\n iJ ® * ee \\n g COT — \\n 2 0.0 0.2 0.4 0.6 0.8 \\n  old ( normalized ) Min - max normalization VS Z - score normalization ● \\n Min - max normalization : ○ \\n Guarantees all attributes will have the exact same scale . \\n ○ \\n Does not handle outliers well . ● \\n Z - score normalization : ○ \\n Handles outliers . \\n ○ \\n Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization \\n ● \\n Min - max normalization : \\n ○ \\n Guarantees all attributes will have the exact same scale . \\n ○ \\n Does not handle outliers well . \\n ● \\n Z - score normalization : ○ \\n Handles outliers . \\n ○ \\n Does not produce normalized data with the exact same scale . Min - max normalization VS Z - score normalization Un - normalized Houses pa \\n o \\n lo } 50 Number of rooms e Min - max normalization : \\n o Guarantees all attributes will have the 0 Miumalp . exact same scale . 0 20 40 60 80 \\n . Years old \\n o Does not handle outliers well . 3 \\n e 2Z - score normalization : s  using z - score normalizatio \\n o Handles outliers . 5 24 ° 33 } @ \\n o Does not produce normalized data z ol x : \\n with the exact same scale . 3 Pee ny YS g T T T \\n 2 22 = 3 4 5 \\n = Years old ( normalized ) Min - max normalization VS Z - score normalization ● \\n Min - max normalization : ○ \\n Guarantees all attributes will have the exact same scale . \\n ○ \\n Does not handle outliers well . ● \\n Z - score normalization : ○ \\n Handles outliers . \\n ○ \\n Does not produce normalized data with the exact same scale . Discretization \\n ● \\n Converting a continuous attribute into an ordinal attribute . \\n ● \\n A potentially infinite number of values are mapped to a small number of categories . \\n ● \\n Discretization is used in both unsupervised and supervised settings . Discretization [ ie ae or L , CONTINUOUS DISCRETE \\n VALUE VALUE e Converting a continuous attribute into an ordinal attribute . \\n e A potentially infinite number of values are mapped to a small number of categories . e Discretization is used in both unsupervised and supervised settings . Discretization ● \\n Converting a continuous attribute into an ordinal attribute . ● \\n A potentially infinite number of values are mapped to a small number of categories . ● \\n Discretization is used in both unsupervised and supervised settings . Discretization \\n ● \\n Unsupervised ○ \\n Binning : Top - down split \\n ○ \\n Histogram analysis : Top - down split ○ \\n Clustering analysis : top - down split or bottom - up merge \\n ● \\n Supervised ○ \\n Decision - tree analysis : top - down split \\n ○ \\n Correlation analysis : bottom - up merge \\n ● \\n Note : All the methods can be applied recursively Discretization e Unsupervised o Binning : Top - down split o Histogram analysis : Top - down split o Clustering analysis : top - down split or bottom - up merge \\n e Supervised o Decision - tree analysis : top - down split o Correlation analysis : bottom - up merge \\n e Note : All the methods can be applied recursively Discretization ● \\n Unsupervised ○ \\n Binning : Top - down split ○ \\n Histogram analysis : Top - down split ○ \\n Clustering analysis : top - down split or bottom - up merge ● \\n Supervised ○ \\n Decision - tree analysis : top - down split ○ \\n Correlation analysis : bottom - up merge ● \\n Note : All the methods can be applied recursively Sampling \\n Sampling is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . \\n ● \\n We use sampling because obtaining the entire dataset of interest is : \\n ○ \\n Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . \\n ○ \\n Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . \\n ● \\n Challenges : \\n ○ \\n Ensuring the sample is representative of the population . \\n ○ \\n Addressing potential bias in the sampling process . \\n Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more \\n manageable for analysis while maintaining its representativeness . e We use sampling because obtaining the entire dataset of interest is : \\n o Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . \\n o Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . e Challenges : \\n o Ensuring the sample is representative of the population . \\n o Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium \\n between resource efficiency and the ability to derive meaningful insights .  is selecting a subset of data from a larger dataset to make it more manageable for analysis while maintaining its representativeness . ● \\n We use sampling because obtaining the entire dataset of interest is : ○ \\n Expensive : Collecting , storing , and processing vast amounts of data can be cost - prohibitive . \\n ○ \\n Time - consuming : Analyzing the complete dataset can be impractical due to time constraints . ● \\n Challenges : ○ \\n Ensuring the sample is representative of the population . \\n ○ \\n Addressing potential bias in the sampling process . Sampling is an essential tool in data analysis , achieving a crucial equilibrium between resource efficiency and the ability to derive meaningful insights . Sample size \\n Selecting an appropriate sample size is a critical decision in research and analysis . Sample size h and | decision in researc itica ize is acr iate sample si ing an appropria Select analysis . eg ee Pe oe \\n lee # PSs \\n et gS Shheaal ote \\n oP gor hae atk A Siee \\n sed FS ? Sample size Selecting an appropriate sample size is a critical decision in research and analysis . Sampling methods \\n Simple random sampling ● \\n Equal probability of selecting any particular item \\n Sampling without replacement \\n ● \\n Once an object is selected , it is removed from the population \\n Sampling with replacement \\n ● \\n A selected object is not removed from the population \\n Stratified sampling \\n ● \\n Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i.e. , approximately the same percentage of the data ) Random sampling \\n with replacement Sampling methods Random sampling \\n without replacement Simple random sampling \\n Cluster sampling Origian ! \\n data e Equal probability of selecting any particular item \\n Sampling without replacement ciel cana \\n e Once an object is selected , it is removed from the population \\n Sampling with replacement \\n e Aselected object is not removed from the population Stratified sampling e Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i.e. , approximately the same percentage of the data ) Sampling methods Simple random sampling ● \\n Equal probability of selecting any particular item Sampling without replacement ● \\n Once an object is selected , it is removed from the population Sampling with replacement ● \\n A selected object is not removed from the population Stratified sampling ● \\n Partition ( or cluster ) the data set , and draw samples from each partition ( proportionally , i.e. , approximately the same percentage of the data ) 3- Similarity and  3- Similarity and  3- Similarity and  and  \\n ● \\n  : \\n ○ \\n Quantifies data object likeness . \\n ○ \\n Higher values indicate greater similarity . \\n ○ \\n Typically within the range [ 0 , 1 ] . \\n ● \\n  : \\n ○ \\n Quantifies data object differences . \\n ○ \\n Lower values indicate greater similarity . \\n ○ \\n Often starts at 0 and varies in the upper limit . \\n ● \\n Proximity : \\n ○ \\n Refers to either similarity or dissimilarity . \\n Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Similarity and  e  : \\n © Quantifies data object likeness . \\n © Higher values indicate greater similarity . \\n © Typically within the range [ 0 , 1 ] . e  : \\n © Quantifies data object differences . e * \\n o Lower values indicate greater similarity . \\n o Often starts at 0 and varies in the upper limit . —  e Proximity : \\n © Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for \\n pattern recognition , clustering , and classification . Similarity and  ● \\n  : ○ \\n Quantifies data object likeness . \\n ○ \\n Higher values indicate greater similarity . \\n ○ \\n Typically within the range [ 0 , 1 ] . ● \\n  : ○ \\n Quantifies data object differences . \\n ○ \\n Lower values indicate greater similarity . \\n ○ \\n Often starts at 0 and varies in the upper limit . ● \\n Proximity : ○ \\n Refers to either similarity or dissimilarity . Similarity reveal valuable data relationships for pattern recognition , clustering , and classification . Properties of a Distance \\n ● \\n Distance t is a metric if it satisfies these properties : ○ \\n Non - Negativity : \\n ■ \\n d(x , y ) ≥ 0 for all x and y. \\n ■ \\n d(x , y ) = 0 if and only if x = y. \\n ○ \\n Symmetry : \\n ■ \\n d(x , y ) = d(y , x ) for all x and y. \\n ○ \\n  : \\n ■ \\n d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. \\n ● \\n Metrics ensure that distances align with real - world geometric properties \\n Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a Distance e Distance t is a metric if it satisfies these properties : o © Non - Negativity : \\n m d(x , y ) 20 for all x and y. \\n m d(x , y ) = 0 if and only if x = y. o Symmetry : \\n m d(x , y ) = d(y , x ) for all x and y. o  : \\n m d(x , Z ) S$ d(x , y ) + d(y , Z ) for all x , y , and z. \\n e Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data \\n analysis . Properties of a Distance ● \\n Distance t is a metric if it satisfies these properties : ○ \\n Non - Negativity : ■ \\n d(x , y ) ≥ 0 for all x and y. \\n ■ \\n d(x , y ) = 0 if and only if x = y. ○ \\n Symmetry : ■ \\n d(x , y ) = d(y , x ) for all x and y. ○ \\n  : ■ \\n d(x , z ) ≤ d(x , y ) + d(y , z ) for all x , y , and z. ● \\n Metrics ensure that distances align with real - world geometric properties Metrics guarantee meaningful and reliable distance measurements in data analysis . Properties of a Similarity \\n ● \\n Identity : \\n ○ \\n s(x , y ) = 1 ( or maximum similarity ) only if x = y. \\n ○ \\n Note : This property may not always hold , e.g. , cosine similarity . \\n ● \\n Symmetry : \\n ○ \\n s(x , y ) = s(y , x ) for all x and y. \\n ○ \\n Symmetry ensures that the order of comparison does not affect the similarity score . \\n Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Properties of a Similarity e Identity : \\n Oo § ( x , y ) = 1 ( or maximum similarity ) only if x = y. \\n o Note : This property may not always hold , e.g. , cosine similarity . e Symmetry : \\n Oo § ( x , y ) = S(y , x ) for all x and y. \\n o Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency \\n of similarity measures in data analysis . Properties of a Similarity ● \\n Identity : ○ \\n s(x , y ) = 1 ( or maximum similarity ) only if x = y. \\n ○ \\n Note : This property may not always hold , e.g. , cosine similarity . ● \\n Symmetry : ○ \\n s(x , y ) = s(y , x ) for all x and y. \\n ○ \\n Symmetry ensures that the order of comparison does not affect the similarity score . Understanding these properties helps ensure the reliability and consistency of similarity measures in data analysis . Similarity and dissimilarity matrix \\n ● \\n  \\n ○ \\n Distances between all data objects in a dataset . \\n ○ \\n Useful for clustering and nearest neighbor algorithms . \\n ○ \\n Symmetric , with values reflecting dissimilarities . \\n ● \\n  \\n ○ \\n Similarities between data objects . \\n ○ \\n Valuable for clustering , recommendation systems , … \\n ○ \\n Often symmetric , with higher values indicating stronger similarities . Similarity and dissimilarity matrix e  \\n o Distances between all data objects in a dataset . o Useful for clustering and nearest neighbor algorithms . o Symmetric , with values reflecting dissimilarities . e  \\n o Similarities between data objects . \\n o Valuable for clustering , recommendation systems , ... \\n o Often symmetric , with higher values indicating \\n stronger similarities . EEE Similarity and dissimilarity matrix ● \\n  ○ \\n Distances between all data objects in a dataset . \\n ○ \\n Useful for clustering and nearest neighbor algorithms . \\n ○ \\n Symmetric , with values reflecting dissimilarities . ● \\n  ○ \\n Similarities between data objects . \\n ○ \\n Valuable for clustering , recommendation systems , … \\n ○ \\n Often symmetric , with higher values indicating stronger similarities . Distances and similarity examples \\n ● \\n Proximity measures for numerical vectors \\n ○ \\n  \\n ○ \\n  \\n ○ \\n  \\n ○ \\n Linear correlation \\n ● \\n Proximity measures for binary vectors \\n ○ \\n  ( SMC ) \\n ○ \\n  and similarity examples e Proximity measures for numerical vectors \\n o  \\n o  \\n o  \\n o Linear correlation e Proximity measures for binary vectors \\n o  ( SMC ) \\n o  and similarity examples ● \\n Proximity measures for numerical vectors ○ \\n  ○ \\n  ○ \\n  ○ \\n Linear correlation ● \\n Proximity measures for binary vectors ○ \\n  ( SMC ) ○ \\n  ( applicable to numerical vectors ) \\n ● : number of attributes . \\n ● \\n , , : : kth attributes for objects x and y , respectively . \\n Standardization is necessary , if scales differ .  ( applicable to numerical vectors ) d(x , y ) — e 7 ? : number of attributes . \\n e Uk , ye : kth attributes for objects x \\n and y , respectively . Standardization is necessary , if \\n scales differ .  ( applicable to numerical vectors ) ● : number of attributes . \\n ● \\n , , : : kth attributes for objects x and y , respectively . Standardization is necessary , if scales differ . Example :  matrix Example :  matrix 4 - \\n 2 @p ! Ft ot | le | \\n 03 ad | opt | of 2.828 ] 3.162 ] 5.099 ] _ a e | pr | agosto aia ] 3.162 \\n 22 | ps | 3.62 ] iat io 0 e , | ps [ | 5090 ] 3162 ) a ] Example :  matrix  ( applicable to numerical vectors ) \\n ● \\n Generalization of  . \\n ● \\n r : parameter ● \\n n : number of attributes \\n ● \\n xk and yk are , respectively , the kth attributes or objects x and y. \\n ● \\n The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) a 1 / r \\n a(x , y ) = { Solow — al \\n k=1 Generalization of  . r : parameter n : number of attributes x , and y , are , respectively , the k * \" attributes or objects x and y. The hyperparameters r Allows to adapt the distance to the characteristics of data .  ( applicable to numerical vectors ) ● \\n Generalization of  . ● \\n r : parameter ● \\n n : number of attributes ● \\n xk and yk are , respectively , the kth attributes or objects x and y. ● \\n The hyperparameters r Allows to adapt the distance to the characteristics of data .  of  \\n ● \\n  ( r = 1 ): \\n ○ \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n ○ \\n Ideal for measuring distances in grid - like paths . \\n ○ \\n Binary vector example : Hamming distance counts differing bits . \\n ● \\n  ( r = 2 ): \\n ○ \\n The most commonly used distance metric . \\n ○ \\n Measures the straight - line distance in Euclidean space . \\n ● \\n  ( r → ∞ ): \\n ○ \\n Also called Lmax norm or L∞ norm distance . \\n ○ \\n Calculates the maximum difference between any component of vectors . \\n ○ \\n Appropriate when movement is unrestricted in any direction .  of  e  ( r = 1 ): \\n o Also known as Manhattan , taxicab , or L1 norm distance . \\n o Ideal for measuring distances in grid - like paths . o Binary vector example : Hamming distance counts differing bits . siaciniaed e  ( r = 2 ): \\n o The most commonly used distance metric . \\n o Measures the straight - line distance in Euclidean space . e  ( r — ~ ): \\n o Also called Lmax norm or L » norm distance . \\n o Calculates the maximum difference between any component of vectors . \\n o Appropriate when movement is unrestricted in any direction .  of  ● \\n  ( r = 1 ): ○ \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n ○ \\n Ideal for measuring distances in grid - like paths . \\n ○ \\n Binary vector example : Hamming distance counts differing bits . ● \\n  ( r = 2 ): ○ \\n The most commonly used distance metric . \\n ○ \\n Measures the straight - line distance in Euclidean space . ● \\n  ( r → ∞ ): ○ \\n Also called Lmax norm or L∞ norm distance . \\n ○ \\n Calculates the maximum difference between any component of vectors . \\n ○ \\n Appropriate when movement is unrestricted in any direction .  of  \\n ● \\n  ( r = 1 ): \\n ○ \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n ○ \\n Ideal for measuring distances in grid - like paths . \\n ○ \\n Binary vector example : Hamming distance counts differing bits . \\n ● \\n  ( r = 2 ): \\n ○ \\n The most commonly used distance metric . \\n ○ \\n Measures the straight - line distance in Euclidean space . \\n ● \\n  ( r → ∞ ): \\n ○ \\n Also called Lmax norm or L∞ norm distance . \\n ○ \\n Calculates the maximum difference between any component of vectors . \\n ○ \\n Appropriate when movement is unrestricted in any direction .  of  e  ( r = 1 ): \\n o Also known as Manhattan , taxicab , or L1 norm distance . \\n o Ideal for measuring distances in grid - like paths . Euclidean \\n o Binary vector example : Hamming distance counts differing bits . @ e  ( r = 2 ): \\n o The most commonly used distance metric . \\n o Measures the straight - line distance in Euclidean space . e  ( r — ~ ): \\n o Also called Lmax norm or L » norm distance . \\n o Calculates the maximum difference between any component of vectors . \\n o Appropriate when movement is unrestricted in any direction .  of  ● \\n  ( r = 1 ): ○ \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n ○ \\n Ideal for measuring distances in grid - like paths . \\n ○ \\n Binary vector example : Hamming distance counts differing bits . ● \\n  ( r = 2 ): ○ \\n The most commonly used distance metric . \\n ○ \\n Measures the straight - line distance in Euclidean space . ● \\n  ( r → ∞ ): ○ \\n Also called Lmax norm or L∞ norm distance . \\n ○ \\n Calculates the maximum difference between any component of vectors . \\n ○ \\n Appropriate when movement is unrestricted in any direction .  of  \\n ● \\n  ( r = 1 ): \\n ○ \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n ○ \\n Ideal for measuring distances in grid - like paths . \\n ○ \\n Binary vector example : Hamming distance counts differing bits . \\n ● \\n  ( r = 2 ): \\n ○ \\n The most commonly used distance metric . \\n ○ \\n Measures the straight - line distance in Euclidean space . \\n ● \\n  ( r → ∞ ): \\n ○ \\n Also called Lmax norm , L∞ or chebyshev distance . \\n ○ \\n Calculates the maximum difference between any component of vectors . \\n ○ \\n Appropriate when movement is unrestricted in any direction .  of  e  ( r = 1 ): \\n o Also known as Manhattan , taxicab , or L1 norm distance . \\n o Ideal for measuring distances in grid - like paths . \\n o Binary vector example : Hamming distance counts differing bits . \\n & e  ( r = 2 ): \\n o The most commonly used distance metric . \\n o Measures the straight - line distance in Euclidean space . e  ( r — ~ ): \\n o Also called Lmax norm , L~ » or chebyshev distance . \\n o Calculates the maximum difference between any component of vectors . \\n o Appropriate when movement is unrestricted in any direction .  of  ● \\n  ( r = 1 ): ○ \\n Also known as Manhattan , taxicab , or L1 norm distance . \\n ○ \\n Ideal for measuring distances in grid - like paths . \\n ○ \\n Binary vector example : Hamming distance counts differing bits . ● \\n  ( r = 2 ): ○ \\n The most commonly used distance metric . \\n ○ \\n Measures the straight - line distance in Euclidean space . ● \\n  ( r → ∞ ): ○ \\n Also called Lmax norm , L∞ or chebyshev distance . \\n ○ \\n Calculates the maximum difference between any component of vectors . \\n ○ \\n Appropriate when movement is unrestricted in any direction . ● \\n A . B is dot product of the two vectors \\n ● \\n It is cosine of the angle between two vectors \\n ● \\n Non - sensitive to magnitudes , focusing on orientation . \\n ● \\n Values are between -1 and 1 : \\n ○ \\n -1 ( completely dissimilar ) ○ \\n 1 ( perfect similarity ) . \\n ○ \\n 0 means orthogonal ( no similarity ) . \\n  ( applicable to numerical vectors )  ( applicable to numerical vectors ) n \\n A - B » AWB fe n \\n di y / o B ; \\n i=1 i=1 A .B is dot product of the two vectors cos(9 ) It is cosine of the angle between two vectors \\n Non - sensitive to magnitudes , focusing on orientation . y \\n Values are between -1 and 1 : o -1 ( completely dissimilar ) © 1 ( perfect similarity ) . o Omeans orthogonal ( no similarity ) . ● \\n A . B is dot product of the two vectors ● \\n It is cosine of the angle between two vectors ● \\n Non - sensitive to magnitudes , focusing on orientation . ● \\n Values are between -1 and 1 : ○ \\n -1 ( completely dissimilar ) ○ \\n 1 ( perfect similarity ) . ○ \\n 0 means orthogonal ( no similarity ) .  ( applicable to numerical vectors ) ● \\n Measure the linear relationship between two variables . \\n ● \\n Evaluates how well one variable predicts another one . \\n ● \\n Values are between -1 and 1 : \\n ○ \\n -1 ( perfect inverse correlation ) ○ \\n 1 ( perfect correlation ) . \\n ○ \\n 0 means orthogonal ( no linear relationship ) . \\n ● \\n Commonly used in statistical analysis and data exploration . \\n ● \\n It unable to capture nonlinear associations . \\n Linear correlation ( applicable to numerical vectors ) Linear correlation ( applicable to numerical vectors ) Perfect positive \\n correlation > ( x%i- * ) ( i- ) \\n > ( xi- * ) ? ( i -¥ ) ? e Measure the linear relationship between two variables . e Evaluates how well one variable predicts another one . e Values are between -1 and 1 : \\n o -1 ( perfect inverse correlation ) \\n o 1 ( perfect correlation ) . paren \\n o Omeans orthogonal ( no linear relationship ) . e Commonly used in statistical analysis and data exploration . e It unable to capture nonlinear associations . ● \\n Measure the linear relationship between two variables . ● \\n Evaluates how well one variable predicts another one . ● \\n Values are between -1 and 1 : ○ \\n -1 ( perfect inverse correlation ) ○ \\n 1 ( perfect correlation ) . \\n ○ \\n 0 means orthogonal ( no linear relationship ) . ● \\n Commonly used in statistical analysis and data exploration . ● \\n It unable to capture nonlinear associations . Linear correlation ( applicable to numerical vectors ) Distances and similarity examples \\n ● \\n Proximity measures for numerical vectors \\n ○ \\n  \\n ○ \\n  \\n ○ \\n  \\n ○ \\n Linear correlation \\n ● \\n Proximity measures for binary vectors \\n ○ \\n  ( SMC ) \\n ○ \\n  and similarity examples e Proximity measures for numerical vectors \\n o  \\n o  \\n o  \\n o Linear correlation e Proximity measures for binary vectors \\n o  ( SMC ) \\n o  and similarity examples ● \\n Proximity measures for numerical vectors ○ \\n  ○ \\n  ○ \\n  ○ \\n Linear correlation ● \\n Proximity measures for binary vectors ○ \\n  ( SMC ) ○ \\n  \\n ● \\n  ( SMC ): the number of matches divided by the total number of attributes . \\n SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) \\n ● \\n f01 = the number of attributes where x was 0 and y was 1 \\n ● \\n f10 = the number of attributes where x was 1 and y was 0 \\n ● \\n f00 = the number of attributes where x was 0 and y was 0 \\n ● \\n f11 = the number of attributes where x was 1 and y was 1  e  ( SMC ): the number of matches divided by the \\n total number of attributes . SMC = ( 4 , t+ Son ) or thio * Soo tA ) Jo , = the number of attributes where x was 0 and y was 1 \\n Ji ) = the number of attributes where x was 1 and y was 0 \\n Soo = the number of attributes where x was 0 and y was 0 f , , = the number of attributes where x was 1 and y was 1  ● \\n  ( SMC ): the number of matches divided by the total number of attributes . SMC = ( f11 + f00 ) / ( f01 + f10 + f00 + f11 ) ● \\n f01 = the number of attributes where x was 0 and y was 1 ● \\n f10 = the number of attributes where x was 1 and y was 0 ● \\n f00 = the number of attributes where x was 0 and y was 0 ● \\n f11 = the number of attributes where x was 1 and y was 1  \\n ● \\n  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . ● \\n It is designed for asymmetric binary attributes . \\n J = f11 / ( f01 + f10 + f11 ) \\n ● \\n f01 = the number of attributes where x was 0 and y was 1 \\n ● \\n f10 = the number of attributes where x was 1 and y was 0 \\n ● \\n f00 = the number of attributes where x was 0 and y was 0 \\n ● \\n f11 = the number of attributes where x was 1 and y was 1  e  ( J ): the number of \" 11 \" matches relative to the total \\n number of “ OO ” non - zero attributes . e tis designed for asymmetric binary attributes . J = Su / ( fo thot fi ) fo , = the number of attributes where x was 0 and y was 1 \\n Ji ) = the number of attributes where x was 1 and y was 0 \\n Jog = the number of attributes where x was 0 and y was 0 \\n f , , = the number of attributes where x was 1 and y was 1  ● \\n  ( J ): the number of \" 11 \" matches relative to the total number of “ 00 ” non - zero attributes . ● \\n It is designed for asymmetric binary attributes . J = f11 / ( f01 + f10 + f11 ) ● \\n f01 = the number of attributes where x was 0 and y was 1 \\n ● \\n f10 = the number of attributes where x was 1 and y was 0 \\n ● \\n f00 = the number of attributes where x was 0 and y was 0 \\n ● \\n f11 = the number of attributes where x was 1 and y was 1 Example : SMC vs  \\n x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 ● \\n f01 = 2 \\n ● \\n f10 = 1 \\n ● \\n f00 = 7 \\n ● \\n f11 = 0 SMC = 0.7 = 0 Example : SMC vs  x= 1000000000 \\n y= 0000001001 SMC = 0.7 = 0 Example : SMC vs  x = 1 0 0 0 0 0 0 0 0 0 y = 0 0 0 0 0 0 1 0 0 1 ● \\n f01 = 2 \\n ● \\n f10 = 1 \\n ● \\n f00 = 7 \\n ● \\n f11 = 0 SMC = 0.7 = 0 How to Choose the  ? \\n Choice of the right proximity measure depends on the domain \\n ● \\n  presence \\n ○ \\n  :  \\n ○ \\n Similarity : Documents are considered similar if they use high number of common words . \\n ● \\n  in Celsius of  \\n ○ \\n  :  \\n ○ \\n Similarity : Two locations are considered similar if their temperatures are similar in magnitude . \\n ● \\n  of Temperature ( Celsius ) \\n ○ \\n  :  \\n ○ \\n Similarity : Two time series are considered similar if their \" shape \" is similar , i.e. , they vary in the same way over time . \\n ● \\n  \\n ○ \\n  :  \\n ○ \\n Similarity : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain e  presence \\n o  :  \\n ° o Similarity : Documents are considered similar if they use high number of common words . e  in Celsius of  \\n o  :  \\n © Similarity : Two locations are considered similar if their temperatures are similar in magnitude . e  of Temperature ( Celsius ) \\n o  :  \\n o Similarity : Two time series are considered similar if their \" shape \" is similar , i.e. , they vary in the same \\n way over time . e  \\n o  :  \\n o Similarity : Measures the linear relationship between two variables . . How to Choose the  ? Choice of the right proximity measure depends on the domain ● \\n  presence ○ \\n  :  \\n ○ \\n Similarity : Documents are considered similar if they use high number of common words . ● \\n  in Celsius of  ○ \\n  :  \\n ○ \\n Similarity : Two locations are considered similar if their temperatures are similar in magnitude . ● \\n  of Temperature ( Celsius ) ○ \\n  :  \\n ○ \\n Similarity : Two time series are considered similar if their \" shape \" is similar , i.e. , they vary in the same way over time . ● \\n  ○ \\n  :  \\n ○ \\n Similarity : Measures the linear relationship between two variables . . Similarity and Dissimilarity and attribute type \\n Similarity / dissimilarity between two objects , x and y , with only one attribute : Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute :  \\n Type \\n : 1 te=—9 % \\n Nominal ue S= ) 9 vey Zy \\n Ordinal ( values mapped to integers 0 ton—1 , | s=1-—d \\n where n is the number of values ) \\n Interval or Ratio = lo s=—-d , s=7qQ , s = e \" d — min_d \\n max - d — min_d s = l1- Similarity and Dissimilarity and attribute type Similarity / dissimilarity between two objects , x and y , with only one attribute :']\n"
     ]
    }
   ],
   "source": [
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    import Preprocessing_data.DM as dm_module  \n",
    "\n",
    "DM = dm_module.texts\n",
    "\n",
    "print(DM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 21:49:24,455 - INFO - Use pytorch device_name: cpu\n",
      "2025-04-03 21:49:24,455 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-04-03 21:49:27,708 - INFO - Loading faiss with AVX2 support.\n",
      "2025-04-03 21:49:28,198 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-04-03 21:49:28,267 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = FAISS.from_texts(DM, embedding_model)\n",
    "\n",
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vector_store.as_retriever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
